{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c198fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# !pip install --upgrade boto3==1.39.8\n",
    "# !sudo apt install antiword \n",
    "import io\n",
    "import uuid\n",
    "import os\n",
    "import random\n",
    "import nltk\n",
    "import boto3\n",
    "import json\n",
    "import pytesseract\n",
    "import textract\n",
    "from pdf2image import convert_from_bytes\n",
    "from PyPDF2 import PdfReader\n",
    "from docx import Document\n",
    "from io import BytesIO\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import extract_msg\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.schema import TextNode, NodeRelationship, RelatedNodeInfo\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# embed_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "# embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "embed_model = AutoModel.from_pretrained(\"intfloat/e5-small-v2\")\n",
    "\n",
    "S3_BUCKET = \"ml-legal-restricted\"\n",
    "EXCEL_PATH = \"full_contracts_with_files.xlsx\"\n",
    "VECTOR_BUCKET_NAME = \"legal-docs-vector-store\"\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "s3 = boto3.client('s3')\n",
    "s3v = boto3.client(\"s3vectors\", region_name=\"us-east-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe3f0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_excel(bucket, key, sheet_name=\"Active Legal Contracts\", column=\"Contract Number\"):\n",
    "    print(f\"üì• Downloading Excel: s3://{bucket}/{key}\")\n",
    "    obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "    excel_data = obj['Body'].read()\n",
    "\n",
    "    df = pd.read_excel(io.BytesIO(excel_data), sheet_name=sheet_name, engine='openpyxl')\n",
    "    if column not in df.columns:\n",
    "        raise ValueError(f\"Column '{column}' not found in sheet '{sheet_name}'\")\n",
    "\n",
    "    contract_numbers = df[column].dropna().astype(str).str.strip().tolist()\n",
    "    return contract_numbers, df, excel_data\n",
    "\n",
    "\n",
    "def list_s3_files_for_contract(bucket, contract_number, prefix_base=\"contract-docs/\"):\n",
    "    prefix = f\"{prefix_base}{contract_number}/\"\n",
    "    files = []\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "\n",
    "    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "        for obj in page.get(\"Contents\", []):\n",
    "            files.append(obj[\"Key\"])\n",
    "    return files\n",
    "\n",
    "\n",
    "def add_modified_sheet_with_files(wb, original_df, file_map):\n",
    "    ws = wb.create_sheet(\"Active Legal Contracts + Files\")\n",
    "    headers = original_df.columns.tolist()\n",
    "    max_files = max((len(files) for files in file_map.values()), default=0)\n",
    "    headers += [f\"File {i+1}\" for i in range(max_files)]\n",
    "    ws.append(headers)\n",
    "\n",
    "    for idx, row in original_df.iterrows():\n",
    "        contract_number = str(row[\"Contract Number\"]).strip()\n",
    "        files = [os.path.basename(f) for f in file_map.get(contract_number, [])]\n",
    "        base_row = row.tolist()\n",
    "        ws.append(base_row + files)\n",
    "\n",
    "\n",
    "def add_s3_paths_sheet(wb, file_map, bucket):\n",
    "    ws = wb.create_sheet(\"S3 File Paths\")\n",
    "    max_files = max((len(files) for files in file_map.values()), default=0)\n",
    "\n",
    "    headers = [\"Contract Number\"] + [f\"S3 File {i+1}\" for i in range(max_files)]\n",
    "    ws.append(headers)\n",
    "\n",
    "    for contract, keys in file_map.items():\n",
    "        s3_paths = [f\"s3://{bucket}/{key}\" for key in keys]\n",
    "        ws.append([contract] + s3_paths)\n",
    "\n",
    "def save_missing_contracts_to_csv(missing_contracts, output_path=\"missing_contracts.csv\"):\n",
    "    if not missing_contracts:\n",
    "        print(\"‚úÖ No missing contracts to save.\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(missing_contracts, columns=[\"Contract Number\"])\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"üìÑ Missing contracts CSV saved to: {output_path}\")\n",
    "    \n",
    "def process_entire_bucket(bucket, excel_key, output_path=\"full_contracts_with_files.xlsx\"):\n",
    "    contract_numbers, original_df, excel_bytes = download_excel(bucket, excel_key)\n",
    "\n",
    "    file_map = {}\n",
    "    missing_contracts = []\n",
    "\n",
    "    print(f\"\\nüîç Scanning {len(contract_numbers)} contract numbers across S3...\")\n",
    "    for idx, number in enumerate(contract_numbers, 1):\n",
    "        if idx % 1000 == 0 or idx == 1:\n",
    "            print(f\"üî¢ [{idx}/{len(contract_numbers)}] Scanning: {number}\")\n",
    "        files = list_s3_files_for_contract(bucket, number)\n",
    "\n",
    "        if not files and len(number) < 8 and number.isdigit():\n",
    "            padded = number.zfill(8)\n",
    "            print(f\"   ‚ûï Retrying with padded contract number: {padded}\")\n",
    "            files = list_s3_files_for_contract(bucket, padded)\n",
    "            \n",
    "        if files:\n",
    "            file_map[number] = files\n",
    "        else:\n",
    "            missing_contracts.append(number)\n",
    "\n",
    "    print(\"\\nüßæ Preparing final Excel workbook...\")\n",
    "    wb = load_workbook(io.BytesIO(excel_bytes))\n",
    "    add_modified_sheet_with_files(wb, original_df, file_map)\n",
    "    add_s3_paths_sheet(wb, file_map, bucket)\n",
    "    wb.save(output_path)\n",
    "    print(f\"‚úÖ Final Excel saved: {output_path}\")\n",
    "\n",
    "    print(\"\\n=== Final Summary ===\")\n",
    "    print(f\"üìÑ Total contracts processed: {len(contract_numbers)}\")\n",
    "    print(f\"‚úÖ Contracts with files: {len(file_map)}\")\n",
    "    print(f\"‚ùå Contracts with NO files found: {len(missing_contracts)}\")\n",
    "    if missing_contracts:\n",
    "        print(f\"üîç Sample missing contract numbers: {missing_contracts[:5]}{'...' if len(missing_contracts) > 5 else ''}\")\n",
    "        save_missing_contracts_to_csv(missing_contracts)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    bucket = \"ml-legal-restricted\"\n",
    "    excel_key = \"tabularData/Active Legal Contracts 7-10-2025 1-17-09 PM.xlsx\"\n",
    "    process_entire_bucket(bucket, excel_key, output_path=\"full_contracts_with_files.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1ef4e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "class E5Embedder:\n",
    "    def __init__(self, model_name=\"intfloat/e5-small-v2\", device=None):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def get_text_embedding_batch(self, texts):\n",
    "        texts = [f\"passage: {text}\" for text in texts]\n",
    "        inputs = self.tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\").to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            embeddings = outputs.last_hidden_state[:, 0]\n",
    "            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "        return embeddings.cpu().numpy().tolist()\n",
    "\n",
    "    def get_text_embedding(self, text):\n",
    "        return self.get_text_embedding_batch([text])[0]\n",
    "\n",
    "embed_model = E5Embedder(\"intfloat/e5-small-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "993c62e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created index: {'ResponseMetadata': {'RequestId': '8ade74ec-0c69-4318-8f2e-920d27820736', 'HostId': '', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Tue, 22 Jul 2025 19:13:47 GMT', 'content-type': 'application/json', 'content-length': '2', 'connection': 'keep-alive', 'x-amz-request-id': '8ade74ec-0c69-4318-8f2e-920d27820736', 'access-control-allow-origin': '*', 'vary': 'origin, access-control-request-method, access-control-request-headers', 'access-control-expose-headers': '*'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "# to recreate vector index store\n",
    "\n",
    "INDEX_NAME = \"token-chunking\"\n",
    "# INDEX_NAME = \"overlap-chunking\"\n",
    "# INDEX_NAME = \"semantic-split-chunking\"\n",
    "VECTOR_DIM = 384\n",
    "DISTANCE_METRIC = \"cosine\"\n",
    "NON_FILTERABLE_KEYS = ['text']\n",
    "\n",
    "response = s3v.delete_index(\n",
    "    vectorBucketName=VECTOR_BUCKET_NAME,\n",
    "    indexName=INDEX_NAME,\n",
    ")\n",
    "\n",
    "response = s3v.create_index(\n",
    "    vectorBucketName=VECTOR_BUCKET_NAME,\n",
    "    indexName=INDEX_NAME,\n",
    "    dataType=\"float32\",\n",
    "    dimension=VECTOR_DIM,\n",
    "    distanceMetric=DISTANCE_METRIC,\n",
    "    metadataConfiguration={\n",
    "        \"nonFilterableMetadataKeys\": NON_FILTERABLE_KEYS\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Created index: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78836f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, s3_path, token_limit=500, tokenizer_name=\"intfloat/e5-small-v2\"):\n",
    "    file_name = os.path.basename(s3_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks, current, count = [], [], 0\n",
    "\n",
    "    for sent in sentences:\n",
    "        toks = tokenizer.tokenize(sent)\n",
    "        if count + len(toks) > token_limit and current:\n",
    "            chunks.append(\" \".join(current))\n",
    "            current, count = [], 0\n",
    "        current.append(sent)\n",
    "        count += len(toks)\n",
    "\n",
    "    if current:\n",
    "        chunks.append(\" \".join(current))\n",
    "\n",
    "    return [{\"key\": str(uuid.uuid4()), \"metadata\": {\"text\": chunk, \"s3_path\": s3_path, \"file_name\": file_name}} for chunk in chunks]\n",
    "\n",
    "def chunk_text_with_overlap(text, s3_path, token_limit=500, chunk_overlap=50, tokenizer_name=\"intfloat/e5-small-v2\"):\n",
    "    file_name = os.path.basename(s3_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "    sentences, chunks, curr, curr_toks = sent_tokenize(text), [], [], 0\n",
    "\n",
    "    for sent in sentences:\n",
    "        stoks = len(tokenizer.tokenize(sent))\n",
    "        if stoks > token_limit:\n",
    "            words = sent.split()\n",
    "            i = 0\n",
    "            while i < len(words):\n",
    "                segment = words[i:i+token_limit]\n",
    "                chunks.append(\" \".join(segment))\n",
    "                i += token_limit - chunk_overlap\n",
    "            continue\n",
    "\n",
    "        if curr_toks + stoks <= token_limit:\n",
    "            curr.append(sent); curr_toks += stoks\n",
    "        else:\n",
    "            chunks.append(\" \".join(curr))\n",
    "            # build overlap\n",
    "            overlap, tot = [], 0\n",
    "            for s in reversed(curr):\n",
    "                l = len(tokenizer.tokenize(s))\n",
    "                if tot + l > chunk_overlap:\n",
    "                    break\n",
    "                overlap.insert(0, s); tot += l\n",
    "            curr = overlap + [sent]\n",
    "            curr_toks = sum(len(tokenizer.tokenize(s)) for s in curr)\n",
    "\n",
    "    if curr:\n",
    "        chunks.append(\" \".join(curr))\n",
    "    \n",
    "    return [{\"key\": str(uuid.uuid4()), \"metadata\": {\"text\": chunk, \"s3_path\": s3_path, \"file_name\": file_name}} for chunk in chunks]\n",
    "\n",
    "def chunk_with_semantic_split(text, s3_path, \n",
    "                              buffer_size=1,\n",
    "                              breakpoint_percentile_threshold=95):\n",
    "\n",
    "    from llama_index.core import Document\n",
    "\n",
    "    metadata = {\"s3_path\": s3_path, \"file_name\": os.path.basename(s3_path)}\n",
    "    doc = Document(text=text, metadata=metadata)\n",
    "\n",
    "    parser = SemanticSplitterNodeParser.from_defaults(\n",
    "        embed_model=embed_model,\n",
    "        buffer_size=buffer_size,\n",
    "        breakpoint_percentile_threshold=breakpoint_percentile_threshold,\n",
    "        include_metadata=False,\n",
    "        include_prev_next_rel=False\n",
    "    )\n",
    "\n",
    "    nodes = parser.get_nodes_from_documents([doc])\n",
    "    texts = [n.text for n in nodes]\n",
    "\n",
    "    result = []\n",
    "    for n in nodes:\n",
    "        unique_id = str(uuid.uuid4())\n",
    "        result.append({\n",
    "            \"key\": unique_id,\n",
    "            \"metadata\": {\n",
    "                \"text\": n.text,\n",
    "                \"s3_path\": s3_path,\n",
    "                \"file_name\": os.path.basename(s3_path)\n",
    "            },\n",
    "        })\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "501fb485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Processing 1/25: contract-docs/66179/Dynamics_-_66179_signed.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (658 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extracted 17 chunks.\n",
      "\n",
      "üìÑ Processing 2/25: contract-docs/77767/Dynamics_-_77767_signed.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (722 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extracted 7 chunks.\n",
      "\n",
      "üìÑ Processing 3/25: contract-docs/65062/33908_SIGNED_ESI CO# 3417pdf.pdf\n",
      "‚úÖ Extracted 2 chunks.\n",
      "\n",
      "üìÑ Processing 4/25: contract-docs/57964/VNSNY-Cotiviti Amend No 3 to MSLA Client Release 3.0 (SBC 02.04.20).docx\n",
      "‚úÖ Extracted 4 chunks.\n",
      "\n",
      "üìÑ Processing 5/25: contract-docs/55622/Highmark - SOW# 1 Prospective Claims (March 2016) [55622].pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (801 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extracted 1 chunks.\n",
      "\n",
      "üìÑ Processing 6/25: contract-docs/80537/VRC EL - BEV Equity Valuation as of 12-31-22 - LEGAL APPROVED - 22Feb2023_Fully executed.pdf\n",
      "‚úÖ Extracted 5 chunks.\n",
      "\n",
      "üìÑ Processing 7/25: contract-docs/64936/24380_WellCare CO 3266 - CO 12 to PQ 13 - Aetna LOBs and Plan Codes (8249).pdf\n",
      "‚úÖ Extracted 2 chunks.\n",
      "\n",
      "üìÑ Processing 8/25: contract-docs/80577/Vena Solutions_NDA_03292017_vendor signed.pdf\n",
      "‚úÖ Extracted 1 chunks.\n",
      "\n",
      "üìÑ Processing 9/25: contract-docs/67566/Ochsner - Cotiviti - NDA - MV - 02222022 - Dynamics_67566.pdf\n",
      "‚úÖ Extracted 9 chunks.\n",
      "\n",
      "üìÑ Processing 10/25: contract-docs/54094/Isos-Cotiviti- PAD Migration SOW - ATS2018720 - Ready for Signature.pdf\n",
      "‚úÖ Extracted 7 chunks.\n",
      "\n",
      "üìÑ Processing 11/25: contract-docs/59747/Cotiviti - NCQA License and Certification Agreement Amendment_8.17.20.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (836 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extracted 42 chunks.\n",
      "\n",
      "üìÑ Processing 12/25: contract-docs/64408/59062_SIGNED_Anthem CE - Ready Set Renew IVR SMS & Email Progam - SOW 880 Signature Ready 101921.pdf\n",
      "‚úÖ Extracted 7 chunks.\n",
      "\n",
      "üìÑ Processing 13/25: contract-docs/00010217/Willis-Mosiac-Towers Watson DUA 01-22-2014.docx\n",
      "‚úÖ Extracted 5 chunks.\n",
      "\n",
      "üìÑ Processing 14/25: contract-docs/53985/Partners Healthcare MSA Amendment 4 (DxCG) jsw 12.18.18 FINAL_signed.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (565 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extracted 5 chunks.\n",
      "\n",
      "üìÑ Processing 15/25: contract-docs/78927/Dynamics_78927_signed.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (754 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extracted 17 chunks.\n",
      "\n",
      "üìÑ Processing 16/25: contract-docs/59164/RStudio Renewal - Q-16631-20200402-1828 - Preeti Vaidya.pdf\n",
      "‚úÖ Extracted 3 chunks.\n",
      "\n",
      "üìÑ Processing 17/25: contract-docs/71355/Dynamics_-_71355_signed.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1025 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extracted 13 chunks.\n",
      "\n",
      "üìÑ Processing 18/25: contract-docs/64596/28695_SIGNED_Boston Medical Center HealthNet Plan CO1 to SOW3 (8778) .pdf\n",
      "‚úÖ Extracted 3 chunks.\n",
      "\n",
      "üìÑ Processing 19/25: contract-docs/61941/Dynamics_61941_Please_review_and_sign_your_do_signed.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (602 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extracted 11 chunks.\n",
      "\n",
      "üìÑ Processing 20/25: contract-docs/00037577/Re  Requisition for Invite Networks  quote # 324MJ-Q   SoJo switches.msg\n",
      "‚úÖ Extracted 9 chunks.\n",
      "\n",
      "üìÑ Processing 21/25: contract-docs/62520/Approval Needed_ Temp Contract Extension (Compliance - Stinson, Leticia) - FP&A - 07.29.2021.pdf\n",
      "‚úÖ Extracted 3 chunks.\n",
      "\n",
      "üìÑ Processing 22/25: contract-docs/00006573/Towers Watson-MI-NDA-Executed.pdf\n",
      "‚úÖ Extracted 5 chunks.\n",
      "\n",
      "üìÑ Processing 23/25: contract-docs/75036/Dynamics_75036_signed.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (969 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extracted 6 chunks.\n",
      "\n",
      "üìÑ Processing 24/25: contract-docs/71954/InSync SOW Rochelle Gilpin .docx\n",
      "‚úÖ Extracted 1 chunks.\n",
      "\n",
      "üìÑ Processing 25/25: contract-docs/74972/Brex - Cotiviti - NDA - MS - 07092024 - Dynamics_74972.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (755 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extracted 12 chunks.\n",
      "\n",
      "=== Summary ===\n",
      "Processed: 25  Failed: 0\n",
      "Pdf: 21  Docx: 3  Txt: 0  Msg: 1  Doc: 0  Ocr: 1  None: 0\n",
      "\n",
      " ‚úÖ Successfully uploaded to token-chunking S3 vector Index\n"
     ]
    }
   ],
   "source": [
    "SUPPORTED_EXTENSIONS = {\".pdf\", \".docx\", \".txt\", \".msg\", \".doc\"}\n",
    "\n",
    "def list_supported_files(bucket, prefix=\"\"):\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "    all_files, supported, unsupported = [], [], []\n",
    "\n",
    "    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "        for obj in page.get(\"Contents\", []):\n",
    "            key = obj[\"Key\"]\n",
    "            all_files.append(key)\n",
    "            ext = os.path.splitext(key)[1].lower()\n",
    "            if ext in SUPPORTED_EXTENSIONS:\n",
    "                supported.append(key)\n",
    "            else:\n",
    "                unsupported.append(key)\n",
    "\n",
    "    print(\"\\n=== File Summary ===\")\n",
    "    print(f\"üìÅ Total files: {len(all_files)}\")\n",
    "    print(f\"‚úÖ Supported files: {len(supported)}\")\n",
    "    print(f\"‚ùå Unsupported: {len(unsupported)} (Sample: {unsupported[:5]})\\n\")\n",
    "    return supported\n",
    "\n",
    "def download_s3_file(bucket, key):\n",
    "    response = s3.get_object(Bucket=bucket, Key=key)\n",
    "    return io.BytesIO(response[\"Body\"].read())\n",
    "\n",
    "def extract_from_doc(file_io):\n",
    "    try:\n",
    "        file_io.seek(0)\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".doc\") as temp_file:\n",
    "            temp_file.write(file_io.read())\n",
    "            temp_path = temp_file.name\n",
    "\n",
    "        text = textract.process(temp_path).decode(\"utf-8\").strip()\n",
    "        os.remove(temp_path)\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùó DOC (textract) error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def extract_with_ocr(file_io):\n",
    "    try:\n",
    "        file_io.seek(0)\n",
    "        images = convert_from_bytes(file_io.read())\n",
    "        return \"\\n\".join(pytesseract.image_to_string(img) for img in images).strip()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùó OCR failed: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_from_pdf(file_io):\n",
    "    try:\n",
    "        reader = PdfReader(file_io)\n",
    "        return \"\\n\".join([p.extract_text() for p in reader.pages if p.extract_text()]).strip()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùó PDF read error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def extract_from_docx(file_io):\n",
    "    from docx import Document\n",
    "    from io import BytesIO\n",
    "\n",
    "    try:\n",
    "        if hasattr(file_io, \"read\"):\n",
    "            file_io.seek(0)\n",
    "            content = file_io.read()\n",
    "            bio = BytesIO(content)\n",
    "            \n",
    "            doc = Document(bio)\n",
    "        else:\n",
    "\n",
    "            bio = BytesIO(file_io)\n",
    "            doc = Document(bio)\n",
    "            \n",
    "        text = \"\\n\".join(p.text for p in doc.paragraphs).strip()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùó DOCX error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def extract_from_txt(file_io):\n",
    "    try:\n",
    "        return file_io.read().decode(\"utf-8\").strip()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùó TXT read error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_from_msg(file_io):\n",
    "    try:\n",
    "        with open(\"temp.msg\", \"wb\") as f:\n",
    "            f.write(file_io.read())\n",
    "        msg = extract_msg.Message(\"temp.msg\")\n",
    "        text = msg.body or \"\"\n",
    "        os.remove(\"temp.msg\")\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùó MSG read error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_text(file_io, ext):\n",
    "    ext = ext.lower()\n",
    "    \n",
    "    file_io.seek(0)\n",
    "    sig = file_io.read(4)\n",
    "    file_io.seek(0)\n",
    "    \n",
    "    extractors = {\n",
    "        \".pdf\": [\n",
    "            extract_from_pdf,\n",
    "            extract_with_ocr\n",
    "        ],\n",
    "        \".docx\": [\n",
    "            extract_from_docx,\n",
    "            extract_with_ocr\n",
    "        ],\n",
    "        \".doc\": [extract_from_doc],\n",
    "        \".txt\": [extract_from_txt,],\n",
    "        \".msg\": [extract_from_msg],\n",
    "    }\n",
    "\n",
    "    for extractor in extractors.get(ext, []):\n",
    "        file_io.seek(0)\n",
    "        text = extractor(file_io)\n",
    "        if text:\n",
    "            return text, extractor.__name__\n",
    "\n",
    "    return \"\", \"none\"\n",
    "\n",
    "\n",
    "def upload_chunks_to_s3_vector_index(chunks, embed_model, vector_bucket_name, chunking_strategy):\n",
    "\n",
    "    texts = [chunk[\"metadata\"][\"text\"] for chunk in chunks]\n",
    "    keys = [chunk[\"key\"] for chunk in chunks]\n",
    "\n",
    "    embeddings = embed_model.get_text_embedding_batch(texts)\n",
    "\n",
    "    vectors = []\n",
    "    for i in range(len(chunks)):\n",
    "        vector_metadata = chunks[i][\"metadata\"].copy()\n",
    "        vectors.append({\n",
    "            \"key\": keys[i],\n",
    "            \"data\": {\"float32\": embeddings[i]},\n",
    "            \"metadata\": vector_metadata\n",
    "        })\n",
    "\n",
    "    response = s3v.put_vectors(\n",
    "        vectorBucketName=vector_bucket_name,\n",
    "        indexName=chunking_strategy,\n",
    "        vectors=vectors\n",
    "    )\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def process_documents(bucket, keys, chunking_strategy):\n",
    "    stats = {\n",
    "        \"processed\": 0, \"failed\": 0,\n",
    "        \"pdf\": 0, \"docx\": 0, \"txt\": 0, \"msg\": 0, \"doc\": 0,\n",
    "        \"ocr\": 0, \"none\": 0\n",
    "    }\n",
    "\n",
    "    all_chunks = []\n",
    "\n",
    "    for idx, key in enumerate(keys, 1):\n",
    "        print(f\"\\nüìÑ Processing {idx}/{len(keys)}: {key}\")\n",
    "        ext = os.path.splitext(key)[1].lower()\n",
    "        s3_path = f\"s3://{bucket}/{key}\"\n",
    "\n",
    "        try:\n",
    "            file_io = download_s3_file(bucket, key)\n",
    "            text, method = extract_text(file_io, ext)\n",
    "\n",
    "            if not text:\n",
    "                print(\"‚ö†Ô∏è No text extracted.\")\n",
    "                stats[\"failed\"] += 1\n",
    "                stats[\"none\"] += 1\n",
    "                continue\n",
    "            \n",
    "            if chunking_strategy == 'token-chunking':\n",
    "                chunks = chunk_text(text, s3_path)\n",
    "            elif chunking_strategy == 'overlap-chunking':\n",
    "                chunks = chunk_text_with_overlap(text, s3_path)\n",
    "            elif chunking_strategy == 'semantic-split-chunking':\n",
    "                chunks = chunk_with_semantic_split(text, s3_path)\n",
    "\n",
    "            print(f\"‚úÖ Extracted {len(chunks)} chunks.\")\n",
    "            # sample = chunks[0]['text'][:80].replace('\\n', ' ')\n",
    "            sample = chunks\n",
    "            # print(f\"üìù Sample: {sample}...\")\n",
    "\n",
    "            all_chunks.extend(chunks)\n",
    "            stats[\"processed\"] += 1\n",
    "            stats[ext.replace(\".\", \"\")] += 1\n",
    "            if \"ocr\" in method: stats[\"ocr\"] += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùó Error: {e}\")\n",
    "            stats[\"failed\"] += 1\n",
    "            stats[\"none\"] += 1\n",
    "\n",
    "    print(\"\\n=== Summary ===\")\n",
    "    first_keys = ['processed', 'failed']\n",
    "    first_line = []\n",
    "    second_line = []\n",
    "\n",
    "    for k, v in stats.items():\n",
    "        key_formatted = k.capitalize().replace('_', ' ')\n",
    "        pair = f\"{key_formatted}: {v}\"\n",
    "        if k.lower() in first_keys:\n",
    "            first_line.append(pair)\n",
    "        else:\n",
    "            second_line.append(pair)\n",
    "\n",
    "    print(\"  \".join(first_line))\n",
    "    print(\"  \".join(second_line))\n",
    "\n",
    "    return stats, all_chunks\n",
    "\n",
    "def read_s3_keys_from_excel(excel_path, sample_size, sheet_name=\"S3 File Paths\"):\n",
    "    print(f\"üì• Reading S3 paths from Excel: {excel_path}, sheet: {sheet_name}\")\n",
    "    df = pd.read_excel(excel_path, sheet_name=sheet_name, engine=\"openpyxl\")\n",
    "\n",
    "    file_columns = [col for col in df.columns if col.startswith(\"S3 File\")]\n",
    "\n",
    "    if not file_columns:\n",
    "        raise ValueError(f\"No columns starting with 'S3 File' found in sheet '{sheet_name}'\")\n",
    "\n",
    "    all_paths = []\n",
    "    for col in file_columns:\n",
    "        for cell in df[col].dropna():\n",
    "            path = str(cell).strip()\n",
    "            if path:\n",
    "                all_paths.append(path)\n",
    "\n",
    "    print(f\"üîç Total paths found: {len(all_paths)}\")\n",
    "\n",
    "    valid_keys = []\n",
    "    invalid_paths = []\n",
    "\n",
    "    for path in all_paths:\n",
    "        if path.startswith(\"s3://\"):\n",
    "            parts = path.replace(\"s3://\", \"\").split(\"/\", 1)\n",
    "            if len(parts) == 2 and parts[1].strip():\n",
    "                valid_keys.append(parts[1])\n",
    "            else:\n",
    "                invalid_paths.append(path)\n",
    "        else:\n",
    "            invalid_paths.append(path)\n",
    "\n",
    "    print(f\"üìÅ Parsed S3 keys: {len(valid_keys)}\")\n",
    "\n",
    "    if invalid_paths:\n",
    "        print(f\"‚ö†Ô∏è Skipped invalid paths: {len(invalid_paths)}\")\n",
    "        print(\"\\n‚ö†Ô∏è Skipped Paths (sample):\")\n",
    "        for bad in invalid_paths[:10]:\n",
    "            print(f\" - {bad}\")\n",
    "        if len(invalid_paths) > 10:\n",
    "            print(f\" ...and {len(invalid_paths) - 10} more.\")\n",
    "\n",
    "    selected_keys = random.sample(valid_keys, min(sample_size, len(valid_keys)))\n",
    "    print(f\"üéØ Randomly selected {len(selected_keys)} files.\")\n",
    "    return selected_keys\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # file_keys = read_s3_keys_from_excel(EXCEL_PATH, sample_size=5)\n",
    "    file_keys = ['contract-docs/66179/Dynamics_-_66179_signed.pdf', 'contract-docs/77767/Dynamics_-_77767_signed.pdf', 'contract-docs/65062/33908_SIGNED_ESI CO# 3417pdf.pdf', 'contract-docs/57964/VNSNY-Cotiviti Amend No 3 to MSLA Client Release 3.0 (SBC 02.04.20).docx', 'contract-docs/55622/Highmark - SOW# 1 Prospective Claims (March 2016) [55622].pdf', 'contract-docs/80537/VRC EL - BEV Equity Valuation as of 12-31-22 - LEGAL APPROVED - 22Feb2023_Fully executed.pdf', 'contract-docs/64936/24380_WellCare CO 3266 - CO 12 to PQ 13 - Aetna LOBs and Plan Codes (8249).pdf', 'contract-docs/80577/Vena Solutions_NDA_03292017_vendor signed.pdf', 'contract-docs/67566/Ochsner - Cotiviti - NDA - MV - 02222022 - Dynamics_67566.pdf', 'contract-docs/54094/Isos-Cotiviti- PAD Migration SOW - ATS2018720 - Ready for Signature.pdf', 'contract-docs/59747/Cotiviti - NCQA License and Certification Agreement Amendment_8.17.20.pdf', 'contract-docs/64408/59062_SIGNED_Anthem CE - Ready Set Renew IVR SMS & Email Progam - SOW 880 Signature Ready 101921.pdf', 'contract-docs/00010217/Willis-Mosiac-Towers Watson DUA 01-22-2014.docx', 'contract-docs/53985/Partners Healthcare MSA Amendment 4 (DxCG) jsw 12.18.18 FINAL_signed.pdf', 'contract-docs/78927/Dynamics_78927_signed.pdf', 'contract-docs/59164/RStudio Renewal - Q-16631-20200402-1828 - Preeti Vaidya.pdf', 'contract-docs/71355/Dynamics_-_71355_signed.pdf', 'contract-docs/64596/28695_SIGNED_Boston Medical Center HealthNet Plan CO1 to SOW3 (8778) .pdf', 'contract-docs/61941/Dynamics_61941_Please_review_and_sign_your_do_signed.pdf', 'contract-docs/00037577/Re  Requisition for Invite Networks  quote # 324MJ-Q   SoJo switches.msg', 'contract-docs/62520/Approval Needed_ Temp Contract Extension (Compliance - Stinson, Leticia) - FP&A - 07.29.2021.pdf', 'contract-docs/00006573/Towers Watson-MI-NDA-Executed.pdf', 'contract-docs/75036/Dynamics_75036_signed.pdf', 'contract-docs/71954/InSync SOW Rochelle Gilpin .docx', 'contract-docs/74972/Brex - Cotiviti - NDA - MS - 07092024 - Dynamics_74972.pdf']\n",
    "\n",
    "    for chunking_strategy in [\"token-chunking\"]: #, \"overlap-chunking\", \"semantic-split-chunking\"]:\n",
    "        final_stats, chunks = process_documents(S3_BUCKET, file_keys, chunking_strategy)\n",
    "        if chunks:\n",
    "            response = upload_chunks_to_s3_vector_index(chunks, embed_model, VECTOR_BUCKET_NAME, chunking_strategy)\n",
    "            if response:\n",
    "                print(f\"\\n ‚úÖ Successfully uploaded to {chunking_strategy} S3 vector Index\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a26c3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197\n"
     ]
    }
   ],
   "source": [
    "#check number of chunks uploaded\n",
    "\n",
    "INDEX_NAME = \"token-chunking\"\n",
    "# INDEX_NAME = \"overlap-chunking\"\n",
    "# INDEX_NAME = \"semantic-split-chunking\"\n",
    "response = s3v.list_vectors(\n",
    "    vectorBucketName=VECTOR_BUCKET_NAME,\n",
    "    indexName=INDEX_NAME,\n",
    "    returnData=True,\n",
    "    returnMetadata=True\n",
    ")\n",
    "\n",
    "vectors = response.get(\"vectors\", [])\n",
    "print(len(vectors))\n",
    "\n",
    "# for vector in vectors:\n",
    "#     print(f\"Key: {vector['key']}\")\n",
    "#     print(f\"Metadata: {vector.get('metadata',{})}\")\n",
    "#     print(f\"Embedding (first 5 dims): {vector['data']['float32'][:5]}\")\n",
    "#     print(\"--------------\")\n",
    "\n",
    "# next_token = response.get(\"nextToken\")\n",
    "# if next_token:\n",
    "#     pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cbec615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Query: 'What obligations does Cotiviti have under Schedule C for Prepay FWAV Services?' ---\n",
      "Retrieved 5\n",
      "    S3_Path 1: s3://ml-legal-restricted/contract-docs/71355/Dynamics_-_71355_signed.pdf\n",
      "    S3_Path 2: s3://ml-legal-restricted/contract-docs/66179/Dynamics_-_66179_signed.pdf\n",
      "    S3_Path 3: s3://ml-legal-restricted/contract-docs/66179/Dynamics_-_66179_signed.pdf\n",
      "    S3_Path 4: s3://ml-legal-restricted/contract-docs/66179/Dynamics_-_66179_signed.pdf\n",
      "    S3_Path 5: s3://ml-legal-restricted/contract-docs/66179/Dynamics_-_66179_signed.pdf\n",
      "\n",
      "--- Processing Query: 'Under Schedule C, what services is Cotiviti required to provide?' ---\n",
      "Retrieved 5\n",
      "    S3_Path 1: s3://ml-legal-restricted/contract-docs/66179/Dynamics_-_66179_signed.pdf\n",
      "    S3_Path 2: s3://ml-legal-restricted/contract-docs/66179/Dynamics_-_66179_signed.pdf\n",
      "    S3_Path 3: s3://ml-legal-restricted/contract-docs/71355/Dynamics_-_71355_signed.pdf\n",
      "    S3_Path 4: s3://ml-legal-restricted/contract-docs/66179/Dynamics_-_66179_signed.pdf\n",
      "    S3_Path 5: s3://ml-legal-restricted/contract-docs/77767/Dynamics_-_77767_signed.pdf\n",
      "\n",
      "--- Processing Query: 'In the Prepay FWAV Services section, what are Cotiviti's main deliverables?' ---\n",
      "Retrieved 5\n",
      "    S3_Path 1: s3://ml-legal-restricted/contract-docs/66179/Dynamics_-_66179_signed.pdf\n",
      "    S3_Path 2: s3://ml-legal-restricted/contract-docs/66179/Dynamics_-_66179_signed.pdf\n",
      "    S3_Path 3: s3://ml-legal-restricted/contract-docs/71355/Dynamics_-_71355_signed.pdf\n",
      "    S3_Path 4: s3://ml-legal-restricted/contract-docs/66179/Dynamics_-_66179_signed.pdf\n",
      "    S3_Path 5: s3://ml-legal-restricted/contract-docs/66179/Dynamics_-_66179_signed.pdf\n"
     ]
    }
   ],
   "source": [
    "def query_s3_vector_store(query_text, INDEX_NAME, top_k = 5):\n",
    "    print(f\"\\n--- Processing Query: '{query_text}' ---\")\n",
    "\n",
    "    query_embedding = embed_model.get_text_embedding(query_text)\n",
    "\n",
    "    try:\n",
    "        response = s3v.query_vectors(\n",
    "            vectorBucketName=VECTOR_BUCKET_NAME,\n",
    "            indexName=INDEX_NAME,\n",
    "            topK=top_k,\n",
    "            queryVector={\n",
    "                'float32': query_embedding\n",
    "            },\n",
    "            returnMetadata=True,\n",
    "            returnDistance=True\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying S3 Vector Store: {e}\")\n",
    "        return None\n",
    "\n",
    "user_questions = [\n",
    "    \"What obligations does Cotiviti have under Schedule C for Prepay FWAV Services?\",\n",
    "    \"Under Schedule C, what services is Cotiviti required to provide?\",\n",
    "    \"In the Prepay FWAV Services section, what are Cotiviti's main deliverables?\",\n",
    "]\n",
    "\n",
    "\n",
    "INDEX_NAME = \"token-chunking\"\n",
    "# INDEX_NAME = \"overlap-chunking\"\n",
    "# INDEX_NAME = \"semantic-split-chunking\"\n",
    "\n",
    "for question in user_questions:\n",
    "    query_results = query_s3_vector_store(question, INDEX_NAME, top_k=5)\n",
    "\n",
    "    if query_results and 'vectors' in query_results:\n",
    "        print(f\"Retrieved {len(query_results['vectors'])}\")\n",
    "        for i, chunk_data in enumerate(query_results['vectors']):\n",
    "            # print(f\"  Chunk {i+1}:\")\n",
    "            # print(f\"    Text: {chunk_data['metadata'].get('text', 'No text found in metadata')[:50]}...\")\n",
    "            print(f\"    S3_Path {i+1}: {chunk_data['metadata'].get('s3_path', 'No S3 path found')}\")\n",
    "            # print(f\"    Distance: {chunk_data.get('distance', 'N/A')}\")\n",
    "    else:\n",
    "        print(f\"No results or an error occurred for query: '{question}'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
