{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c198fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# !pip install --upgrade boto3==1.39.8\n",
    "# !sudo apt install antiword \n",
    "import io\n",
    "import uuid\n",
    "import os\n",
    "import random\n",
    "import nltk\n",
    "import boto3\n",
    "import requests\n",
    "import json\n",
    "import pytesseract\n",
    "import textract\n",
    "from pdf2image import convert_from_bytes\n",
    "from PyPDF2 import PdfReader\n",
    "from docx import Document\n",
    "from io import BytesIO\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import extract_msg\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.schema import TextNode, NodeRelationship, RelatedNodeInfo\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# embed_model = AutoModel.from_pretrained(\"intfloat/e5-small-v2\")\n",
    "\n",
    "S3_BUCKET = \"ml-legal-restricted\"\n",
    "EXCEL_PATH = \"full_contracts_with_files.xlsx\"\n",
    "VECTOR_BUCKET_NAME = \"legal-docs-vector-store\"\n",
    "EMBEDINNGS_URL = \"https://zgggzg2iqg.execute-api.us-east-1.amazonaws.com/dev/get_embeddings\"\n",
    "API_KEY = \"2jIpWCyNRg3Y8lkbmWG0tkyXwYlJn5QaZ1F3yKf7\"\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "s3 = boto3.client('s3')\n",
    "s3v = boto3.client(\"s3vectors\", region_name=\"us-east-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe3f0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_excel(bucket, key, sheet_name=\"Active Legal Contracts\", column=\"Contract Number\"):\n",
    "    print(f\"üì• Downloading Excel: s3://{bucket}/{key}\")\n",
    "    obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "    excel_data = obj['Body'].read()\n",
    "\n",
    "    df = pd.read_excel(io.BytesIO(excel_data), sheet_name=sheet_name, engine='openpyxl')\n",
    "    if column not in df.columns:\n",
    "        raise ValueError(f\"Column '{column}' not found in sheet '{sheet_name}'\")\n",
    "\n",
    "    contract_numbers = df[column].dropna().astype(str).str.strip().tolist()\n",
    "    return contract_numbers, df, excel_data\n",
    "\n",
    "\n",
    "def list_s3_files_for_contract(bucket, contract_number, prefix_base=\"contract-docs/\"):\n",
    "    prefix = f\"{prefix_base}{contract_number}/\"\n",
    "    files = []\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "\n",
    "    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "        for obj in page.get(\"Contents\", []):\n",
    "            files.append(obj[\"Key\"])\n",
    "    return files\n",
    "\n",
    "\n",
    "def add_modified_sheet_with_files(wb, original_df, file_map):\n",
    "    ws = wb.create_sheet(\"Active Legal Contracts + Files\")\n",
    "    headers = original_df.columns.tolist()\n",
    "    max_files = max((len(files) for files in file_map.values()), default=0)\n",
    "    headers += [f\"File {i+1}\" for i in range(max_files)]\n",
    "    ws.append(headers)\n",
    "\n",
    "    for idx, row in original_df.iterrows():\n",
    "        contract_number = str(row[\"Contract Number\"]).strip()\n",
    "        files = [os.path.basename(f) for f in file_map.get(contract_number, [])]\n",
    "        base_row = row.tolist()\n",
    "        ws.append(base_row + files)\n",
    "\n",
    "\n",
    "def add_s3_paths_sheet(wb, file_map, bucket):\n",
    "    ws = wb.create_sheet(\"S3 File Paths\")\n",
    "    max_files = max((len(files) for files in file_map.values()), default=0)\n",
    "\n",
    "    headers = [\"Contract Number\"] + [f\"S3 File {i+1}\" for i in range(max_files)]\n",
    "    ws.append(headers)\n",
    "\n",
    "    for contract, keys in file_map.items():\n",
    "        s3_paths = [f\"s3://{bucket}/{key}\" for key in keys]\n",
    "        ws.append([contract] + s3_paths)\n",
    "\n",
    "def save_missing_contracts_to_csv(missing_contracts, output_path=\"missing_contracts.csv\"):\n",
    "    if not missing_contracts:\n",
    "        print(\"‚úÖ No missing contracts to save.\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(missing_contracts, columns=[\"Contract Number\"])\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"üìÑ Missing contracts CSV saved to: {output_path}\")\n",
    "    \n",
    "def process_entire_bucket(bucket, excel_key, output_path=\"full_contracts_with_files.xlsx\"):\n",
    "    contract_numbers, original_df, excel_bytes = download_excel(bucket, excel_key)\n",
    "\n",
    "    file_map = {}\n",
    "    missing_contracts = []\n",
    "\n",
    "    print(f\"\\nüîç Scanning {len(contract_numbers)} contract numbers across S3...\")\n",
    "    for idx, number in enumerate(contract_numbers, 1):\n",
    "        if idx % 1000 == 0 or idx == 1:\n",
    "            print(f\"üî¢ [{idx}/{len(contract_numbers)}] Scanning: {number}\")\n",
    "        files = list_s3_files_for_contract(bucket, number)\n",
    "\n",
    "        if not files and len(number) < 8 and number.isdigit():\n",
    "            padded = number.zfill(8)\n",
    "            print(f\"   ‚ûï Retrying with padded contract number: {padded}\")\n",
    "            files = list_s3_files_for_contract(bucket, padded)\n",
    "            \n",
    "        if files:\n",
    "            file_map[number] = files\n",
    "        else:\n",
    "            missing_contracts.append(number)\n",
    "\n",
    "    print(\"\\nüßæ Preparing final Excel workbook...\")\n",
    "    wb = load_workbook(io.BytesIO(excel_bytes))\n",
    "    add_modified_sheet_with_files(wb, original_df, file_map)\n",
    "    add_s3_paths_sheet(wb, file_map, bucket)\n",
    "    wb.save(output_path)\n",
    "    print(f\"‚úÖ Final Excel saved: {output_path}\")\n",
    "\n",
    "    print(\"\\n=== Final Summary ===\")\n",
    "    print(f\"üìÑ Total contracts processed: {len(contract_numbers)}\")\n",
    "    print(f\"‚úÖ Contracts with files: {len(file_map)}\")\n",
    "    print(f\"‚ùå Contracts with NO files found: {len(missing_contracts)}\")\n",
    "    if missing_contracts:\n",
    "        print(f\"üîç Sample missing contract numbers: {missing_contracts[:5]}{'...' if len(missing_contracts) > 5 else ''}\")\n",
    "        save_missing_contracts_to_csv(missing_contracts)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    bucket = \"ml-legal-restricted\"\n",
    "    excel_key = \"tabularData/Active Legal Contracts 7-10-2025 1-17-09 PM.xlsx\"\n",
    "    process_entire_bucket(bucket, excel_key, output_path=\"full_contracts_with_files.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1ef4e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_text_embedding(texts, model='e5_mistral_embed_384'):\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "        \n",
    "    if not isinstance(texts, list) or not texts:\n",
    "        raise ValueError(\"Input 'texts' must be a non-empty list of strings.\")\n",
    "\n",
    "    embeddings = []\n",
    "\n",
    "    for text in texts:\n",
    "        if not isinstance(text, str):\n",
    "            raise ValueError(\"Each item in 'texts' must be a string.\")\n",
    "\n",
    "        payload = {\n",
    "            \"model_name\": model,\n",
    "            \"texts\": [text]\n",
    "        }\n",
    "\n",
    "        headers = {\n",
    "        \"x-api-key\": API_KEY\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.post(EMBEDINNGS_URL, json=payload, headers=headers)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            raw_body = response.json().get('body')\n",
    "\n",
    "            parsed_body = json.loads(raw_body)\n",
    "\n",
    "            embedding = parsed_body.get('embeddings')\n",
    "            if not embedding or not isinstance(embedding, list) or len(embedding) != 1:\n",
    "                raise KeyError(f\"No valid embedding found in response for text: '{text}'\")\n",
    "\n",
    "            # embeddings.append(embedding[0])\n",
    "            embedding_float32 = np.array(embedding[0], dtype=np.float32).tolist()\n",
    "            embeddings.append(embedding_float32)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to get embedding!\") #for '{text}': {e}\")\n",
    "            embeddings.append(None)\n",
    "\n",
    "    return embeddings[0] if len(embeddings) == 1 else embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "993c62e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created index: {'ResponseMetadata': {'RequestId': '07311431-3181-46c0-b4e3-d53fcbe4c85f', 'HostId': '', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 25 Jul 2025 01:33:49 GMT', 'content-type': 'application/json', 'content-length': '2', 'connection': 'keep-alive', 'x-amz-request-id': '07311431-3181-46c0-b4e3-d53fcbe4c85f', 'access-control-allow-origin': '*', 'vary': 'origin, access-control-request-method, access-control-request-headers', 'access-control-expose-headers': '*'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "# to recreate vector index store\n",
    "\n",
    "# INDEX_NAME = \"token-chunking\"\n",
    "# INDEX_NAME = \"overlap-chunking\"\n",
    "# INDEX_NAME = \"semantic-split-chunking\"\n",
    "INDEX_NAME = 'token-chunking-poc'\n",
    "VECTOR_DIM = 384\n",
    "DISTANCE_METRIC = \"cosine\"\n",
    "NON_FILTERABLE_KEYS = ['text']\n",
    "\n",
    "response = s3v.delete_index(\n",
    "    vectorBucketName=VECTOR_BUCKET_NAME,\n",
    "    indexName=INDEX_NAME,\n",
    ")\n",
    "\n",
    "response = s3v.create_index(\n",
    "    vectorBucketName=VECTOR_BUCKET_NAME,\n",
    "    indexName=INDEX_NAME,\n",
    "    dataType=\"float32\",\n",
    "    dimension=VECTOR_DIM,\n",
    "    distanceMetric=DISTANCE_METRIC,\n",
    "    metadataConfiguration={\n",
    "        \"nonFilterableMetadataKeys\": NON_FILTERABLE_KEYS\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Created index: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78836f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, metadata, token_limit=400, tokenizer_name=\"intfloat/e5-small-v2\"):\n",
    "    \n",
    "    s3_path = metadata.get(\"s3_path\", \"\")\n",
    "    file_name = os.path.basename(s3_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks, current, count = [], [], 0\n",
    "\n",
    "    for sent in sentences:\n",
    "        toks = tokenizer.tokenize(sent)\n",
    "        if count + len(toks) > token_limit and current:\n",
    "            chunks.append(\" \".join(current))\n",
    "            current, count = [], 0\n",
    "        current.append(sent)\n",
    "        count += len(toks)\n",
    "\n",
    "    if current:\n",
    "        chunks.append(\" \".join(current))\n",
    "\n",
    "    return [\n",
    "        {\n",
    "            \"key\": str(uuid.uuid4()),\n",
    "            \"metadata\": {\n",
    "                \"text\": chunk,\n",
    "                \"file_name\": file_name,\n",
    "                \"s3_path\": s3_path,\n",
    "                \"client_account\": metadata.get(\"client_account\"),\n",
    "                \"document_type\": metadata.get(\"document_type\")\n",
    "            }\n",
    "        }\n",
    "        for chunk in chunks\n",
    "    ]\n",
    "    \n",
    "\n",
    "def chunk_text_with_overlap(text, metadata, token_limit=400, chunk_overlap=80, tokenizer_name=\"intfloat/e5-small-v2\"):\n",
    "    \n",
    "    s3_path = metadata.get(\"s3_path\", \"\")\n",
    "    file_name = os.path.basename(s3_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "    sentences, chunks, curr, curr_toks = sent_tokenize(text), [], [], 0\n",
    "\n",
    "    for sent in sentences:\n",
    "        stoks = len(tokenizer.tokenize(sent))\n",
    "        if stoks > token_limit:\n",
    "            words = sent.split()\n",
    "            i = 0\n",
    "            while i < len(words):\n",
    "                segment = words[i:i+token_limit]\n",
    "                chunks.append(\" \".join(segment))\n",
    "                i += token_limit - chunk_overlap\n",
    "            continue\n",
    "\n",
    "        if curr_toks + stoks <= token_limit:\n",
    "            curr.append(sent); curr_toks += stoks\n",
    "        else:\n",
    "            chunks.append(\" \".join(curr))\n",
    "            # build overlap\n",
    "            overlap, tot = [], 0\n",
    "            for s in reversed(curr):\n",
    "                l = len(tokenizer.tokenize(s))\n",
    "                if tot + l > chunk_overlap:\n",
    "                    break\n",
    "                overlap.insert(0, s); tot += l\n",
    "            curr = overlap + [sent]\n",
    "            curr_toks = sum(len(tokenizer.tokenize(s)) for s in curr)\n",
    "\n",
    "    if curr:\n",
    "        chunks.append(\" \".join(curr))\n",
    "    \n",
    "    return [\n",
    "        {\n",
    "            \"key\": str(uuid.uuid4()),\n",
    "            \"metadata\": {\n",
    "                \"text\": chunk,\n",
    "                \"file_name\": file_name,\n",
    "                \"s3_path\": s3_path,\n",
    "                \"client_account\": metadata.get(\"client_account\"),\n",
    "                \"document_type\": metadata.get(\"document_type\")\n",
    "            }\n",
    "        }\n",
    "        for chunk in chunks\n",
    "    ]\n",
    "\n",
    "#TODO ebedding model integgratiom\n",
    "def chunk_with_semantic_split(text, data, buffer_size=1, breakpoint_percentile_threshold=95):\n",
    "\n",
    "    from llama_index.core import Document\n",
    "\n",
    "    s3_path = data.get(\"s3_path\", \"\")\n",
    "    metadata = {\n",
    "        \"s3_path\": s3_path,\n",
    "        \"file_name\": os.path.basename(s3_path),\n",
    "        \"client_account\": data.get(\"client_account\"),\n",
    "        \"document_type\": data.get(\"document_type\")\n",
    "                }\n",
    "    doc = Document(text=text, metadata=metadata)\n",
    "\n",
    "    parser = SemanticSplitterNodeParser.from_defaults(\n",
    "        embed_model=embed_model, #need to fix this\n",
    "        buffer_size=buffer_size,\n",
    "        breakpoint_percentile_threshold=breakpoint_percentile_threshold,\n",
    "        include_metadata=False,\n",
    "        include_prev_next_rel=False\n",
    "    )\n",
    "\n",
    "    nodes = parser.get_nodes_from_documents([doc])\n",
    "    texts = [n.text for n in nodes]\n",
    "\n",
    "    result = []\n",
    "    for n in nodes:\n",
    "        unique_id = str(uuid.uuid4())\n",
    "        result.append({\n",
    "            \"key\": unique_id,\n",
    "            \"metadata\": {\n",
    "                \"text\": n.text,\n",
    "                \"s3_path\": s3_path,\n",
    "                \"file_name\": os.path.basename(s3_path)\n",
    "            },\n",
    "        })\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "501fb485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Rows included after filtering (S3_vectors != True): 48\n",
      "\n",
      "üìÑ Processing 1/48: contract-docs/52910/1199 ASP Upsell Financial Summary.xlsx\n",
      "‚ö†Ô∏è No text extracted.\n",
      "\n",
      "üìÑ Processing 2/48: contract-docs/76438/3M License Agreement_Lorica Health_Codefinder_GPCS_ 29th March 2023 to 28th March 2026_Final.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (836 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Failed to get embedding!\n",
      "‚ùó Error: 'NoneType' object is not subscriptable\n",
      "\n",
      "üìÑ Processing 3/48: contract-docs/61807/BAA to Master Software and Service Agreement (Exhibit A).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (908 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Failed to get embedding!\n",
      "‚ùó Error: 'NoneType' object is not subscriptable\n",
      "\n",
      "üìÑ Processing 4/48: contract-docs/60590/Coviti_CTR55948-20_NEW_3_Year_v6_MJG_9.8.2020_EK.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (908 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Failed to get embedding!\n",
      "‚ùó Error: 'NoneType' object is not subscriptable\n",
      "\n",
      "üìÑ Processing 5/48: contract-docs/58822/AAH.05 Professional Medical Coding Instructor License Agreement between Cotiviti and AAPC(712376.3).docx\n",
      "‚ùó DOCX error: \"There is no item named 'customXML/item3.xml' in the archive\"\n",
      "‚ùó OCR failed: Unable to get page count.\n",
      "Syntax Warning: May not be a PDF file (continuing anyway)\n",
      "Syntax Error: Couldn't find trailer dictionary\n",
      "Syntax Error: Couldn't find trailer dictionary\n",
      "Syntax Error: Couldn't read xref table\n",
      "\n",
      "‚ö†Ô∏è No text extracted.\n",
      "\n",
      "üìÑ Processing 6/48: contract-docs/59807/EXTERNAL EMAIL - USE CAUTION FW Cotiviti - AArete FFP SOW Termination Notice.htm\n",
      "‚ö†Ô∏è No text extracted.\n",
      "\n",
      "üìÑ Processing 7/48: contract-docs/51772/Verscend PAF 6-05-18 (AArete).xlsx\n",
      "‚ö†Ô∏è No text extracted.\n",
      "\n",
      "üìÑ Processing 8/48: contract-docs/53624/AK Steel - Verscend - NDA - JSW - 082018 - Executed DX 53624.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (529 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Failed to get embedding!\n",
      "‚ùó Error: 'NoneType' object is not subscriptable\n",
      "\n",
      "üìÑ Processing 9/48: contract-docs/53624/AK Steel Verscend NDA jsw 8.16.18 FINAL_signed.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (529 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Failed to get embedding!\n",
      "‚ùó Error: 'NoneType' object is not subscriptable\n",
      "\n",
      "üìÑ Processing 10/48: contract-docs/58512/ASA_2021 Crosswalk eFile_Quote_.docx\n",
      "‚ùó OCR failed: Unable to get page count.\n",
      "Syntax Warning: May not be a PDF file (continuing anyway)\n",
      "Syntax Error: Couldn't find trailer dictionary\n",
      "Syntax Error: Couldn't find trailer dictionary\n",
      "Syntax Error: Couldn't read xref table\n",
      "\n",
      "‚ö†Ô∏è No text extracted.\n",
      "\n",
      "üìÑ Processing 11/48: contract-docs/58512/ASA_3PCompliance.docx\n",
      "‚ùó OCR failed: Unable to get page count.\n",
      "Syntax Warning: May not be a PDF file (continuing anyway)\n",
      "Syntax Error: Couldn't find trailer dictionary\n",
      "Syntax Error: Couldn't find trailer dictionary\n",
      "Syntax Error: Couldn't read xref table\n",
      "\n",
      "‚ö†Ô∏è No text extracted.\n",
      "\n",
      "üìÑ Processing 12/48: contract-docs/58512/ASA_3PSecurity.docx\n",
      "‚ùó OCR failed: Unable to get page count.\n",
      "Syntax Warning: May not be a PDF file (continuing anyway)\n",
      "Syntax Error: Couldn't find trailer dictionary\n",
      "Syntax Error: Couldn't find trailer dictionary\n",
      "Syntax Error: Couldn't read xref table\n",
      "\n",
      "‚ö†Ô∏è No text extracted.\n",
      "\n",
      "üìÑ Processing 13/48: contract-docs/80476/ATT_BA Terms_Final_07122012_Executed.tif\n",
      "‚ö†Ô∏è No text extracted.\n",
      "\n",
      "üìÑ Processing 14/48: contract-docs/52124/Verscend PAF_Ablebits_4.30.18.xlsx\n",
      "‚ö†Ô∏è No text extracted.\n",
      "\n",
      "üìÑ Processing 15/48: contract-docs/59859/Abstrakti-Software_3P_Short_Form_V8.1.xlsx\n",
      "‚ö†Ô∏è No text extracted.\n",
      "\n",
      "üìÑ Processing 16/48: contract-docs/58338/TMHP Accenture - Signed Cotiviti Letter 2019 12 16.html\n",
      "‚ö†Ô∏è No text extracted.\n",
      "\n",
      "üìÑ Processing 17/48: contract-docs/69060/Accurate PUR Review 2022.07.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5094 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Failed to get embedding!\n",
      "‚ùó Error: 'NoneType' object is not subscriptable\n",
      "\n",
      "üìÑ Processing 18/48: contract-docs/69061/Accurate PUR Review 2022.07.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5094 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Failed to get embedding!\n",
      "‚ùó Error: 'NoneType' object is not subscriptable\n",
      "\n",
      "üìÑ Processing 19/48: contract-docs/72101/Cotiviti_Proposal_2023-0509_Cotiviti Edits_v2.pptx\n",
      "‚ö†Ô∏è No text extracted.\n",
      "\n",
      "üìÑ Processing 20/48: contract-docs/56974/Actian - Cotiviti - Q - Actian Corporation (subscription renewal for two years) 6.30.19 - SC - 06272019 - 56974.pdf'\n",
      "‚ö†Ô∏è No text extracted.\n",
      "\n",
      "üìÑ Processing 21/48: contract-docs/56911/Actian - Cotiviti - Name Change Amendment - SC - 06272019 - 56911.pdf'\n",
      "‚ö†Ô∏è No text extracted.\n",
      "\n",
      "üìÑ Processing 22/48: contract-docs/78744/_CPIG_FINAL_Short_Form_Findings for Birch Grove (R-03129) - Feb 2025.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (30982 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Failed to get embedding!\n",
      "‚ùó Error: 'NoneType' object is not subscriptable\n",
      "\n",
      "üìÑ Processing 23/48: contract-docs/53757/ACG 3rd Fl data-2  PAF 090518.xlsx\n",
      "‚ö†Ô∏è No text extracted.\n",
      "\n",
      "üìÑ Processing 24/48: contract-docs/52628/Draper Parking Lights PAF 062218.xlsx\n",
      "‚ö†Ô∏è No text extracted.\n",
      "\n",
      "üìÑ Processing 25/48: contract-docs/52627/SoJo 3rd Fl data runs PAF 062218.xlsx\n",
      "‚ö†Ô∏è No text extracted.\n",
      "\n",
      "üìÑ Processing 26/48: contract-docs/51879/Verscend PAF_ACG_Fiber_Draper_3.22.18.xlsx\n",
      "‚ö†Ô∏è No text extracted.\n",
      "\n",
      "üìÑ Processing 27/48: contract-docs/51901/ACG Audio Visual Relocation PAF040418.xlsx\n",
      "‚ö†Ô∏è No text extracted.\n",
      "\n",
      "üìÑ Processing 28/48: contract-docs/51883/Verscend PAF_ACG_CableManagement_3.28.18.xlsx\n",
      "‚ö†Ô∏è No text extracted.\n",
      "\n",
      "üìÑ Processing 29/48: contract-docs/51761/Verscend PAF_ACG_Fiber_3.14.18.xlsx\n",
      "‚ö†Ô∏è No text extracted.\n",
      "\n",
      "üìÑ Processing 30/48: contract-docs/74838/DM-BR Teams Exchange 8.22.24.png\n",
      "‚ö†Ô∏è No text extracted.\n",
      "\n",
      "üìÑ Processing 31/48: contract-docs/53934/Verscend PAF Check paper User Form XL v18 9-16-18.xlsx\n",
      "‚ö†Ô∏è No text extracted.\n",
      "\n",
      "üìÑ Processing 32/48: contract-docs/60088/Approval Aerotek SOW - Theresa Walker 2020.docx\n",
      "‚ùó OCR failed: Unable to get page count.\n",
      "Syntax Warning: May not be a PDF file (continuing anyway)\n",
      "Syntax Error: Couldn't find trailer dictionary\n",
      "Syntax Error: Couldn't find trailer dictionary\n",
      "Syntax Error: Couldn't read xref table\n",
      "\n",
      "‚ö†Ô∏è No text extracted.\n",
      "\n",
      "üìÑ Processing 33/48: contract-docs/59919/Approval Aerotek SOW - Anita Mason.PNG\n",
      "‚ö†Ô∏è No text extracted.\n",
      "\n",
      "üìÑ Processing 34/48: contract-docs/59533/Approval - TEMPO03948 (Aerotek).png\n",
      "‚ö†Ô∏è No text extracted.\n",
      "\n",
      "üìÑ Processing 35/48: contract-docs/59532/Approval - req TEMPO03949 (Peopleshare & Aerotek).png\n",
      "‚ö†Ô∏è No text extracted.\n",
      "\n",
      "üìÑ Processing 36/48: contract-docs/59531/Approval - req TEMPO03949 (Peopleshare & Aerotek).png\n",
      "‚ö†Ô∏è No text extracted.\n",
      "\n",
      "üìÑ Processing 37/48: contract-docs/59530/Approval - req TEMPO03949 (Peopleshare & Aerotek).png\n",
      "‚ö†Ô∏è No text extracted.\n",
      "\n",
      "üìÑ Processing 38/48: contract-docs/56916/Names in the Approval vs SOW Aerotek.docx\n",
      "‚ùó OCR failed: Unable to get page count.\n",
      "Syntax Warning: May not be a PDF file (continuing anyway)\n",
      "Syntax Error: Couldn't find trailer dictionary\n",
      "Syntax Error: Couldn't find trailer dictionary\n",
      "Syntax Error: Couldn't read xref table\n",
      "\n",
      "‚ö†Ô∏è No text extracted.\n",
      "\n",
      "üìÑ Processing 39/48: contract-docs/56898/Aerotek-Amendment_Ready for signatures_(MV_06.19.19).rtf\n",
      "‚ö†Ô∏è No text extracted.\n",
      "\n",
      "üìÑ Processing 40/48: contract-docs/55830/Aerotek SOW Deneia Washington Req approval Audit Quality Specialist.docx\n",
      "‚ùó OCR failed: Unable to get page count.\n",
      "Syntax Warning: May not be a PDF file (continuing anyway)\n",
      "Syntax Error: Couldn't find trailer dictionary\n",
      "Syntax Error: Couldn't find trailer dictionary\n",
      "Syntax Error: Couldn't read xref table\n",
      "\n",
      "‚ö†Ô∏è No text extracted.\n",
      "\n",
      "üìÑ Processing 41/48: contract-docs/75543/CVS_Cotiviti_DRL_20240827.xlsx\n",
      "‚ö†Ô∏è No text extracted.\n",
      "\n",
      "üìÑ Processing 42/48: contract-docs/68779/Copy of DD_2022-077_Data_Checklist_Approval.xlsx\n",
      "‚ö†Ô∏è No text extracted.\n",
      "\n",
      "üìÑ Processing 43/48: contract-docs/68660/Eliza OhioRise Compliance Approval.PNG\n",
      "‚ö†Ô∏è No text extracted.\n",
      "\n",
      "üìÑ Processing 44/48: contract-docs/57383/'C-Amend-022-LA-Cotiviti(iHT)-2002 (bd 10-18-19) gpt.docx'\n",
      "‚ö†Ô∏è No text extracted.\n",
      "\n",
      "üìÑ Processing 45/48: contract-docs/53367/Aetna CV SOW - Proposed Amendment Items 07272018 .docx\n",
      "‚ùó OCR failed: Unable to get page count.\n",
      "Syntax Warning: May not be a PDF file (continuing anyway)\n",
      "Syntax Error: Couldn't find trailer dictionary\n",
      "Syntax Error: Couldn't find trailer dictionary\n",
      "Syntax Error: Couldn't read xref table\n",
      "\n",
      "‚ö†Ô∏è No text extracted.\n",
      "\n",
      "üìÑ Processing 46/48: contract-docs/66603/47927_Aetna SOW_7_Comprehensive_Diabetes_Program 111611.docm\n",
      "‚ö†Ô∏è No text extracted.\n",
      "\n",
      "üìÑ Processing 47/48: contract-docs/81252/DM-JG Teams Re Signatory_7.10.25.png\n",
      "‚ö†Ô∏è No text extracted.\n",
      "\n",
      "üìÑ Processing 48/48: contract-docs/78953/Aetna Life Insurance Company - Cotiviti - Termination - DM- 032425 - Dynamics_78953.docx\n",
      "‚ùó OCR failed: Unable to get page count.\n",
      "Syntax Warning: May not be a PDF file (continuing anyway)\n",
      "Syntax Error: Couldn't find trailer dictionary\n",
      "Syntax Error: Couldn't find trailer dictionary\n",
      "Syntax Error: Couldn't read xref table\n",
      "\n",
      "‚ö†Ô∏è No text extracted.\n",
      "\n",
      "=== Summary ===\n",
      "Processed: 0  Failed: 48\n",
      "Pdf: 0  Docx: 0  Txt: 0  Msg: 0  Doc: 0  Ocr: 0  None: 48\n"
     ]
    }
   ],
   "source": [
    "SUPPORTED_EXTENSIONS = {\".pdf\", \".docx\", \".txt\", \".msg\", \".doc\"}\n",
    "\n",
    "def list_supported_files(bucket, prefix=\"\"):\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "    all_files, supported, unsupported = [], [], []\n",
    "\n",
    "    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "        for obj in page.get(\"Contents\", []):\n",
    "            key = obj[\"Key\"]\n",
    "            all_files.append(key)\n",
    "            ext = os.path.splitext(key)[1].lower()\n",
    "            if ext in SUPPORTED_EXTENSIONS:\n",
    "                supported.append(key)\n",
    "            else:\n",
    "                unsupported.append(key)\n",
    "\n",
    "    print(\"\\n=== File Summary ===\")\n",
    "    print(f\"üìÅ Total files: {len(all_files)}\")\n",
    "    print(f\"‚úÖ Supported files: {len(supported)}\")\n",
    "    print(f\"‚ùå Unsupported: {len(unsupported)} (Sample: {unsupported[:5]})\\n\")\n",
    "    return supported\n",
    "\n",
    "def download_s3_file(bucket, key):\n",
    "    response = s3.get_object(Bucket=bucket, Key=key)\n",
    "    return io.BytesIO(response[\"Body\"].read())\n",
    "\n",
    "def extract_from_doc(file_io):\n",
    "    try:\n",
    "        file_io.seek(0)\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".doc\") as temp_file:\n",
    "            temp_file.write(file_io.read())\n",
    "            temp_path = temp_file.name\n",
    "\n",
    "        text = textract.process(temp_path).decode(\"utf-8\").strip()\n",
    "        os.remove(temp_path)\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùó DOC (textract) error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def extract_with_ocr(file_io):\n",
    "    try:\n",
    "        file_io.seek(0)\n",
    "        images = convert_from_bytes(file_io.read())\n",
    "        return \"\\n\".join(pytesseract.image_to_string(img) for img in images).strip()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùó OCR failed: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_from_pdf(file_io):\n",
    "    try:\n",
    "        reader = PdfReader(file_io)\n",
    "        return \"\\n\".join([p.extract_text() for p in reader.pages if p.extract_text()]).strip()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùó PDF read error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def extract_from_docx(file_io):\n",
    "    from docx import Document\n",
    "    from io import BytesIO\n",
    "\n",
    "    try:\n",
    "        if hasattr(file_io, \"read\"):\n",
    "            file_io.seek(0)\n",
    "            content = file_io.read()\n",
    "            bio = BytesIO(content)\n",
    "            \n",
    "            doc = Document(bio)\n",
    "        else:\n",
    "\n",
    "            bio = BytesIO(file_io)\n",
    "            doc = Document(bio)\n",
    "            \n",
    "        text = \"\\n\".join(p.text for p in doc.paragraphs).strip()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùó DOCX error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def extract_from_txt(file_io):\n",
    "    try:\n",
    "        return file_io.read().decode(\"utf-8\").strip()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùó TXT read error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_from_msg(file_io):\n",
    "    try:\n",
    "        with open(\"temp.msg\", \"wb\") as f:\n",
    "            f.write(file_io.read())\n",
    "        msg = extract_msg.Message(\"temp.msg\")\n",
    "        text = msg.body or \"\"\n",
    "        os.remove(\"temp.msg\")\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùó MSG read error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_text(file_io, ext):\n",
    "    ext = ext.lower()\n",
    "    \n",
    "    file_io.seek(0)\n",
    "    sig = file_io.read(4)\n",
    "    file_io.seek(0)\n",
    "    \n",
    "    extractors = {\n",
    "        \".pdf\": [\n",
    "            extract_from_pdf,\n",
    "            extract_with_ocr\n",
    "        ],\n",
    "        \".docx\": [\n",
    "            extract_from_docx,\n",
    "            extract_with_ocr\n",
    "        ],\n",
    "        \".doc\": [extract_from_doc],\n",
    "        \".txt\": [extract_from_txt,],\n",
    "        \".msg\": [extract_from_msg],\n",
    "    }\n",
    "\n",
    "    for extractor in extractors.get(ext, []):\n",
    "        file_io.seek(0)\n",
    "        text = extractor(file_io)\n",
    "        if text:\n",
    "            return text, extractor.__name__\n",
    "\n",
    "    return \"\", \"none\"\n",
    "\n",
    "\n",
    "def upload_chunks_to_s3_vector_index(chunks, vector_bucket_name, INDEX_NAME):\n",
    "\n",
    "    MAX_BATCH_SIZE = 500\n",
    "    texts = [chunk[\"metadata\"][\"text\"] for chunk in chunks]\n",
    "    keys = [chunk[\"key\"] for chunk in chunks]\n",
    "\n",
    "    embeddings = get_text_embedding(texts)\n",
    "    if embeddings and isinstance(embeddings[0], float):\n",
    "        embeddings = [embeddings]\n",
    "\n",
    "    vectors = []\n",
    "    for i in range(len(chunks)):\n",
    "        if embeddings[i] == None:\n",
    "            continue\n",
    "        vector_metadata = chunks[i][\"metadata\"].copy()\n",
    "        vectors.append({\n",
    "            \"key\": keys[i],\n",
    "            \"data\": {\"float32\": embeddings[i]},\n",
    "            \"metadata\": vector_metadata\n",
    "        })\n",
    "\n",
    "    responses = []\n",
    "    for i in range(0, len(vectors), MAX_BATCH_SIZE):\n",
    "        # print(f\"Uploading batch {i // MAX_BATCH_SIZE + 1} of {((len(vectors) - 1) // MAX_BATCH_SIZE + 1)}\")\n",
    "        batch = vectors[i:i + MAX_BATCH_SIZE]\n",
    "        response = s3v.put_vectors(\n",
    "            vectorBucketName=vector_bucket_name,\n",
    "            indexName=INDEX_NAME,\n",
    "            vectors=batch\n",
    "        )\n",
    "        responses.append(response)\n",
    "\n",
    "    return responses\n",
    "\n",
    "\n",
    "def mark_csv_file_processed(file_key, bucket=\"ml-legal-restricted\", csv_path=\"gathered_contract_files.csv\"):\n",
    "    full_s3_path = f\"s3://{bucket}/{file_key}\"\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    match_idx = df[df['S3_full_path'] == full_s3_path].index\n",
    "\n",
    "    if not match_idx.empty:\n",
    "        df.loc[match_idx, 'S3_vectors'] = True\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def process_documents(bucket, items, INDEX_NAME):\n",
    "    stats = {\n",
    "        \"processed\": 0, \"failed\": 0,\n",
    "        \"pdf\": 0, \"docx\": 0, \"txt\": 0, \"msg\": 0, \"doc\": 0,\n",
    "        \"ocr\": 0, \"none\": 0\n",
    "    }\n",
    "\n",
    "    all_chunks = []\n",
    "\n",
    "    for idx, item  in enumerate(items, 1):\n",
    "        file_key = item[\"file_key\"]\n",
    "        metadata = item.get(\"metadata\", {}).copy()\n",
    "\n",
    "        print(f\"\\nüìÑ Processing {idx}/{len(items)}: {file_key}\")\n",
    "        ext = os.path.splitext(file_key)[1].lower()\n",
    "        metadata[\"s3_path\"] = f\"s3://{bucket}/{file_key}\"\n",
    "\n",
    "        try:\n",
    "            file_io = download_s3_file(bucket, file_key)\n",
    "            text, method = extract_text(file_io, ext)\n",
    "\n",
    "            if not text:\n",
    "                print(\"‚ö†Ô∏è No text extracted.\")\n",
    "                stats[\"failed\"] += 1\n",
    "                stats[\"none\"] += 1\n",
    "                continue\n",
    "            \n",
    "            if INDEX_NAME == 'token-chunking':\n",
    "                chunks = chunk_text(text, metadata)\n",
    "            elif INDEX_NAME == 'overlap-chunking':\n",
    "                chunks = chunk_text_with_overlap(text, metadata)\n",
    "            elif INDEX_NAME == 'semantic-split-chunking':\n",
    "                chunks = chunk_with_semantic_split(text, metadata)\n",
    "            else:\n",
    "                chunks = chunk_text(text, metadata)\n",
    "\n",
    "            # sample = chunks[0]['text'][:80].replace('\\n', ' ')\n",
    "            # sample = chunks\n",
    "            # print(f\"üìù Sample: {sample}...\")\n",
    "\n",
    "            # all_chunks.extend(chunks)\n",
    "            response = upload_chunks_to_s3_vector_index(chunks, VECTOR_BUCKET_NAME, INDEX_NAME)\n",
    "            if response:\n",
    "                marked = mark_csv_file_processed(file_key)\n",
    "                if marked:\n",
    "                    print(f\"‚úÖ Extracted {len(chunks)} and Successfully uploaded to S3 vector Index and Marked in CSV\")\n",
    "            else:\n",
    "                print(f\"‚ùóError: Extracted {len(chunks)} and unable to upload to S3 vector Index\")\n",
    "            \n",
    "            stats[\"processed\"] += 1\n",
    "            stats[ext.replace(\".\", \"\")] += 1\n",
    "            if \"ocr\" in method: stats[\"ocr\"] += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùó Error: {e}\")\n",
    "            stats[\"failed\"] += 1\n",
    "            stats[\"none\"] += 1\n",
    "\n",
    "    print(\"\\n=== Summary ===\")\n",
    "    first_keys = ['processed', 'failed']\n",
    "    first_line = []\n",
    "    second_line = []\n",
    "\n",
    "    for k, v in stats.items():\n",
    "        key_formatted = k.capitalize().replace('_', ' ')\n",
    "        pair = f\"{key_formatted}: {v}\"\n",
    "        if k.lower() in first_keys:\n",
    "            first_line.append(pair)\n",
    "        else:\n",
    "            second_line.append(pair)\n",
    "\n",
    "    print(\"  \".join(first_line))\n",
    "    print(\"  \".join(second_line))\n",
    "\n",
    "    return stats\n",
    "\n",
    "def read_s3_keys_from_excel(excel_path, sample_size, sheet_name=\"S3 File Paths\"):\n",
    "    print(f\"üì• Reading S3 paths from Excel: {excel_path}, sheet: {sheet_name}\")\n",
    "    df = pd.read_excel(excel_path, sheet_name=sheet_name, engine=\"openpyxl\")\n",
    "\n",
    "    file_columns = [col for col in df.columns if col.startswith(\"S3 File\")]\n",
    "\n",
    "    if not file_columns:\n",
    "        raise ValueError(f\"No columns starting with 'S3 File' found in sheet '{sheet_name}'\")\n",
    "\n",
    "    all_paths = []\n",
    "    for col in file_columns:\n",
    "        for cell in df[col].dropna():\n",
    "            path = str(cell).strip()\n",
    "            if path:\n",
    "                all_paths.append(path)\n",
    "\n",
    "    print(f\"üîç Total paths found: {len(all_paths)}\")\n",
    "\n",
    "    valid_keys = []\n",
    "    invalid_paths = []\n",
    "\n",
    "    for path in all_paths:\n",
    "        if path.startswith(\"s3://\"):\n",
    "            parts = path.replace(\"s3://\", \"\").split(\"/\", 1)\n",
    "            if len(parts) == 2 and parts[1].strip():\n",
    "                valid_keys.append(parts[1])\n",
    "            else:\n",
    "                invalid_paths.append(path)\n",
    "        else:\n",
    "            invalid_paths.append(path)\n",
    "\n",
    "    print(f\"üìÅ Parsed S3 keys: {len(valid_keys)}\")\n",
    "\n",
    "    if invalid_paths:\n",
    "        print(f\"‚ö†Ô∏è Skipped invalid paths: {len(invalid_paths)}\")\n",
    "        print(\"\\n‚ö†Ô∏è Skipped Paths (sample):\")\n",
    "        for bad in invalid_paths[:10]:\n",
    "            print(f\" - {bad}\")\n",
    "        if len(invalid_paths) > 10:\n",
    "            print(f\" ...and {len(invalid_paths) - 10} more.\")\n",
    "\n",
    "    # selected_keys = random.sample(valid_keys, min(sample_size, len(valid_keys)))\n",
    "    # print(f\"üéØ Randomly selected {len(selected_keys)} files.\")\n",
    "    # return selected_keys\n",
    "\n",
    "    print(f\"üìÑ Reading metadata from sheet: 'Active Legal Contracts'\")\n",
    "    df_meta = pd.read_excel(excel_path, sheet_name=\"Active Legal Contracts\", engine=\"openpyxl\")\n",
    "    meta_map = df_meta.set_index('Contract Number')[['Account', 'Document Type']].to_dict('index')\n",
    "\n",
    "    result = []\n",
    "    for key in valid_keys:\n",
    "        parts = key.split(\"/\")\n",
    "        contract_number = parts[1] if len(parts) > 1 else None\n",
    "        metadata = meta_map.get(int(contract_number), {}) if contract_number and contract_number.isdigit() else {}\n",
    "\n",
    "        result.append({\n",
    "            'file_key': key,\n",
    "            'metadata': {\n",
    "                'client_account': metadata.get('Account', 'NA'),\n",
    "                'document_type': metadata.get('Document Type', 'NA')\n",
    "            }\n",
    "        })\n",
    "\n",
    "    return result\n",
    "\n",
    "def extract_csv_file_metadata(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    #incase we want to resume where we left off\n",
    "    df['S3_vectors'] = df['S3_vectors'].astype(str).str.lower()\n",
    "    df_filtered = df[df['S3_vectors'] != 'true']\n",
    "    print(f\"‚úÖ Rows included after filtering (S3_vectors != True): {len(df_filtered)}\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for _, row in df_filtered.iterrows():\n",
    "        file_key = row['S3_full_path'].replace(f\"s3://{row['S3_full_path'].split('/')[2]}/\", \"\")\n",
    "        metadata = {\n",
    "            'client_account': row.get('client_account', 'NA'),\n",
    "            'document_type': row.get('doc_type', 'NA')\n",
    "        }\n",
    "\n",
    "        results.append({\n",
    "            'file_key': file_key,\n",
    "            'metadata': metadata\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # file_keys_with_meta = read_s3_keys_from_excel(EXCEL_PATH, sample_size=20)\n",
    "    INDEX_NAME = \"token-chunking-poc\"\n",
    "\n",
    "    file_keys_with_meta = extract_csv_file_metadata(\"gathered_contract_files.csv\")\n",
    "    \n",
    "    final_stats = process_documents(S3_BUCKET, file_keys_with_meta, INDEX_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a26c3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks vector & stored: 17598\n"
     ]
    }
   ],
   "source": [
    "INDEX_NAME = 'token-chunking-poc'\n",
    "\n",
    "paginator = s3v.get_paginator('list_vectors')\n",
    "\n",
    "total_vectors_count = 0\n",
    "\n",
    "page_iterator = paginator.paginate(\n",
    "    vectorBucketName=VECTOR_BUCKET_NAME,\n",
    "    indexName=INDEX_NAME,\n",
    "    returnData=True,\n",
    "    returnMetadata=True,\n",
    "    PaginationConfig={\n",
    "        'PageSize': 1000  \n",
    "    }\n",
    ")\n",
    "\n",
    "for page in page_iterator:\n",
    "    vectors = page.get('vectors', [])\n",
    "    total_vectors_count += len(vectors)\n",
    "\n",
    "print(f\"Total chunks vector & stored: {total_vectors_count}\")\n",
    "\n",
    "#17374"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6cbec615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Query: 'What obligations does Cotiviti have under Schedule C for Prepay FWAV Services?' ---\n",
      "Retrieved 0\n",
      "\n",
      "--- Processing Query: 'Under Schedule C, what services is Cotiviti required to provide?' ---\n",
      "Retrieved 0\n",
      "\n",
      "--- Processing Query: 'In the Prepay FWAV Services section, what are Cotiviti's main deliverables?' ---\n",
      "Retrieved 0\n",
      "\n",
      "--- Processing Query: 'What restrictions are placed on disclosing confidential information?' ---\n",
      "Retrieved 0\n",
      "\n",
      "--- Processing Query: 'What is Amendment #4 to the Verisk Health License Agreement about?' ---\n",
      "Retrieved 0\n",
      "\n",
      "--- Processing Query: 'What is the purpose of Amendment #4 as stated in the document?' ---\n",
      "Retrieved 0\n"
     ]
    }
   ],
   "source": [
    "def query_s3_vector_store(query_text, client_account_filter, INDEX_NAME, top_k = 5):\n",
    "    print(f\"\\n--- Processing Query: '{query_text}' ---\")\n",
    "\n",
    "    query_embedding = get_text_embedding(query_text)\n",
    "    filter_expression = None\n",
    "    if client_account_filter is not None:\n",
    "        filter_expression = {\n",
    "            \"client_account\": {\n",
    "                \"$eq\": client_account_filter\n",
    "            }\n",
    "        }\n",
    "\n",
    "    try:\n",
    "        response = s3v.query_vectors(\n",
    "            vectorBucketName=VECTOR_BUCKET_NAME,\n",
    "            indexName=INDEX_NAME,\n",
    "            topK=top_k,\n",
    "            queryVector={\n",
    "                'float32': query_embedding\n",
    "            },\n",
    "            returnMetadata=True,\n",
    "            returnDistance=True,\n",
    "            filter=filter_expression\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying S3 Vector Store: {e}\")\n",
    "        return None\n",
    "\n",
    "user_questions = [\n",
    "    \"What obligations does Cotiviti have under Schedule C for Prepay FWAV Services?\", #66179\n",
    "    \"Under Schedule C, what services is Cotiviti required to provide?\", #66179\n",
    "    \"In the Prepay FWAV Services section, what are Cotiviti's main deliverables?\", #66179\n",
    "    \"What restrictions are placed on disclosing confidential information?\", #67566\n",
    "    \"What is Amendment #4 to the Verisk Health License Agreement about?\", #53985\n",
    "    \"What is the purpose of Amendment #4 as stated in the document?\", #53985\n",
    "]\n",
    "\n",
    "INDEX_NAME = \"token-chunking-poc\"\n",
    "client = None #'UST Global'\n",
    "\n",
    "for question in user_questions:\n",
    "    query_results = query_s3_vector_store(question, client, INDEX_NAME, top_k=5)\n",
    "\n",
    "    if query_results and 'vectors' in query_results:\n",
    "        print(f\"Retrieved {len(query_results['vectors'])}\")\n",
    "        for i, chunk_data in enumerate(query_results['vectors']):\n",
    "            print(f\"    Metadata {i+1}: {chunk_data['metadata']}\")\n",
    "            # print(f\"    Distance: {chunk_data.get('distance', 'N/A')}\")\n",
    "    else:\n",
    "        print(f\"No results or an error occurred for query: '{question}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6875a76d",
   "metadata": {},
   "source": [
    "LLM Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c3724f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "import time\n",
    "\n",
    "AZURE_OPENAI_ENDPOINT = \"https://ironclad-openai-001.openai.azure.com/\"\n",
    "AZURE_OPENAI_API_KEY = \"936856630b764210913d9a8fd6c8212b\"\n",
    "AZURE_DEPLOYMENT_NAME = \"gpt-4o\"\n",
    "\n",
    "INDEX_NAME = \"token-chunking-poc\"\n",
    "client_name = None\n",
    "\n",
    "def load_azure_client():\n",
    "    return AzureOpenAI(\n",
    "        api_key=AZURE_OPENAI_API_KEY,\n",
    "        api_version=\"2023-05-15\",\n",
    "        azure_endpoint=AZURE_OPENAI_ENDPOINT\n",
    "    )\n",
    "\n",
    "azure_client = load_azure_client()\n",
    "\n",
    "def build_prompt(query, top_chunks):\n",
    "    context = \"\"\n",
    "    source_refs = {}\n",
    "    chunks = top_chunks['vectors']\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        ref = f\"[{i+1}]\"\n",
    "        metadata = chunk.get(\"metadata\", {})\n",
    "        source = metadata.get(\"s3_path\", \"unknown\")\n",
    "        chunk_text = metadata.get(\"text\", \"\")\n",
    "        \n",
    "        context += f\"{ref} ({source}):\\n{chunk_text}\\n\\n\"\n",
    "        source_refs[ref] = metadata\n",
    "\n",
    "    prompt = f\"\"\"You are a helpful assistant. Use only the following context to answer the question.\n",
    "Cite sources using [1], [2], etc., based only on the exact chunks below. Do not make up citations. Do not include sources not explicitly mentioned.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    return prompt, source_refs\n",
    "\n",
    "def run_query_pipeline(query, top_k=5):\n",
    "    print(f\"\\nüîç Running RAG query for: {query}\\n\")\n",
    "    start = time.time()\n",
    "\n",
    "    chunks = query_s3_vector_store(query, client_name, INDEX_NAME, top_k=top_k)\n",
    "\n",
    "    if not chunks:\n",
    "        print(\"‚ùó No chunks returned from vector store.\")\n",
    "        return None, {}, 0, []\n",
    "\n",
    "    prompt, refs = build_prompt(query, chunks)\n",
    "\n",
    "    response = azure_client.chat.completions.create(\n",
    "        model=AZURE_DEPLOYMENT_NAME,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant. Use only the following context to answer the question. \"\n",
    "                           \"Cite sources using [1], [2], etc., based only on the exact chunks below. \"\n",
    "                           \"Do not make up citations. Do not include sources not explicitly mentioned.\"\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    answer = response.choices[0].message.content.strip()\n",
    "    latency = round((time.time() - start) * 1000, 2)\n",
    "\n",
    "    print(f\"\\nAnswer:\\n{answer}\\n\")\n",
    "    print(f\"Latency: {latency} ms\")\n",
    "    print(f\"\\nTop {top_k} Chunks Returned:\")\n",
    "    chunks = chunks['vectors']\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        meta = chunk.get(\"metadata\", {})\n",
    "        print(f\"\\n--- Chunk [{i}] ---\")\n",
    "        print(f\"Text: {meta.get('text', '')[:300]}{'...' if len(meta.get('text', '')) > 300 else ''}\")\n",
    "        print(\"Metadata:\")\n",
    "        for k, v in meta.items():\n",
    "            if k != \"text\":\n",
    "                print(f\"  {k}: {v}\")\n",
    "\n",
    "    return answer, refs, latency, chunks\n",
    "\n",
    "\n",
    "def pretty_print_rag_output(answer, refs, latency, chunks):\n",
    "    print(\"\\n=== Final RAG Output ===\")\n",
    "    \n",
    "    print(\"\\nAnswer:\\n\" + \"-\"*60)\n",
    "    print(answer)\n",
    "\n",
    "    print(\"\\nSource References:\\n\" + \"-\"*60)\n",
    "    for ref, meta in refs.items():\n",
    "        print(f\"{ref}:\")\n",
    "        for key, value in meta.items():\n",
    "            if key == \"text\":\n",
    "                print(f\"  text: {value[:200].strip()}...\" if len(value) > 200 else f\"  text: {value.strip()}\")\n",
    "            else:\n",
    "                print(f\"  {key}: {value}\")\n",
    "\n",
    "    print(\"\\nLatency:\\n\" + \"-\"*60)\n",
    "    print(f\"{latency} ms\")\n",
    "\n",
    "    print(\"\\nChunks:\\n\" + \"-\"*60)\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        meta = chunk.get(\"metadata\", {})\n",
    "        print(f\"Chunk [{i}]:\")\n",
    "        print(f\"  file_name: {meta.get('file_name')}\")\n",
    "        print(f\"  client_account: {meta.get('client_account')}\")\n",
    "        print(f\"  document_type: {meta.get('document_type')}\")\n",
    "        print(f\"  s3_path: {meta.get('s3_path')}\")\n",
    "        print(\"  text preview:\")\n",
    "        text_preview = meta.get(\"text\", \"\")\n",
    "        print(\"    \" + text_preview[:300].replace(\"\\n\", \" \") + (\"...\" if len(text_preview) > 300 else \"\"))\n",
    "        print(\"-\"*60)\n",
    "\n",
    "query = \"Can we use client data to develop or test new services, and enhance, improve, or modify our existing services?\"\n",
    "\n",
    "answer, refs, latency, chunks = run_query_pipeline(query, top_k=5)\n",
    "pretty_print_rag_output(answer, refs, latency, chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd9aa1c",
   "metadata": {},
   "source": [
    "#Batch RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99d4aefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running query 1/15: \"Can we use client data (including PHI) to develop or test new services, and enhance, improve, or modify our existing services?\"\n",
      "\n",
      "üîç Running RAG query for: Can we use client data (including PHI) to develop or test new services, and enhance, improve, or modify our existing services?\n",
      "\n",
      "\n",
      "--- Processing Query: 'Can we use client data (including PHI) to develop or test new services, and enhance, improve, or modify our existing services?' ---\n",
      "\n",
      "Answer:\n",
      "Based on the available context, using client data, including PHI (Protected Health Information), to develop or test new services, or enhance, improve, or modify existing services, may be permissible under certain conditions. Specifically, 3M can use PHI for the proper management and administration of its operations or to carry out its legal responsibilities, provided it obtains reasonable assurances from third parties to whom the PHI is disclosed that it will remain confidential, be used or further disclosed only as required by law or for the intended purpose, and any Security Incidents will be reported [5]. \n",
      "\n",
      "However, the context does not explicitly mention using PHI for developing new services beyond what's outlined here, so care should be taken to ensure compliance with applicable laws and agreements governing PHI use.\n",
      "\n",
      "Latency: 1904.67 ms\n",
      "\n",
      "Top 5 Chunks Returned:\n",
      "\n",
      "--- Chunk [1] ---\n",
      "Text: report to VENDOR or the relevant covered entity any discovery of a breach of unsecured PHI, without unreasonable delay, or any use or\n",
      "disclosure of the PHI not provided for in this Addendum of which it becomes aware;\n",
      "\n",
      "d. to the extent 3M maintains or otherwise holds, uses or discloses Unsecured PHI,...\n",
      "Metadata:\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/79737/3M_VAR Agmt and BAA_08232013_fully executed.pdf\n",
      "  document_type: Reseller Agreement\n",
      "  file_name: 3M_VAR Agmt and BAA_08232013_fully executed.pdf\n",
      "  client_account: 3M Health Information System, Inc.\n",
      "\n",
      "--- Chunk [2] ---\n",
      "Text: h. document disclosures of PH] made pursuant to applicable law and information related to such disclosures as would be required for\n",
      "\n",
      "VENDOR or the relevant covered entity to respond to a request by an individual for an accounting of disclosures of PHI in accordance with 45\n",
      "CFR ¬ß164.528 or Section 13...\n",
      "Metadata:\n",
      "  document_type: Reseller Agreement\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/79737/3M_VAR Agmt and BAA_08232013_fully executed.pdf\n",
      "  file_name: 3M_VAR Agmt and BAA_08232013_fully executed.pdf\n",
      "  client_account: 3M Health Information System, Inc.\n",
      "\n",
      "--- Chunk [3] ---\n",
      "Text: when used in this Addendum shall have a\n",
      "meaning as defined by the Privacy and Security Regulations or the HITECH Act but for the purposes of this Addendum shall be limited to PHI\n",
      "and/or EPHI received from, or created or received by 3M on behalf of, VENDOR and/or a covered entity. Wherever the term P...\n",
      "Metadata:\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/79737/3M_VAR Agmt and BAA_08232013_fully executed.pdf\n",
      "  client_account: 3M Health Information System, Inc.\n",
      "  document_type: Reseller Agreement\n",
      "  file_name: 3M_VAR Agmt and BAA_08232013_fully executed.pdf\n",
      "\n",
      "--- Chunk [4] ---\n",
      "Text: a. Discloser hereby represents and warrants to Recipient  that Discloser has the right and authority to \n",
      "execute this Agreement , disclose the Covered  Data to Recipient  and that the provision of such Covered  \n",
      "Data to Recipient  hereunder is not in violation or in breach of any law, regulation, co...\n",
      "Metadata:\n",
      "  document_type: DUA\n",
      "  client_account: F.B.P. Insurance Services (Precept/McGriff)\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/75516/PHHC - Cotiviti - DUA - TE - 10222024 - Dynamics_75516.pdf\n",
      "  file_name: PHHC - Cotiviti - DUA - TE - 10222024 - Dynamics_75516.pdf\n",
      "\n",
      "--- Chunk [5] ---\n",
      "Text: Except as otherwise limited in this Addendum, 3M may use or disclose PHI:\n",
      "\n",
      "@ on behalf of, or to provide services to, VENDOR or the relevant covered entity, as provided for in the Agreement and\n",
      "in accordance with the Privacy Rule; 3M shall request, use and disclose only the minimum amount of PH} nec...\n",
      "Metadata:\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/79737/3M_VAR Agmt and BAA_08232013_fully executed.pdf\n",
      "  document_type: Reseller Agreement\n",
      "  file_name: 3M_VAR Agmt and BAA_08232013_fully executed.pdf\n",
      "  client_account: 3M Health Information System, Inc.\n",
      "\n",
      "Running query 2/15: \"Can we use aggregated client data or de-identified client data for the purposes of developing, testing, enhancing, improving, or modifying services?\"\n",
      "\n",
      "üîç Running RAG query for: Can we use aggregated client data or de-identified client data for the purposes of developing, testing, enhancing, improving, or modifying services?\n",
      "\n",
      "\n",
      "--- Processing Query: 'Can we use aggregated client data or de-identified client data for the purposes of developing, testing, enhancing, improving, or modifying services?' ---\n",
      "\n",
      "Answer:\n",
      "Yes, aggregated client data or de-identified client data can be used for the purposes of developing, testing, enhancing, improving, or modifying services. The context provided states that Personify Health allows the vendor to aggregate and de-identify the data, and use the aggregated de-identified information for preparing commercially available normative and benchmarking data, as well as for internal and external research, analysis, and product development purposes [1].\n",
      "\n",
      "Latency: 1221.28 ms\n",
      "\n",
      "Top 5 Chunks Returned:\n",
      "\n",
      "--- Chunk [1] ---\n",
      "Text: 2. C overed Data . Subject to the terms and conditions of this Agreement, Discloser  hereby agrees to disclose to Recipient  \n",
      "the Covered  Data, solely to effectuate the Purpose set forth herein . Recipient hereby acknowledges and agrees that \n",
      "Discloser and  those individual Members  that are the su...\n",
      "Metadata:\n",
      "  client_account: F.B.P. Insurance Services (Precept/McGriff)\n",
      "  file_name: PHHC - Cotiviti - DUA - TE - 10222024 - Dynamics_75516.pdf\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/75516/PHHC - Cotiviti - DUA - TE - 10222024 - Dynamics_75516.pdf\n",
      "  document_type: DUA\n",
      "\n",
      "--- Chunk [2] ---\n",
      "Text: The Data Processing Agreement also does not apply to any (1) demonstration \n",
      "accounts, trials, beta releases, release preview or other similar versions of the services or (2) any features, services or products which are provided \n",
      "pursuant to a separate agreement or by a party other than Oracle (as de...\n",
      "Metadata:\n",
      "  client_account: Oracle America, Inc\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/63293/Eliza_1_Month_Renewal_Estimate_930170.pdf\n",
      "  document_type: Quote\n",
      "  file_name: Eliza_1_Month_Renewal_Estimate_930170.pdf\n",
      "\n",
      "--- Chunk [3] ---\n",
      "Text: The Sit e may be changed \n",
      "to another URL by Vendor in its sole discretion, provided that Vendor shall provide Customer with notice thereof o r otherwise redirect Customer \n",
      "to such new URL for a reasonable amount of time. Modules/Datasets  Include:  \n",
      "ÔÇ∑ Global Equity Contact and Ownership Data, Excel ...\n",
      "Metadata:\n",
      "  file_name: Cotiviti USA LLC GMI BDC FSWS Amended and Restated Schedule A May 2018.pdf\n",
      "  client_account: S&P Global Ratings\n",
      "  document_type: Amendment\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/78587/Cotiviti USA LLC GMI BDC FSWS Amended and Restated Schedule A May 2018.pdf\n",
      "\n",
      "--- Chunk [4] ---\n",
      "Text: Additionally, Vendor has adopted and implemented policies, procedures and systems to respond to its legal obligations and the needs of the \n",
      "Data Subject whose Relevant Personal Data is being processed (‚ÄúRelevant Data Subjects‚Äù), including the provision for Relevant  Data Subjects \n",
      "to unsubscribe/opt...\n",
      "Metadata:\n",
      "  file_name: Cotiviti USA LLC GMI BDC FSWS Amended and Restated Schedule A May 2018.pdf\n",
      "  document_type: Amendment\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/78587/Cotiviti USA LLC GMI BDC FSWS Amended and Restated Schedule A May 2018.pdf\n",
      "  client_account: S&P Global Ratings\n",
      "\n",
      "--- Chunk [5] ---\n",
      "Text: Supplier  is not, and shall not be , responsible for any claims, \n",
      "damages, loss es or breach (including under any theory of indemnification) that are directly \n",
      "or indirectly related to  incorrect or incomplete information supplied by or on behalf of \n",
      "Anthem . 2.2. Anthem  will p rovide Supplier  wit...\n",
      "Metadata:\n",
      "  file_name: SOW_Cotiviti_DM_Sep19 (002).pdf\n",
      "  client_account: Anthem, Inc (WellPoint, Inc.)\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/55821/SOW_Cotiviti_DM_Sep19 (002).pdf\n",
      "  document_type: SOW\n",
      "\n",
      "Running query 3/15: \"Are there any restrictions on using artificial intelligence or machine learning in delivering the services?\"\n",
      "\n",
      "üîç Running RAG query for: Are there any restrictions on using artificial intelligence or machine learning in delivering the services?\n",
      "\n",
      "\n",
      "--- Processing Query: 'Are there any restrictions on using artificial intelligence or machine learning in delivering the services?' ---\n",
      "\n",
      "Answer:\n",
      "The context provided does not include any specific restrictions on using artificial intelligence or machine learning in delivering the services. The chunks mention various service provision details, data privacy requirements, and disaster recovery plans but do not explicitly address the use of AI or ML technologies in service delivery. Therefore, there are no cited restrictions on AI or ML based on the given context.\n",
      "\n",
      "Latency: 1195.98 ms\n",
      "\n",
      "Top 5 Chunks Returned:\n",
      "\n",
      "--- Chunk [1] ---\n",
      "Text: Third party Products and/or Modules , if any, may be  subjec t to additional restrictions as set forth on Annex B , if \n",
      "applicable. b. Effect of Non -Renewal of this Schedule . Upon receipt of notice of Customer‚Äôs intent not to renew this Schedule, Vendor may discontinue \n",
      "Customer‚Äôs access to the ‚ÄúE...\n",
      "Metadata:\n",
      "  file_name: Cotiviti USA LLC GMI BDC FSWS Amended and Restated Schedule A May 2018.pdf\n",
      "  document_type: Amendment\n",
      "  client_account: S&P Global Ratings\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/78587/Cotiviti USA LLC GMI BDC FSWS Amended and Restated Schedule A May 2018.pdf\n",
      "\n",
      "--- Chunk [2] ---\n",
      "Text: If Vendor participates in tests, it shall support the following activities: \n",
      "The provision of technical experts at the Vendor data center to participate as needed during the Humana systems recovery test, during normal business hours. Document Vendor‚Äôs involvement in Humana‚Äôs disaster recovery tests ...\n",
      "Metadata:\n",
      "  client_account: Humana\n",
      "  file_name: Humana Second Amended and Restated Schedule A redline 5-8-23.docx\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/70008/Humana Second Amended and Restated Schedule A redline 5-8-23.docx\n",
      "  document_type: SOW\n",
      "\n",
      "--- Chunk [3] ---\n",
      "Text: Finance   BDC \n",
      "  \n",
      "DocuSign Envelope ID: 2FA44D5F-58C2-46EE-B320-7CCCBAB9F204\n",
      "2 \n",
      "Ipreo Rep: Andrew Parven  6. Additional Provisions . a. Additional Use Restrictions . i. BD Corporate: Customer may access and use the Databases,  as applicable, solely for the following purposes: (i) to view any of the ...\n",
      "Metadata:\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/78587/Cotiviti USA LLC GMI BDC FSWS Amended and Restated Schedule A May 2018.pdf\n",
      "  client_account: S&P Global Ratings\n",
      "  file_name: Cotiviti USA LLC GMI BDC FSWS Amended and Restated Schedule A May 2018.pdf\n",
      "  document_type: Amendment\n",
      "\n",
      "--- Chunk [4] ---\n",
      "Text: send us an e -mail to legal@verscend .com and in the body of such request you must \n",
      "state your e -mail, full name, IS Postal Address, telephone number, and account number. Required hardware and software   \n",
      "Operating Systems:  Windows2000¬¨ or WindowsXP¬¨  \n",
      "Browsers (for \n",
      "SENDERS):  Internet Explorer 6...\n",
      "Metadata:\n",
      "  client_account: Anthem, Inc (WellPoint, Inc.)\n",
      "  file_name: Dynamics_-_66186_signed.pdf\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/66186/Dynamics_-_66186_signed.pdf\n",
      "  document_type: NDA\n",
      "\n",
      "--- Chunk [5] ---\n",
      "Text: If Humana requires expanded capacity, Cotiviti will evaluate the request and provide an estimate of the additional annual fee. Support for additional load testing will be billed at $200 per hour. Attachment B \n",
      "to \n",
      "SCHEDULE A:\n",
      "QUALITY INTELLIGENCE FEES (as of July 1, 2023)\n",
      "\n",
      "\n",
      "*All measures have a aggr...\n",
      "Metadata:\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/70008/Humana Second Amended and Restated Schedule A redline 5-8-23.docx\n",
      "  client_account: Humana\n",
      "  document_type: SOW\n",
      "  file_name: Humana Second Amended and Restated Schedule A redline 5-8-23.docx\n",
      "\n",
      "Running query 4/15: \"Are there any specific terms regarding the use of client data for training and fine-tuning AI models?\"\n",
      "\n",
      "üîç Running RAG query for: Are there any specific terms regarding the use of client data for training and fine-tuning AI models?\n",
      "\n",
      "\n",
      "--- Processing Query: 'Are there any specific terms regarding the use of client data for training and fine-tuning AI models?' ---\n",
      "\n",
      "Answer:\n",
      "The provided context does not contain any explicit terms regarding the use of client data for training and fine-tuning AI models. The context includes data processing agreements, use restrictions, policies for data subjects to opt-out, site usage instructions, subscription services, and data security measures, but does not specifically address using client data for AI model training or fine-tuning.\n",
      "\n",
      "Latency: 1281.66 ms\n",
      "\n",
      "Top 5 Chunks Returned:\n",
      "\n",
      "--- Chunk [1] ---\n",
      "Text: Additionally, Vendor has adopted and implemented policies, procedures and systems to respond to its legal obligations and the needs of the \n",
      "Data Subject whose Relevant Personal Data is being processed (‚ÄúRelevant Data Subjects‚Äù), including the provision for Relevant  Data Subjects \n",
      "to unsubscribe/opt...\n",
      "Metadata:\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/78587/Cotiviti USA LLC GMI BDC FSWS Amended and Restated Schedule A May 2018.pdf\n",
      "  client_account: S&P Global Ratings\n",
      "  document_type: Amendment\n",
      "  file_name: Cotiviti USA LLC GMI BDC FSWS Amended and Restated Schedule A May 2018.pdf\n",
      "\n",
      "--- Chunk [2] ---\n",
      "Text: The Sit e may be changed \n",
      "to another URL by Vendor in its sole discretion, provided that Vendor shall provide Customer with notice thereof o r otherwise redirect Customer \n",
      "to such new URL for a reasonable amount of time. Modules/Datasets  Include:  \n",
      "ÔÇ∑ Global Equity Contact and Ownership Data, Excel ...\n",
      "Metadata:\n",
      "  client_account: S&P Global Ratings\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/78587/Cotiviti USA LLC GMI BDC FSWS Amended and Restated Schedule A May 2018.pdf\n",
      "  file_name: Cotiviti USA LLC GMI BDC FSWS Amended and Restated Schedule A May 2018.pdf\n",
      "  document_type: Amendment\n",
      "\n",
      "--- Chunk [3] ---\n",
      "Text: Finance   BDC \n",
      "  \n",
      "DocuSign Envelope ID: 2FA44D5F-58C2-46EE-B320-7CCCBAB9F204\n",
      "2 \n",
      "Ipreo Rep: Andrew Parven  6. Additional Provisions . a. Additional Use Restrictions . i. BD Corporate: Customer may access and use the Databases,  as applicable, solely for the following purposes: (i) to view any of the ...\n",
      "Metadata:\n",
      "  document_type: Amendment\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/78587/Cotiviti USA LLC GMI BDC FSWS Amended and Restated Schedule A May 2018.pdf\n",
      "  client_account: S&P Global Ratings\n",
      "  file_name: Cotiviti USA LLC GMI BDC FSWS Amended and Restated Schedule A May 2018.pdf\n",
      "\n",
      "--- Chunk [4] ---\n",
      "Text: The Data Processing Agreement also does not apply to any (1) demonstration \n",
      "accounts, trials, beta releases, release preview or other similar versions of the services or (2) any features, services or products which are provided \n",
      "pursuant to a separate agreement or by a party other than Oracle (as de...\n",
      "Metadata:\n",
      "  file_name: Eliza_1_Month_Renewal_Estimate_930170.pdf\n",
      "  client_account: Oracle America, Inc\n",
      "  document_type: Quote\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/63293/Eliza_1_Month_Renewal_Estimate_930170.pdf\n",
      "\n",
      "--- Chunk [5] ---\n",
      "Text: Recipient  shall issue appropriate instruction to each employee given access to the Covered Data regarding restrictions at \n",
      "least as restrictive as those set forth in this Agreement and  shall provide physical security of the Covered  Data to the same \n",
      "or greater degree that Recipient  protects its ...\n",
      "Metadata:\n",
      "  client_account: F.B.P. Insurance Services (Precept/McGriff)\n",
      "  file_name: PHHC - Cotiviti - DUA - TE - 10222024 - Dynamics_75516.pdf\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/75516/PHHC - Cotiviti - DUA - TE - 10222024 - Dynamics_75516.pdf\n",
      "  document_type: DUA\n",
      "\n",
      "Running query 5/15: \"Are there any limitations on storing client data (including PHI) in the cloud?\"\n",
      "\n",
      "üîç Running RAG query for: Are there any limitations on storing client data (including PHI) in the cloud?\n",
      "\n",
      "\n",
      "--- Processing Query: 'Are there any limitations on storing client data (including PHI) in the cloud?' ---\n",
      "\n",
      "Answer:\n",
      "The documents do not explicitly address limitations on storing client data, including PHI, in the cloud. They focus primarily on the obligations and safeguards that 3M must implement when handling PHI. Specifically, 3M is required to use appropriate administrative, physical, and technical safeguards to protect the confidentiality, integrity, and availability of electronic PHI (EPHI) that it creates, receives, maintains, or transmits on behalf of a vendor or covered entity [3]. This includes ensuring that any subcontractors also implement reasonable and appropriate safeguards [3]. Therefore, while the documents describe how PHI should be protected, they do not directly address limitations related to cloud storage.\n",
      "\n",
      "Latency: 2233.12 ms\n",
      "\n",
      "Top 5 Chunks Returned:\n",
      "\n",
      "--- Chunk [1] ---\n",
      "Text: report to VENDOR or the relevant covered entity any discovery of a breach of unsecured PHI, without unreasonable delay, or any use or\n",
      "disclosure of the PHI not provided for in this Addendum of which it becomes aware;\n",
      "\n",
      "d. to the extent 3M maintains or otherwise holds, uses or discloses Unsecured PHI,...\n",
      "Metadata:\n",
      "  document_type: Reseller Agreement\n",
      "  file_name: 3M_VAR Agmt and BAA_08232013_fully executed.pdf\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/79737/3M_VAR Agmt and BAA_08232013_fully executed.pdf\n",
      "  client_account: 3M Health Information System, Inc.\n",
      "\n",
      "--- Chunk [2] ---\n",
      "Text: h. document disclosures of PH] made pursuant to applicable law and information related to such disclosures as would be required for\n",
      "\n",
      "VENDOR or the relevant covered entity to respond to a request by an individual for an accounting of disclosures of PHI in accordance with 45\n",
      "CFR ¬ß164.528 or Section 13...\n",
      "Metadata:\n",
      "  file_name: 3M_VAR Agmt and BAA_08232013_fully executed.pdf\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/79737/3M_VAR Agmt and BAA_08232013_fully executed.pdf\n",
      "  document_type: Reseller Agreement\n",
      "  client_account: 3M Health Information System, Inc.\n",
      "\n",
      "--- Chunk [3] ---\n",
      "Text: when used in this Addendum shall have a\n",
      "meaning as defined by the Privacy and Security Regulations or the HITECH Act but for the purposes of this Addendum shall be limited to PHI\n",
      "and/or EPHI received from, or created or received by 3M on behalf of, VENDOR and/or a covered entity. Wherever the term P...\n",
      "Metadata:\n",
      "  document_type: Reseller Agreement\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/79737/3M_VAR Agmt and BAA_08232013_fully executed.pdf\n",
      "  client_account: 3M Health Information System, Inc.\n",
      "  file_name: 3M_VAR Agmt and BAA_08232013_fully executed.pdf\n",
      "\n",
      "--- Chunk [4] ---\n",
      "Text: a. Discloser hereby represents and warrants to Recipient  that Discloser has the right and authority to \n",
      "execute this Agreement , disclose the Covered  Data to Recipient  and that the provision of such Covered  \n",
      "Data to Recipient  hereunder is not in violation or in breach of any law, regulation, co...\n",
      "Metadata:\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/75516/PHHC - Cotiviti - DUA - TE - 10222024 - Dynamics_75516.pdf\n",
      "  client_account: F.B.P. Insurance Services (Precept/McGriff)\n",
      "  document_type: DUA\n",
      "  file_name: PHHC - Cotiviti - DUA - TE - 10222024 - Dynamics_75516.pdf\n",
      "\n",
      "--- Chunk [5] ---\n",
      "Text: Except as otherwise limited in this Addendum, 3M may use or disclose PHI:\n",
      "\n",
      "@ on behalf of, or to provide services to, VENDOR or the relevant covered entity, as provided for in the Agreement and\n",
      "in accordance with the Privacy Rule; 3M shall request, use and disclose only the minimum amount of PH} nec...\n",
      "Metadata:\n",
      "  client_account: 3M Health Information System, Inc.\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/79737/3M_VAR Agmt and BAA_08232013_fully executed.pdf\n",
      "  document_type: Reseller Agreement\n",
      "  file_name: 3M_VAR Agmt and BAA_08232013_fully executed.pdf\n",
      "\n",
      "Running query 6/15: \"Are there any restrictions on using cloud services to assist in the processing of client data (including PHI)?\"\n",
      "\n",
      "üîç Running RAG query for: Are there any restrictions on using cloud services to assist in the processing of client data (including PHI)?\n",
      "\n",
      "\n",
      "--- Processing Query: 'Are there any restrictions on using cloud services to assist in the processing of client data (including PHI)?' ---\n",
      "\n",
      "Answer:\n",
      "Yes, there are restrictions regarding the use of electronic protected health information (EPHI) received from or created on behalf of VENDOR or a covered entity. These restrictions generally involve several key obligations and activities to ensure the protection of EPHI when using cloud services or otherwise processing client data. \n",
      "\n",
      "3M agrees to implement appropriate administrative, physical, and technical safeguards to protect the confidentiality, integrity, and availability of EPHI. Furthermore, any agent, including subcontractors to whom EPHI is provided by 3M, must agree to implement reasonable and appropriate safeguards to protect it [3]. \n",
      "\n",
      "Additionally, there is a restriction on remuneration received in exchange for PHI. As per Section 13405(d) of the HITECH Act, 3M may not receive payment for PHI unless permitted by the Act or regulations issued by the Secretary, except for activities undertaken on behalf of VENDOR or the covered entity under the Agreement [4]. \n",
      "\n",
      "These obligations emphasize the importance of security measures and compliance with legal standards when using cloud services for processing PHI [3], [4].\n",
      "\n",
      "Latency: 2554.07 ms\n",
      "\n",
      "Top 5 Chunks Returned:\n",
      "\n",
      "--- Chunk [1] ---\n",
      "Text: report to VENDOR or the relevant covered entity any discovery of a breach of unsecured PHI, without unreasonable delay, or any use or\n",
      "disclosure of the PHI not provided for in this Addendum of which it becomes aware;\n",
      "\n",
      "d. to the extent 3M maintains or otherwise holds, uses or discloses Unsecured PHI,...\n",
      "Metadata:\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/79737/3M_VAR Agmt and BAA_08232013_fully executed.pdf\n",
      "  document_type: Reseller Agreement\n",
      "  client_account: 3M Health Information System, Inc.\n",
      "  file_name: 3M_VAR Agmt and BAA_08232013_fully executed.pdf\n",
      "\n",
      "--- Chunk [2] ---\n",
      "Text: h. document disclosures of PH] made pursuant to applicable law and information related to such disclosures as would be required for\n",
      "\n",
      "VENDOR or the relevant covered entity to respond to a request by an individual for an accounting of disclosures of PHI in accordance with 45\n",
      "CFR ¬ß164.528 or Section 13...\n",
      "Metadata:\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/79737/3M_VAR Agmt and BAA_08232013_fully executed.pdf\n",
      "  client_account: 3M Health Information System, Inc.\n",
      "  file_name: 3M_VAR Agmt and BAA_08232013_fully executed.pdf\n",
      "  document_type: Reseller Agreement\n",
      "\n",
      "--- Chunk [3] ---\n",
      "Text: when used in this Addendum shall have a\n",
      "meaning as defined by the Privacy and Security Regulations or the HITECH Act but for the purposes of this Addendum shall be limited to PHI\n",
      "and/or EPHI received from, or created or received by 3M on behalf of, VENDOR and/or a covered entity. Wherever the term P...\n",
      "Metadata:\n",
      "  client_account: 3M Health Information System, Inc.\n",
      "  document_type: Reseller Agreement\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/79737/3M_VAR Agmt and BAA_08232013_fully executed.pdf\n",
      "  file_name: 3M_VAR Agmt and BAA_08232013_fully executed.pdf\n",
      "\n",
      "--- Chunk [4] ---\n",
      "Text: Except as otherwise limited in this Addendum, 3M may use or disclose PHI:\n",
      "\n",
      "@ on behalf of, or to provide services to, VENDOR or the relevant covered entity, as provided for in the Agreement and\n",
      "in accordance with the Privacy Rule; 3M shall request, use and disclose only the minimum amount of PH} nec...\n",
      "Metadata:\n",
      "  document_type: Reseller Agreement\n",
      "  file_name: 3M_VAR Agmt and BAA_08232013_fully executed.pdf\n",
      "  client_account: 3M Health Information System, Inc.\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/79737/3M_VAR Agmt and BAA_08232013_fully executed.pdf\n",
      "\n",
      "--- Chunk [5] ---\n",
      "Text: a. Discloser hereby represents and warrants to Recipient  that Discloser has the right and authority to \n",
      "execute this Agreement , disclose the Covered  Data to Recipient  and that the provision of such Covered  \n",
      "Data to Recipient  hereunder is not in violation or in breach of any law, regulation, co...\n",
      "Metadata:\n",
      "  file_name: PHHC - Cotiviti - DUA - TE - 10222024 - Dynamics_75516.pdf\n",
      "  document_type: DUA\n",
      "  client_account: F.B.P. Insurance Services (Precept/McGriff)\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/75516/PHHC - Cotiviti - DUA - TE - 10222024 - Dynamics_75516.pdf\n",
      "\n",
      "Running query 7/15: \"Is there any requirement for human oversight or decision-making in the use of AI for delivering the services?\"\n",
      "\n",
      "üîç Running RAG query for: Is there any requirement for human oversight or decision-making in the use of AI for delivering the services?\n",
      "\n",
      "\n",
      "--- Processing Query: 'Is there any requirement for human oversight or decision-making in the use of AI for delivering the services?' ---\n",
      "\n",
      "Answer:\n",
      "The provided context does not explicitly mention any requirement for human oversight or decision-making in the use of AI for delivering the services. The documents primarily outline contractual obligations, service procedures, data privacy requirements, and technical support activities, but do not address specific requirements regarding the integration or oversight of AI technologies. Therefore, based on the available context, there is no identified requirement for human oversight in the use of AI services.\n",
      "\n",
      "Latency: 1329.77 ms\n",
      "\n",
      "Top 5 Chunks Returned:\n",
      "\n",
      "--- Chunk [1] ---\n",
      "Text: Additionally, Vendor has adopted and implemented policies, procedures and systems to respond to its legal obligations and the needs of the \n",
      "Data Subject whose Relevant Personal Data is being processed (‚ÄúRelevant Data Subjects‚Äù), including the provision for Relevant  Data Subjects \n",
      "to unsubscribe/opt...\n",
      "Metadata:\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/78587/Cotiviti USA LLC GMI BDC FSWS Amended and Restated Schedule A May 2018.pdf\n",
      "  client_account: S&P Global Ratings\n",
      "  document_type: Amendment\n",
      "  file_name: Cotiviti USA LLC GMI BDC FSWS Amended and Restated Schedule A May 2018.pdf\n",
      "\n",
      "--- Chunk [2] ---\n",
      "Text: If Vendor participates in tests, it shall support the following activities: \n",
      "The provision of technical experts at the Vendor data center to participate as needed during the Humana systems recovery test, during normal business hours. Document Vendor‚Äôs involvement in Humana‚Äôs disaster recovery tests ...\n",
      "Metadata:\n",
      "  file_name: Humana Second Amended and Restated Schedule A redline 5-8-23.docx\n",
      "  client_account: Humana\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/70008/Humana Second Amended and Restated Schedule A redline 5-8-23.docx\n",
      "  document_type: SOW\n",
      "\n",
      "--- Chunk [3] ---\n",
      "Text: If Humana determines that more states need to be added, Humana agrees to provide written notification to Cotiviti at least2months prior to submission date. Pricing for those additional Non NCQA Measures will be determined by the Non NCQA/Custom Measures as outlined in Attachment B. and submitted to ...\n",
      "Metadata:\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/70008/Humana Second Amended and Restated Schedule A redline 5-8-23.docx\n",
      "  client_account: Humana\n",
      "  file_name: Humana Second Amended and Restated Schedule A redline 5-8-23.docx\n",
      "  document_type: SOW\n",
      "\n",
      "--- Chunk [4] ---\n",
      "Text: As such, any definitions, obligations or requirements in the Agreement shall apply \n",
      "to this SOW, as though fully set forth herein. In the event of conflict between this SOW \n",
      "and the Agreement, the Agreement shall govern. 8.2. Each party represents and warrants to the other that this SOW has been dul...\n",
      "Metadata:\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/55821/SOW_Cotiviti_DM_Sep19 (002).pdf\n",
      "  document_type: SOW\n",
      "  file_name: SOW_Cotiviti_DM_Sep19 (002).pdf\n",
      "  client_account: Anthem, Inc (WellPoint, Inc.)\n",
      "\n",
      "--- Chunk [5] ---\n",
      "Text: Third party Products and/or Modules , if any, may be  subjec t to additional restrictions as set forth on Annex B , if \n",
      "applicable. b. Effect of Non -Renewal of this Schedule . Upon receipt of notice of Customer‚Äôs intent not to renew this Schedule, Vendor may discontinue \n",
      "Customer‚Äôs access to the ‚ÄúE...\n",
      "Metadata:\n",
      "  document_type: Amendment\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/78587/Cotiviti USA LLC GMI BDC FSWS Amended and Restated Schedule A May 2018.pdf\n",
      "  file_name: Cotiviti USA LLC GMI BDC FSWS Amended and Restated Schedule A May 2018.pdf\n",
      "  client_account: S&P Global Ratings\n",
      "\n",
      "Running query 8/15: \"Does the client have any ownership rights in developed IP or developed materials generated from the use of their data?\"\n",
      "\n",
      "üîç Running RAG query for: Does the client have any ownership rights in developed IP or developed materials generated from the use of their data?\n",
      "\n",
      "\n",
      "--- Processing Query: 'Does the client have any ownership rights in developed IP or developed materials generated from the use of their data?' ---\n",
      "\n",
      "Answer:\n",
      "The context provided does not explicitly mention client ownership rights over developed IP or developed materials generated from the use of their data. Instead, it discusses the definition and handling of Confidential Information and data privacy responsibilities. For instance, the contract specifies confidentiality terms and the role of Customer and Vendor as Data Controllers but does not elaborate on ownership rights related to IP or materials developed from the use of client data [4], [5].\n",
      "\n",
      "Latency: 1286.02 ms\n",
      "\n",
      "Top 5 Chunks Returned:\n",
      "\n",
      "--- Chunk [1] ---\n",
      "Text: The Sit e may be changed \n",
      "to another URL by Vendor in its sole discretion, provided that Vendor shall provide Customer with notice thereof o r otherwise redirect Customer \n",
      "to such new URL for a reasonable amount of time. Modules/Datasets  Include:  \n",
      "ÔÇ∑ Global Equity Contact and Ownership Data, Excel ...\n",
      "Metadata:\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/78587/Cotiviti USA LLC GMI BDC FSWS Amended and Restated Schedule A May 2018.pdf\n",
      "  file_name: Cotiviti USA LLC GMI BDC FSWS Amended and Restated Schedule A May 2018.pdf\n",
      "  document_type: Amendment\n",
      "  client_account: S&P Global Ratings\n",
      "\n",
      "--- Chunk [2] ---\n",
      "Text: Additionally, Vendor has adopted and implemented policies, procedures and systems to respond to its legal obligations and the needs of the \n",
      "Data Subject whose Relevant Personal Data is being processed (‚ÄúRelevant Data Subjects‚Äù), including the provision for Relevant  Data Subjects \n",
      "to unsubscribe/opt...\n",
      "Metadata:\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/78587/Cotiviti USA LLC GMI BDC FSWS Amended and Restated Schedule A May 2018.pdf\n",
      "  file_name: Cotiviti USA LLC GMI BDC FSWS Amended and Restated Schedule A May 2018.pdf\n",
      "  client_account: S&P Global Ratings\n",
      "  document_type: Amendment\n",
      "\n",
      "--- Chunk [3] ---\n",
      "Text: If any quarterly statement is more than ninety (90) days past due, 3M may, in its sole discretion and in addition to\n",
      "rights under Section 12.2, suspend its performance pursuant to Section 4.2.4 until the quarterly statement, and all payments due thereunder, are\n",
      "brought current. 6. CONFIDENTIAL! 61 C...\n",
      "Metadata:\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/79737/3M_VAR Agmt and BAA_08232013_fully executed.pdf\n",
      "  file_name: 3M_VAR Agmt and BAA_08232013_fully executed.pdf\n",
      "  document_type: Reseller Agreement\n",
      "  client_account: 3M Health Information System, Inc.\n",
      "\n",
      "--- Chunk [4] ---\n",
      "Text: Confidential Information  \n",
      "For the purposes of this Agreement, \"Confidential Information\" shall mean, with respect to a party, \n",
      "all information, ideas, concepts, plans, names of resources, business records, costs, income, customer \n",
      "and future plans,  financial and marketing data, software programs, ...\n",
      "Metadata:\n",
      "  document_type: DUA\n",
      "  client_account: Lowe's Companies\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/69010/GRI - Cotiviti - DUA - AS - 07202022 - Dynamics_69010.pdf\n",
      "  file_name: GRI - Cotiviti - DUA - AS - 07202022 - Dynamics_69010.pdf\n",
      "\n",
      "--- Chunk [5] ---\n",
      "Text: Third party Products and/or Modules , if any, may be  subjec t to additional restrictions as set forth on Annex B , if \n",
      "applicable. b. Effect of Non -Renewal of this Schedule . Upon receipt of notice of Customer‚Äôs intent not to renew this Schedule, Vendor may discontinue \n",
      "Customer‚Äôs access to the ‚ÄúE...\n",
      "Metadata:\n",
      "  document_type: Amendment\n",
      "  client_account: S&P Global Ratings\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/78587/Cotiviti USA LLC GMI BDC FSWS Amended and Restated Schedule A May 2018.pdf\n",
      "  file_name: Cotiviti USA LLC GMI BDC FSWS Amended and Restated Schedule A May 2018.pdf\n",
      "\n",
      "Running query 9/15: \"Are there any specific terms regarding the ownership and usage rights of IP developed through the use of client data?\"\n",
      "\n",
      "üîç Running RAG query for: Are there any specific terms regarding the ownership and usage rights of IP developed through the use of client data?\n",
      "\n",
      "\n",
      "--- Processing Query: 'Are there any specific terms regarding the ownership and usage rights of IP developed through the use of client data?' ---\n",
      "\n",
      "Answer:\n",
      "The provided context does not include specific terms regarding ownership and usage rights of IP developed using client data. The documents mention several aspects related to data, such as definitions of \"Data\", \"Database\", and \"Personal Data\", in the context of data protection legislation [4]. They also highlight the confidentiality of information exchanged under an agreement [5]. However, there are no explicit details on ownership and usage rights of intellectual property developed through client data in the provided context.\n",
      "\n",
      "Latency: 1554.78 ms\n",
      "\n",
      "Top 5 Chunks Returned:\n",
      "\n",
      "--- Chunk [1] ---\n",
      "Text: The Sit e may be changed \n",
      "to another URL by Vendor in its sole discretion, provided that Vendor shall provide Customer with notice thereof o r otherwise redirect Customer \n",
      "to such new URL for a reasonable amount of time. Modules/Datasets  Include:  \n",
      "ÔÇ∑ Global Equity Contact and Ownership Data, Excel ...\n",
      "Metadata:\n",
      "  client_account: S&P Global Ratings\n",
      "  document_type: Amendment\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/78587/Cotiviti USA LLC GMI BDC FSWS Amended and Restated Schedule A May 2018.pdf\n",
      "  file_name: Cotiviti USA LLC GMI BDC FSWS Amended and Restated Schedule A May 2018.pdf\n",
      "\n",
      "--- Chunk [2] ---\n",
      "Text: Ipreo Corporate Schedule NA (v04052018)  \n",
      "  \n",
      "AMENDED AND RESTATED  SCHEDULE A \n",
      "GLOBAL MARKETS INTEL LIGENCE,  BD CORPORATE,  FACTSET WORKSTATION  \n",
      "This Amended and Restated Schedule A (‚ÄúSchedule ‚Äù) to the Master Services Agreement (‚Äú Master Services Agreement ‚Äù) entered into as of  May 31, 2016  \n",
      "be...\n",
      "Metadata:\n",
      "  client_account: S&P Global Ratings\n",
      "  document_type: Amendment\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/78587/Cotiviti USA LLC GMI BDC FSWS Amended and Restated Schedule A May 2018.pdf\n",
      "  file_name: Cotiviti USA LLC GMI BDC FSWS Amended and Restated Schedule A May 2018.pdf\n",
      "\n",
      "--- Chunk [3] ---\n",
      "Text: Structured Deliverables Include:    \n",
      "ÔÇ∑ Daily Reports summarizing Customer, peer, industry and market performance;  \n",
      "ÔÇ∑ Communication of major ownership changes in the Customer‚Äôs equity as they are uncovered;  \n",
      "ÔÇ∑ Assistance in prioritizing institutional investor meetin gs based on investment opportuni...\n",
      "Metadata:\n",
      "  file_name: Cotiviti USA LLC GMI BDC FSWS Amended and Restated Schedule A May 2018.pdf\n",
      "  client_account: S&P Global Ratings\n",
      "  document_type: Amendment\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/78587/Cotiviti USA LLC GMI BDC FSWS Amended and Restated Schedule A May 2018.pdf\n",
      "\n",
      "--- Chunk [4] ---\n",
      "Text: DocuSign Envelope ID: 2FA44D5F-58C2-46EE-B320-7CCCBAB9F204\n",
      "3 \n",
      "Ipreo Rep: Andrew Parven  iv. Notwithstanding anything to the contrary in the Ag reement, as used herein or in the Agreement, the following terms have the meanings \n",
      "ascribed to them as follows:  \n",
      "‚ÄúBusiness to Business Communications ‚Äù sha...\n",
      "Metadata:\n",
      "  document_type: Amendment\n",
      "  client_account: S&P Global Ratings\n",
      "  file_name: Cotiviti USA LLC GMI BDC FSWS Amended and Restated Schedule A May 2018.pdf\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/78587/Cotiviti USA LLC GMI BDC FSWS Amended and Restated Schedule A May 2018.pdf\n",
      "\n",
      "--- Chunk [5] ---\n",
      "Text: If any quarterly statement is more than ninety (90) days past due, 3M may, in its sole discretion and in addition to\n",
      "rights under Section 12.2, suspend its performance pursuant to Section 4.2.4 until the quarterly statement, and all payments due thereunder, are\n",
      "brought current. 6. CONFIDENTIAL! 61 C...\n",
      "Metadata:\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/79737/3M_VAR Agmt and BAA_08232013_fully executed.pdf\n",
      "  document_type: Reseller Agreement\n",
      "  client_account: 3M Health Information System, Inc.\n",
      "  file_name: 3M_VAR Agmt and BAA_08232013_fully executed.pdf\n",
      "\n",
      "Running query 10/15: \"Are there any restrictions, prohibitions, or notice requirements related to the incorporation of open-source code into any solution?\"\n",
      "\n",
      "üîç Running RAG query for: Are there any restrictions, prohibitions, or notice requirements related to the incorporation of open-source code into any solution?\n",
      "\n",
      "\n",
      "--- Processing Query: 'Are there any restrictions, prohibitions, or notice requirements related to the incorporation of open-source code into any solution?' ---\n",
      "\n",
      "Answer:\n",
      "Based on the provided context, there is no mention of restrictions, prohibitions, or notice requirements specifically related to the incorporation of open-source code into any solution. The context includes information about contract renewals, confidentiality obligations, data processing roles, and return of materials, but it does not address anything specifically related to open-source code or its incorporation.\n",
      "\n",
      "Latency: 1216.53 ms\n",
      "\n",
      "Top 5 Chunks Returned:\n",
      "\n",
      "--- Chunk [1] ---\n",
      "Text: Upon expiration of the Initial Period , \n",
      "this Schedule shall automatically renew for a period equal to the length of such Initial Period (each, a ‚Äú Renewal Period ‚Äù), unless either Party provides \n",
      "written notice of non -renewal at least sixty (60) days prior to the expiration of such Initial Period ...\n",
      "Metadata:\n",
      "  file_name: Cotiviti USA LLC GMI BDC FSWS Amended and Restated Schedule A May 2018.pdf\n",
      "  document_type: Amendment\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/78587/Cotiviti USA LLC GMI BDC FSWS Amended and Restated Schedule A May 2018.pdf\n",
      "  client_account: S&P Global Ratings\n",
      "\n",
      "--- Chunk [2] ---\n",
      "Text: Personnel will have access only to the\n",
      "Confidential Information they need for such purposes. Each\n",
      "party will ensure that its Personnel comply with this Agreement\n",
      "and will promptly notify the other party of any breach of this\n",
      "Agreement. Each party agrees that it will take all reasonable\n",
      "measures to p...\n",
      "Metadata:\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/80694/Black Duck Software_NDA_12232014_fully executed.pdf\n",
      "  client_account: Black Duck Software\n",
      "  file_name: Black Duck Software_NDA_12232014_fully executed.pdf\n",
      "  document_type: NDA\n",
      "\n",
      "--- Chunk [3] ---\n",
      "Text: Third party Products and/or Modules , if any, may be  subjec t to additional restrictions as set forth on Annex B , if \n",
      "applicable. b. Effect of Non -Renewal of this Schedule . Upon receipt of notice of Customer‚Äôs intent not to renew this Schedule, Vendor may discontinue \n",
      "Customer‚Äôs access to the ‚ÄúE...\n",
      "Metadata:\n",
      "  document_type: Amendment\n",
      "  file_name: Cotiviti USA LLC GMI BDC FSWS Amended and Restated Schedule A May 2018.pdf\n",
      "  client_account: S&P Global Ratings\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/78587/Cotiviti USA LLC GMI BDC FSWS Amended and Restated Schedule A May 2018.pdf\n",
      "\n",
      "--- Chunk [4] ---\n",
      "Text: Each party acknowledges that a disclosing party may be irreparably damaged to the \n",
      "extent that any of the terms of this Agreement are violated and agrees that such terms shall be \n",
      "enforceable through (i) issuance of an injunction restraining the unauthorized copying, \n",
      "duplication, use, dissemination...\n",
      "Metadata:\n",
      "  client_account: Lowe's Companies\n",
      "  file_name: GRI - Cotiviti - DUA - AS - 07202022 - Dynamics_69010.pdf\n",
      "  document_type: DUA\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/69010/GRI - Cotiviti - DUA - AS - 07202022 - Dynamics_69010.pdf\n",
      "\n",
      "--- Chunk [5] ---\n",
      "Text: This Work Order Form must be signed by both parties prior to initiating the \n",
      "services described herein. The parties hereto have executed this Work Order Form as of the date set forth above.\n",
      "Metadata:\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/71455/Kaiser - Cotiviti - WOCR - TE - 04062023 - Dynamics_71455.pdf\n",
      "  client_account: Kaiser Permanente\n",
      "  document_type: Work Order / Change Request\n",
      "  file_name: Kaiser - Cotiviti - WOCR - TE - 04062023 - Dynamics_71455.pdf\n",
      "\n",
      "Running query 11/15: \"Are there any obligations to notify the client of material improvements or enhancements to a solution?\"\n",
      "\n",
      "üîç Running RAG query for: Are there any obligations to notify the client of material improvements or enhancements to a solution?\n",
      "\n",
      "\n",
      "--- Processing Query: 'Are there any obligations to notify the client of material improvements or enhancements to a solution?' ---\n",
      "\n",
      "Answer:\n",
      "The provided context does not explicitly mention any obligations to notify the client of material improvements or enhancements to a solution. However, it outlines several contractual obligations and rights regarding termination, confidentiality, and authorized actions in case of breaches, among other aspects. For example, it describes procedures and rights upon termination of agreements ([2], [3], [4]), confidentiality obligations ([5]), and data subject rights ([1]). Without additional context explicitly addressing notifications of improvements or enhancements, no definitive answer can be provided based on the available information.\n",
      "\n",
      "Latency: 1351.21 ms\n",
      "\n",
      "Top 5 Chunks Returned:\n",
      "\n",
      "--- Chunk [1] ---\n",
      "Text: Additionally, Vendor has adopted and implemented policies, procedures and systems to respond to its legal obligations and the needs of the \n",
      "Data Subject whose Relevant Personal Data is being processed (‚ÄúRelevant Data Subjects‚Äù), including the provision for Relevant  Data Subjects \n",
      "to unsubscribe/opt...\n",
      "Metadata:\n",
      "  file_name: Cotiviti USA LLC GMI BDC FSWS Amended and Restated Schedule A May 2018.pdf\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/78587/Cotiviti USA LLC GMI BDC FSWS Amended and Restated Schedule A May 2018.pdf\n",
      "  client_account: S&P Global Ratings\n",
      "  document_type: Amendment\n",
      "\n",
      "--- Chunk [2] ---\n",
      "Text: Either party may immediately terminate this Agreement by giving the other party written notice of termination upon\n",
      "the occurrence of any of the following events: (i) the other party fails to remedy a material breach of this Agreement within forty-five (45) days after the\n",
      "other party has received wri...\n",
      "Metadata:\n",
      "  client_account: 3M Health Information System, Inc.\n",
      "  file_name: 3M_VAR Agmt and BAA_08232013_fully executed.pdf\n",
      "  document_type: Reseller Agreement\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/79737/3M_VAR Agmt and BAA_08232013_fully executed.pdf\n",
      "\n",
      "--- Chunk [3] ---\n",
      "Text: As such, any definitions, obligations or requirements in the Agreement shall apply \n",
      "to this SOW, as though fully set forth herein. In the event of conflict between this SOW \n",
      "and the Agreement, the Agreement shall govern. 8.2. Each party represents and warrants to the other that this SOW has been dul...\n",
      "Metadata:\n",
      "  document_type: SOW\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/55821/SOW_Cotiviti_DM_Sep19 (002).pdf\n",
      "  client_account: Anthem, Inc (WellPoint, Inc.)\n",
      "  file_name: SOW_Cotiviti_DM_Sep19 (002).pdf\n",
      "\n",
      "--- Chunk [4] ---\n",
      "Text: Upon any material breach of this Agreement by VENDOR for which 3M has given VENDOR prior written notice\n",
      "and an opportunity to remedy such breach, 3M may, at its sole discretion, suspend its performance of any obligation related to such breach, after\n",
      "the remedy period, and such suspension may be in a...\n",
      "Metadata:\n",
      "  document_type: Reseller Agreement\n",
      "  file_name: 3M_VAR Agmt and BAA_08232013_fully executed.pdf\n",
      "  client_account: 3M Health Information System, Inc.\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/79737/3M_VAR Agmt and BAA_08232013_fully executed.pdf\n",
      "\n",
      "--- Chunk [5] ---\n",
      "Text: Each party acknowledges that a disclosing party may be irreparably damaged to the \n",
      "extent that any of the terms of this Agreement are violated and agrees that such terms shall be \n",
      "enforceable through (i) issuance of an injunction restraining the unauthorized copying, \n",
      "duplication, use, dissemination...\n",
      "Metadata:\n",
      "  client_account: Lowe's Companies\n",
      "  document_type: DUA\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/69010/GRI - Cotiviti - DUA - AS - 07202022 - Dynamics_69010.pdf\n",
      "  file_name: GRI - Cotiviti - DUA - AS - 07202022 - Dynamics_69010.pdf\n",
      "\n",
      "Running query 12/15: \"Is client approval required for implementing significant changes or upgrades to the services?\"\n",
      "\n",
      "üîç Running RAG query for: Is client approval required for implementing significant changes or upgrades to the services?\n",
      "\n",
      "\n",
      "--- Processing Query: 'Is client approval required for implementing significant changes or upgrades to the services?' ---\n",
      "\n",
      "Answer:\n",
      "The provided context does not specify whether client approval is required for implementing significant changes or upgrades to the services. The documents provided focus on various aspects such as the execution of agreements [2], [5], conversion rate calculations [3], and data processing agreements [4]. None explicitly mention policies or requirements concerning client approval for service changes or upgrades.\n",
      "\n",
      "Latency: 1286.04 ms\n",
      "\n",
      "Top 5 Chunks Returned:\n",
      "\n",
      "--- Chunk [1] ---\n",
      "Text: Additionally, Vendor has adopted and implemented policies, procedures and systems to respond to its legal obligations and the needs of the \n",
      "Data Subject whose Relevant Personal Data is being processed (‚ÄúRelevant Data Subjects‚Äù), including the provision for Relevant  Data Subjects \n",
      "to unsubscribe/opt...\n",
      "Metadata:\n",
      "  document_type: Amendment\n",
      "  file_name: Cotiviti USA LLC GMI BDC FSWS Amended and Restated Schedule A May 2018.pdf\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/78587/Cotiviti USA LLC GMI BDC FSWS Amended and Restated Schedule A May 2018.pdf\n",
      "  client_account: S&P Global Ratings\n",
      "\n",
      "--- Chunk [2] ---\n",
      "Text: This Work Order Form must be signed by both parties prior to initiating the \n",
      "services described herein. The parties hereto have executed this Work Order Form as of the date set forth above.\n",
      "Metadata:\n",
      "  document_type: Work Order / Change Request\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/71455/Kaiser - Cotiviti - WOCR - TE - 04062023 - Dynamics_71455.pdf\n",
      "  client_account: Kaiser Permanente\n",
      "  file_name: Kaiser - Cotiviti - WOCR - TE - 04062023 - Dynamics_71455.pdf\n",
      "\n",
      "--- Chunk [3] ---\n",
      "Text: For purpose s of this SOW, Conversion Rate shall be calculated by dividing the number of \n",
      "claims where a valid Recovery Opportunity is submitted in a given audit type and/or line \n",
      "of business for a given month by the total number of claims submitted by Supplier for \n",
      "review in that same month. Recove...\n",
      "Metadata:\n",
      "  document_type: SOW\n",
      "  file_name: SOW_Cotiviti_DM_Sep19 (002).pdf\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/55821/SOW_Cotiviti_DM_Sep19 (002).pdf\n",
      "  client_account: Anthem, Inc (WellPoint, Inc.)\n",
      "\n",
      "--- Chunk [4] ---\n",
      "Text: The Data Processing Agreement also does not apply to any (1) demonstration \n",
      "accounts, trials, beta releases, release preview or other similar versions of the services or (2) any features, services or products which are provided \n",
      "pursuant to a separate agreement or by a party other than Oracle (as de...\n",
      "Metadata:\n",
      "  file_name: Eliza_1_Month_Renewal_Estimate_930170.pdf\n",
      "  client_account: Oracle America, Inc\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/63293/Eliza_1_Month_Renewal_Estimate_930170.pdf\n",
      "  document_type: Quote\n",
      "\n",
      "--- Chunk [5] ---\n",
      "Text: [Signature ¬†Page ¬†Follows] ¬†\n",
      "DocuSign Envelope ID: 4B765F6E-94CE-4410-91CC-9587F38F12BE\n",
      " 5 IN ¬†WITNESS ¬†WHEREOF, ¬†the ¬†Parties ¬†hereto ¬†have ¬†caused ¬†this ¬†Agreement ¬†to ¬†be ¬†executed ¬†by ¬†their ¬†duly ¬†authorized ¬†representatives ¬†effective ¬†as ¬†of ¬†the ¬†date ¬†and ¬†year ¬†first ¬†written ¬†above.\n",
      "Metadata:\n",
      "  file_name: Dynamics_-_63126_signed.pdf\n",
      "  client_account: Westport Healthcare Advisory Group\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/63126/Dynamics_-_63126_signed.pdf\n",
      "  document_type: NDA\n",
      "\n",
      "Running query 13/15: \"Are there any restrictions or requirements related to the use of third-party vendors or subcontractors in the processing of client data?\"\n",
      "\n",
      "üîç Running RAG query for: Are there any restrictions or requirements related to the use of third-party vendors or subcontractors in the processing of client data?\n",
      "\n",
      "\n",
      "--- Processing Query: 'Are there any restrictions or requirements related to the use of third-party vendors or subcontractors in the processing of client data?' ---\n",
      "\n",
      "Answer:\n",
      "Yes, there are restrictions and requirements related to the use of third-party vendors or subcontractors in the processing of client data. Specifically, third-party products and/or modules may be subject to additional restrictions as set forth on Annex B, if applicable [2]. Additionally, there are requirements around the use of Vendor Customer Data, including the stipulation that during the term of the agreement, 3M requires the temporary access and use of such Vendor Customer Data for processing queries and returning results to the Vendor Implementation. The Vendor warrants that they have full authority to transmit, reproduce, adapt, modify, translate, and distribute the Vendor Customer Data to 3M. Furthermore, 3M shall not use or disclose Vendor Customer Data for any purpose other than to provide and support the GPCS services and will not disclose it to law enforcement unless required by law [5].\n",
      "\n",
      "Latency: 2682.28 ms\n",
      "\n",
      "Top 5 Chunks Returned:\n",
      "\n",
      "--- Chunk [1] ---\n",
      "Text: Additionally, Vendor has adopted and implemented policies, procedures and systems to respond to its legal obligations and the needs of the \n",
      "Data Subject whose Relevant Personal Data is being processed (‚ÄúRelevant Data Subjects‚Äù), including the provision for Relevant  Data Subjects \n",
      "to unsubscribe/opt...\n",
      "Metadata:\n",
      "  client_account: S&P Global Ratings\n",
      "  document_type: Amendment\n",
      "  file_name: Cotiviti USA LLC GMI BDC FSWS Amended and Restated Schedule A May 2018.pdf\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/78587/Cotiviti USA LLC GMI BDC FSWS Amended and Restated Schedule A May 2018.pdf\n",
      "\n",
      "--- Chunk [2] ---\n",
      "Text: Third party Products and/or Modules , if any, may be  subjec t to additional restrictions as set forth on Annex B , if \n",
      "applicable. b. Effect of Non -Renewal of this Schedule . Upon receipt of notice of Customer‚Äôs intent not to renew this Schedule, Vendor may discontinue \n",
      "Customer‚Äôs access to the ‚ÄúE...\n",
      "Metadata:\n",
      "  document_type: Amendment\n",
      "  client_account: S&P Global Ratings\n",
      "  file_name: Cotiviti USA LLC GMI BDC FSWS Amended and Restated Schedule A May 2018.pdf\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/78587/Cotiviti USA LLC GMI BDC FSWS Amended and Restated Schedule A May 2018.pdf\n",
      "\n",
      "--- Chunk [3] ---\n",
      "Text: This indemnification does not extend to VENDOR\n",
      "Customers, and is not assignable or intended to be flowed- down to VENDOR Customers; however, subject to Section 10, nothing prohibits VENDOR from\n",
      "making an indemnification to VENDOR Customers that Is consistent with this indemnification so long as no V...\n",
      "Metadata:\n",
      "  client_account: 3M Health Information System, Inc.\n",
      "  file_name: 3M_VAR Agmt and BAA_08232013_fully executed.pdf\n",
      "  document_type: Reseller Agreement\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/79737/3M_VAR Agmt and BAA_08232013_fully executed.pdf\n",
      "\n",
      "--- Chunk [4] ---\n",
      "Text: In the event 3M is compelled to disclose\n",
      "VENDOR Customer Data to law enforcement, 3M will use commercially reasonable efforts to notify VENDOR in advance of such disclosure\n",
      "untess legally prohibited. 3M agrees that VENDOR Customer Data which 3M receives shall remain in the United States of America. ...\n",
      "Metadata:\n",
      "  file_name: 3M_VAR Agmt and BAA_08232013_fully executed.pdf\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/79737/3M_VAR Agmt and BAA_08232013_fully executed.pdf\n",
      "  document_type: Reseller Agreement\n",
      "  client_account: 3M Health Information System, Inc.\n",
      "\n",
      "--- Chunk [5] ---\n",
      "Text: 3M also grants VENDOR,\n",
      "anonexclusive, nontransferable, license to make available the Connectivity Materials and all or any part of the textual material from the 3M Documents,\n",
      "to VENDOR‚Äôs wholly-owned foreign subsidiaries located in the countries of India and Ukraine, and to VENDOR's development cent...\n",
      "Metadata:\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/79737/3M_VAR Agmt and BAA_08232013_fully executed.pdf\n",
      "  document_type: Reseller Agreement\n",
      "  client_account: 3M Health Information System, Inc.\n",
      "  file_name: 3M_VAR Agmt and BAA_08232013_fully executed.pdf\n",
      "\n",
      "Running query 14/15: \"Are there any clauses requiring explicit client consent for the use of their data in specific ways, such as for R&D purposes or AI training?\"\n",
      "\n",
      "üîç Running RAG query for: Are there any clauses requiring explicit client consent for the use of their data in specific ways, such as for R&D purposes or AI training?\n",
      "\n",
      "\n",
      "--- Processing Query: 'Are there any clauses requiring explicit client consent for the use of their data in specific ways, such as for R&D purposes or AI training?' ---\n",
      "\n",
      "Answer:\n",
      "The provided context does not explicitly mention any clauses requiring explicit client consent for the use of their data for specific purposes such as R&D or AI training. The document from Cotiviti mentions policies and procedures for data subjects to unsubscribe or opt-out from the database and marketing communications, which indicates a focus on permission regarding communications, but does not specifically address consent for using data for R&D or AI training purposes [1].  \n",
      "\n",
      "Additionally, the context outlines various agreements and terms related to services, fees, and use types, but does not include specifics on obtaining client consent for data usage beyond the defined contexts [2], [3], [4], [5].\n",
      "\n",
      "Latency: 2016.06 ms\n",
      "\n",
      "Top 5 Chunks Returned:\n",
      "\n",
      "--- Chunk [1] ---\n",
      "Text: Additionally, Vendor has adopted and implemented policies, procedures and systems to respond to its legal obligations and the needs of the \n",
      "Data Subject whose Relevant Personal Data is being processed (‚ÄúRelevant Data Subjects‚Äù), including the provision for Relevant  Data Subjects \n",
      "to unsubscribe/opt...\n",
      "Metadata:\n",
      "  file_name: Cotiviti USA LLC GMI BDC FSWS Amended and Restated Schedule A May 2018.pdf\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/78587/Cotiviti USA LLC GMI BDC FSWS Amended and Restated Schedule A May 2018.pdf\n",
      "  document_type: Amendment\n",
      "  client_account: S&P Global Ratings\n",
      "\n",
      "--- Chunk [2] ---\n",
      "Text: The Sit e may be changed \n",
      "to another URL by Vendor in its sole discretion, provided that Vendor shall provide Customer with notice thereof o r otherwise redirect Customer \n",
      "to such new URL for a reasonable amount of time. Modules/Datasets  Include:  \n",
      "ÔÇ∑ Global Equity Contact and Ownership Data, Excel ...\n",
      "Metadata:\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/78587/Cotiviti USA LLC GMI BDC FSWS Amended and Restated Schedule A May 2018.pdf\n",
      "  document_type: Amendment\n",
      "  client_account: S&P Global Ratings\n",
      "  file_name: Cotiviti USA LLC GMI BDC FSWS Amended and Restated Schedule A May 2018.pdf\n",
      "\n",
      "--- Chunk [3] ---\n",
      "Text: [Signature ¬†Page ¬†Follows] ¬†\n",
      "DocuSign Envelope ID: 4B765F6E-94CE-4410-91CC-9587F38F12BE\n",
      " 5 IN ¬†WITNESS ¬†WHEREOF, ¬†the ¬†Parties ¬†hereto ¬†have ¬†caused ¬†this ¬†Agreement ¬†to ¬†be ¬†executed ¬†by ¬†their ¬†duly ¬†authorized ¬†representatives ¬†effective ¬†as ¬†of ¬†the ¬†date ¬†and ¬†year ¬†first ¬†written ¬†above.\n",
      "Metadata:\n",
      "  document_type: NDA\n",
      "  file_name: Dynamics_-_63126_signed.pdf\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/63126/Dynamics_-_63126_signed.pdf\n",
      "  client_account: Westport Healthcare Advisory Group\n",
      "\n",
      "--- Chunk [4] ---\n",
      "Text: C. Annual Software Developers License Fee:\n",
      "\n",
      "LICENSE Fee\n",
      "\n",
      "   \n",
      "    \n",
      "\n",
      " \n",
      "\n",
      "GPCS DSF _| Grouper Plus Content Services Development Support Fee $6,01 2.00\n",
      "\n",
      " \n",
      "\f\n",
      "@M VALUE ADDED RESELLER SOFTWARE AGREEMENT Pace 15 oF 24\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "EXHIBIT B\n",
      "VENDOR SOFTWARE\n",
      "VENDOR Software\n",
      "product Use Type #1 Use Typ...\n",
      "Metadata:\n",
      "  document_type: Reseller Agreement\n",
      "  client_account: 3M Health Information System, Inc.\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/79737/3M_VAR Agmt and BAA_08232013_fully executed.pdf\n",
      "  file_name: 3M_VAR Agmt and BAA_08232013_fully executed.pdf\n",
      "\n",
      "--- Chunk [5] ---\n",
      "Text: The Data Processing Agreement also does not apply to any (1) demonstration \n",
      "accounts, trials, beta releases, release preview or other similar versions of the services or (2) any features, services or products which are provided \n",
      "pursuant to a separate agreement or by a party other than Oracle (as de...\n",
      "Metadata:\n",
      "  file_name: Eliza_1_Month_Renewal_Estimate_930170.pdf\n",
      "  document_type: Quote\n",
      "  client_account: Oracle America, Inc\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/63293/Eliza_1_Month_Renewal_Estimate_930170.pdf\n",
      "\n",
      "Running query 15/15: \"Does the client have any rights to audit our data processing practices or AI model training processes?\"\n",
      "\n",
      "üîç Running RAG query for: Does the client have any rights to audit our data processing practices or AI model training processes?\n",
      "\n",
      "\n",
      "--- Processing Query: 'Does the client have any rights to audit our data processing practices or AI model training processes?' ---\n",
      "\n",
      "Answer:\n",
      "The provided context does not contain any explicit details regarding the rights of a client to audit data processing practices or AI model training processes. Therefore, based on the given information, no mention is made of any client having such audit rights in relation to data processing or AI model training.\n",
      "\n",
      "Latency: 1255.64 ms\n",
      "\n",
      "Top 5 Chunks Returned:\n",
      "\n",
      "--- Chunk [1] ---\n",
      "Text: Additionally, Vendor has adopted and implemented policies, procedures and systems to respond to its legal obligations and the needs of the \n",
      "Data Subject whose Relevant Personal Data is being processed (‚ÄúRelevant Data Subjects‚Äù), including the provision for Relevant  Data Subjects \n",
      "to unsubscribe/opt...\n",
      "Metadata:\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/78587/Cotiviti USA LLC GMI BDC FSWS Amended and Restated Schedule A May 2018.pdf\n",
      "  file_name: Cotiviti USA LLC GMI BDC FSWS Amended and Restated Schedule A May 2018.pdf\n",
      "  document_type: Amendment\n",
      "  client_account: S&P Global Ratings\n",
      "\n",
      "--- Chunk [2] ---\n",
      "Text: The Sit e may be changed \n",
      "to another URL by Vendor in its sole discretion, provided that Vendor shall provide Customer with notice thereof o r otherwise redirect Customer \n",
      "to such new URL for a reasonable amount of time. Modules/Datasets  Include:  \n",
      "ÔÇ∑ Global Equity Contact and Ownership Data, Excel ...\n",
      "Metadata:\n",
      "  document_type: Amendment\n",
      "  file_name: Cotiviti USA LLC GMI BDC FSWS Amended and Restated Schedule A May 2018.pdf\n",
      "  client_account: S&P Global Ratings\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/78587/Cotiviti USA LLC GMI BDC FSWS Amended and Restated Schedule A May 2018.pdf\n",
      "\n",
      "--- Chunk [3] ---\n",
      "Text: Oracle America, Inc.\n",
      "2300 Oracle Way\n",
      "Austin, TX 78741\n",
      "800 762 5524\n",
      "www.netsuite.com  Page 1 of 4Estimate\n",
      "     Date 10/29/2021\n",
      "     Estimate # 930170\n",
      "       \n",
      "       \n",
      "       \n",
      "Customer Name & Address    \n",
      "Eliza Corporation\n",
      "75 Sylvan St\n",
      "Danvers MA 01923\n",
      "United States   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "Item Qty Descripti...\n",
      "Metadata:\n",
      "  file_name: Eliza_1_Month_Renewal_Estimate_930170.pdf\n",
      "  client_account: Oracle America, Inc\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/63293/Eliza_1_Month_Renewal_Estimate_930170.pdf\n",
      "  document_type: Quote\n",
      "\n",
      "--- Chunk [4] ---\n",
      "Text: The Data Processing Agreement also does not apply to any (1) demonstration \n",
      "accounts, trials, beta releases, release preview or other similar versions of the services or (2) any features, services or products which are provided \n",
      "pursuant to a separate agreement or by a party other than Oracle (as de...\n",
      "Metadata:\n",
      "  document_type: Quote\n",
      "  client_account: Oracle America, Inc\n",
      "  file_name: Eliza_1_Month_Renewal_Estimate_930170.pdf\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/63293/Eliza_1_Month_Renewal_Estimate_930170.pdf\n",
      "\n",
      "--- Chunk [5] ---\n",
      "Text: Terms of Your Order\n",
      "1. Agreement\n",
      "Except as set forth above, the terms and conditions of the applicable agreement between you and Oracle (including any updated URL Terms or \n",
      "other applicable web based terms in effect as of the date of this document) shall apply to the products and/or services set for...\n",
      "Metadata:\n",
      "  document_type: Quote\n",
      "  file_name: Eliza_1_Month_Renewal_Estimate_930170.pdf\n",
      "  client_account: Oracle America, Inc\n",
      "  s3_path: s3://ml-legal-restricted/contract-docs/63293/Eliza_1_Month_Renewal_Estimate_930170.pdf\n",
      "\n",
      "Results saved to 'rag_batch_output.txt'\n"
     ]
    }
   ],
   "source": [
    "def run_batch_rag_queries(queries, top_k=5):\n",
    "    all_results = []\n",
    "\n",
    "    for i, query in enumerate(queries, 1):\n",
    "        print(f\"\\nRunning query {i}/{len(queries)}: \\\"{query}\\\"\")\n",
    "        answer, refs, latency, chunks = run_query_pipeline(query, top_k=top_k)\n",
    "\n",
    "        result_entry = {\n",
    "            \"query\": query,\n",
    "            \"answer\": answer,\n",
    "            \"latency_ms\": latency,\n",
    "            \"refs\": refs,\n",
    "            \"chunks\": chunks\n",
    "        }\n",
    "\n",
    "        all_results.append(result_entry)\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def write_rag_results_to_file(results, file_path):\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, result in enumerate(results, 1):\n",
    "            f.write(f\"\\n=== Query {i} ===\\n\")\n",
    "            f.write(f\"Query: {result['query']}\\n\")\n",
    "            f.write(f\"Latency: {result['latency_ms']} ms\\n\")\n",
    "\n",
    "            f.write(\"\\nAnswer:\\n\")\n",
    "            f.write(result[\"answer\"] + \"\\n\")\n",
    "\n",
    "            f.write(\"\\nSource References:\\n\")\n",
    "            for ref, meta in result[\"refs\"].items():\n",
    "                f.write(f\"{ref}:\\n\")\n",
    "                for k, v in meta.items():\n",
    "                    if k == \"text\":\n",
    "                        preview = v[:200].strip().replace(\"\\n\", \" \")\n",
    "                        f.write(f\"  text: {preview}...\\n\" if len(v) > 200 else f\"  text: {preview}\\n\")\n",
    "                    else:\n",
    "                        f.write(f\"  {k}: {v}\\n\")\n",
    "\n",
    "            f.write(\"\\nChunks:\\n\")\n",
    "            for j, chunk in enumerate(result[\"chunks\"], 1):\n",
    "                meta = chunk.get(\"metadata\", {})\n",
    "                f.write(f\"Chunk [{j}]:\\n\")\n",
    "                f.write(f\"  file_name: {meta.get('file_name')}\\n\")\n",
    "                f.write(f\"  client_account: {meta.get('client_account')}\\n\")\n",
    "                f.write(f\"  document_type: {meta.get('document_type')}\\n\")\n",
    "                f.write(f\"  s3_path: {meta.get('s3_path')}\\n\")\n",
    "                preview = meta.get(\"text\", \"\").replace(\"\\n\", \" \")[:300]\n",
    "                f.write(f\"  text preview: {preview}...\\n\" if len(preview) == 300 else f\"  text preview: {preview}\\n\")\n",
    "                f.write(\"-\" * 60 + \"\\n\")\n",
    "            f.write(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "\n",
    "doc_questions = [\n",
    "    \"Can we use client data (including PHI) to develop or test new services, and enhance, improve, or modify our existing services?\",\n",
    "    \"Can we use aggregated client data or de-identified client data for the purposes of developing, testing, enhancing, improving, or modifying services?\",\n",
    "    \"Are there any restrictions on using artificial intelligence or machine learning in delivering the services?\",\n",
    "    \"Are there any specific terms regarding the use of client data for training and fine-tuning AI models?\",\n",
    "    \"Are there any limitations on storing client data (including PHI) in the cloud?\",\n",
    "    \"Are there any restrictions on using cloud services to assist in the processing of client data (including PHI)?\",\n",
    "    \"Is there any requirement for human oversight or decision-making in the use of AI for delivering the services?\",\n",
    "    \"Does the client have any ownership rights in developed IP or developed materials generated from the use of their data?\",\n",
    "    \"Are there any specific terms regarding the ownership and usage rights of IP developed through the use of client data?\",\n",
    "    \"Are there any restrictions, prohibitions, or notice requirements related to the incorporation of open-source code into any solution?\",\n",
    "    \"Are there any obligations to notify the client of material improvements or enhancements to a solution?\",\n",
    "    \"Is client approval required for implementing significant changes or upgrades to the services?\",\n",
    "    \"Are there any restrictions or requirements related to the use of third-party vendors or subcontractors in the processing of client data?\",\n",
    "    \"Are there any clauses requiring explicit client consent for the use of their data in specific ways, such as for R&D purposes or AI training?\",\n",
    "    \"Does the client have any rights to audit our data processing practices or AI model training processes?\"\n",
    "]\n",
    "\n",
    "all_results = run_batch_rag_queries(doc_questions, top_k=5)\n",
    "output_file=\"rag_batch_output.txt\"\n",
    "write_rag_results_to_file(all_results, output_file)\n",
    "print(f\"\\nResults saved to '{output_file}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c17d824",
   "metadata": {},
   "source": [
    "AWS open-search-S3 vector mirroring and querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6b243d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "from requests_aws4auth import AWS4Auth\n",
    "\n",
    "region = 'us-east-1'\n",
    "host = 'g7a6yvmq4wc43rvrzp89.us-east-1.aoss.amazonaws.com'\n",
    "\n",
    "session = boto3.Session()\n",
    "credentials = session.get_credentials().get_frozen_credentials()\n",
    "\n",
    "awsauth = AWS4Auth(\n",
    "    credentials.access_key,\n",
    "    credentials.secret_key,\n",
    "    region,\n",
    "    'aoss',\n",
    "    session_token=credentials.token\n",
    ")\n",
    "\n",
    "opensearch_client = OpenSearch(\n",
    "    hosts=[{'host': host, 'port': 443}],\n",
    "    http_auth=awsauth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection\n",
    ")\n",
    "\n",
    "index_name = 'token-chunking'  # Use hyphen or underscore, avoid spaces\n",
    "\n",
    "# Check if index exists, returns True or False\n",
    "index_exists = opensearch_client.indices.exists(index=index_name)\n",
    "\n",
    "if index_exists:\n",
    "    print(f\"Index '{index_name}' exists.\")\n",
    "else:\n",
    "    print(f\"Index '{index_name}' does NOT exist.\")\n",
    "\n",
    "def query_opensearch_vector_store(query_text, index_name, embed_model, vector_field='embedding', top_k=5):\n",
    "    print(f\"\\n--- Querying OpenSearch: '{query_text}' ---\")\n",
    "\n",
    "    query_vector = embed_model.get_text_embedding(query_text)\n",
    "\n",
    "    query_body = {\n",
    "        \"size\": top_k,\n",
    "        \"query\": {\n",
    "            \"knn\": {\n",
    "                \"embedding\": {\n",
    "                    \"vector\": query_vector,\n",
    "                    \"k\": top_k\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = opensearch_client.search(index=index_name, body=query_body)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying OpenSearch: {e}\")\n",
    "        return None\n",
    "\n",
    "response = query_opensearch_vector_store(\"climate change effects\", \"token-chunking\", embed_model)\n",
    "if response:\n",
    "    for hit in response[\"hits\"][\"hits\"]:\n",
    "        print(f\"Score: {hit['_score']}, Metadata: {hit['_source'].get('metadata')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1c8664",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
