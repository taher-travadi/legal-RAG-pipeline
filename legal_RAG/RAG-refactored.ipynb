{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c93264a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Downloading Excel: s3://ml-legal-restricted/tabularData/Active Legal Contracts 8-1-2025 10-54-06 AM.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Processing 14644 contracts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14644/14644 [06:47<00:00, 35.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Contracts with matching files saved to: contracts_files.csv\n",
      "Total contracts with matches: 12676\n",
      "Total matched file rows: 12676\n",
      "\n",
      "‚ùó Could not find matching S3 files for 1968 contracts.\n",
      "Details saved to: missing_files.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from urllib.parse import urlparse, parse_qs, urlunparse, unquote\n",
    "\n",
    "S3_BUCKET = \"ml-legal-restricted\"\n",
    "EXCEL_KEY = \"tabularData/Active Legal Contracts 8-1-2025 10-54-06 AM.xlsx\"\n",
    "OUTPUT_CSV = \"contracts_files.csv\"\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "COLUMN_MAP = {\n",
    "    \"Contract Number\": \"contract_number\",\n",
    "    \"Account\": \"account_name\",\n",
    "    \"Document Type\": \"doc_type\",\n",
    "    \"Status Reason\": \"status_reason\",\n",
    "    \"Contract Title\": \"contract_title\",\n",
    "    \"Contract Requester\": \"contract_requester\",\n",
    "    \"Reviewing Attorney\": \"reviewing_attorney\",\n",
    "    \"Created On\": \"created_on\",\n",
    "    \"Document Effective Date\": \"document_effective_date\",\n",
    "    \"Parent Contract\": \"parent_contract\",\n",
    "    \"Solution Line\": \"solution_line\",\n",
    "    \"Account Type\": \"account_type\",\n",
    "    \"Document URL\": \"sharepoint_document_url\",\n",
    "    \"Document Title\": \"document_title\",\n",
    "    \"Related Product\": \"related_product\",\n",
    "}\n",
    "\n",
    "def normalize_url(u: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize URLs so that small formatting differences don't break matches.\n",
    "    - Strip whitespace\n",
    "    - Lowercase scheme/host\n",
    "    - Decode % encodings on the path (so %20 -> space)\n",
    "    - Remove trailing slashes on the path\n",
    "    \"\"\"\n",
    "    if not isinstance(u, str):\n",
    "        return \"\"\n",
    "    u = u.strip()\n",
    "    if not u:\n",
    "        return \"\"\n",
    "    try:\n",
    "        p = urlparse(u)\n",
    "        scheme = (p.scheme or \"\").lower()\n",
    "        netloc = (p.netloc or \"\").lower()\n",
    "        path = unquote(p.path or \"\")\n",
    "        if path.endswith(\"/\") and len(path) > 1:\n",
    "            path = path.rstrip(\"/\")\n",
    "        return urlunparse((scheme, netloc, path, p.params, p.query, p.fragment))\n",
    "    except Exception:\n",
    "        return u\n",
    "\n",
    "def fetch_excel_data(bucket: str, key: str, sheet_name=\"Active Legal Contracts\") -> pd.DataFrame:\n",
    "    \"\"\"Download and parse Excel file from S3, select & rename desired columns, normalize URL, add file_name.\"\"\"\n",
    "    print(f\"üì• Downloading Excel: s3://{bucket}/{key}\")\n",
    "    try:\n",
    "        excel_data = s3.get_object(Bucket=bucket, Key=key)[\"Body\"].read()\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to download Excel from s3://{bucket}/{key}: {e}\")\n",
    "\n",
    "    df = pd.read_excel(io.BytesIO(excel_data), sheet_name=sheet_name, engine=\"openpyxl\")\n",
    "    missing_cols = [col for col in COLUMN_MAP.keys() if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing columns in Excel: {missing_cols}\")\n",
    "    \n",
    "    df = df[list(COLUMN_MAP.keys())].dropna(subset=[\"Contract Number\"])\n",
    "    df[\"Contract Number\"] = df[\"Contract Number\"].astype(str).str.strip()\n",
    "\n",
    "    df = df.rename(columns=COLUMN_MAP)\n",
    "\n",
    "    df[\"sharepoint_document_url\"] = (\n",
    "        df[\"sharepoint_document_url\"].fillna(\"\").astype(str).apply(normalize_url)\n",
    "    )\n",
    "\n",
    "    df = df[df[\"sharepoint_document_url\"].str.strip() != \"\"]\n",
    "\n",
    "    def _filename_from_url(u: str) -> str:\n",
    "        try:\n",
    "            p = urlparse(u)\n",
    " \n",
    "            qs = parse_qs(p.query)\n",
    "            if \"file\" in qs and qs[\"file\"]:\n",
    "                return unquote(qs[\"file\"][0])\n",
    "            path = unquote(p.path or \"\")\n",
    "            return os.path.basename(path)\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "\n",
    "    df[\"file_name\"] = df[\"sharepoint_document_url\"].apply(_filename_from_url)\n",
    "\n",
    "    return df\n",
    "\n",
    "def find_matching_contract_file(\n",
    "    bucket: str,\n",
    "    contract_number: str,\n",
    "    target_filename: str,\n",
    "    prefix_base=\"contract-docs/\",\n",
    ") -> str | None:\n",
    "    \"\"\"\n",
    "    Search under contract-docs/{contract_number}/ and return the *single* key\n",
    "    that exactly matches the target file name. If not found, return None.\n",
    "    \"\"\"\n",
    "    if not target_filename:\n",
    "        return None\n",
    "\n",
    "    prefix = f\"{prefix_base}{contract_number}/\"\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "\n",
    "    try:\n",
    "        for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "            for obj in page.get(\"Contents\", []):\n",
    "                key = obj.get(\"Key\", \"\")\n",
    "                if not key or key.endswith(\"/\"):\n",
    "                    continue\n",
    "                if os.path.basename(key) == target_filename:\n",
    "                    return key\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error listing S3 for prefix {prefix}: {e}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "def process_contracts_with_metadata(bucket: str, excel_key: str, output_csv: str):\n",
    "    try:\n",
    "        df = fetch_excel_data(bucket, excel_key)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to read Excel: {e}\")\n",
    "        return\n",
    "\n",
    "    output_rows = []\n",
    "    not_found_rows = []\n",
    "\n",
    "    print(f\"\\nüîç Processing {len(df)} contracts...\")\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing\"):\n",
    "        try:\n",
    "            contract_number = str(row.get(\"contract_number\", \"\")).strip()\n",
    "            file_name = str(row.get(\"file_name\", \"\")).strip()\n",
    "\n",
    "            key = find_matching_contract_file(bucket, contract_number, file_name)\n",
    "\n",
    "            if not key and contract_number.isdigit() and len(contract_number) < 8:\n",
    "                padded = contract_number.zfill(8)\n",
    "                key = find_matching_contract_file(bucket, padded, file_name)\n",
    "\n",
    "            if key:\n",
    "                full_path = f\"s3://{bucket}/{key}\"\n",
    "                entry = {\"contract_number\": contract_number, \"s3_full_path\": full_path}\n",
    "                for csv_col in COLUMN_MAP.values():\n",
    "                    entry[csv_col] = row.get(csv_col, \"\")\n",
    "                entry[\"file_name\"] = file_name\n",
    "                output_rows.append(entry)\n",
    "            else:\n",
    "                not_found_rows.append(\n",
    "                    {\n",
    "                        \"contract_number\": contract_number,\n",
    "                        \"file_name\": file_name,\n",
    "                        \"sharepoint_document_url\": row.get(\"sharepoint_document_url\", \"\"),\n",
    "                    }\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Error processing contract '{row.get('contract_number', '')}': {e}\")\n",
    "\n",
    "    output_df = pd.DataFrame(output_rows)\n",
    "\n",
    "    ordered_cols = (\n",
    "        [\"contract_number\", \"s3_full_path\"]\n",
    "        + list(COLUMN_MAP.values())\n",
    "        + [\"file_name\"]\n",
    "    )\n",
    "\n",
    "    seen = set()\n",
    "    ordered_cols = [c for c in ordered_cols if not (c in seen or seen.add(c))]\n",
    "    if not output_df.empty:\n",
    "        output_df = output_df.reindex(columns=ordered_cols, fill_value=\"\")\n",
    "\n",
    "    try:\n",
    "        output_df.to_csv(output_csv, index=False)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to write output CSV '{output_csv}': {e}\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n‚úÖ Contracts with matching files saved to: {output_csv}\")\n",
    "    print(f\"Total contracts with matches: {output_df['contract_number'].nunique() if not output_df.empty else 0}\")\n",
    "    print(f\"Total matched file rows: {len(output_df)}\")\n",
    "\n",
    "    if not_found_rows:\n",
    "        missing_df = pd.DataFrame(not_found_rows)\n",
    "        missing_df.to_csv('missing_files.csv', index=False)\n",
    "        print(f\"\\n‚ùó Could not find matching S3 files for {len(missing_df)} contracts.\")\n",
    "        print(f\"Details saved to: {'missing_files.csv'}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_contracts_with_metadata(S3_BUCKET, EXCEL_KEY, OUTPUT_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0393e724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "EMBEDINNGS_URL = \"https://zgggzg2iqg.execute-api.us-east-1.amazonaws.com/dev/get_embeddings\"\n",
    "API_KEY = \"2jIpWCyNRg3Y8lkbmWG0tkyXwYlJn5QaZ1F3yKf7\"\n",
    "\n",
    "def _extract_embeddings_obj(obj):\n",
    "\n",
    "    if isinstance(obj, dict) and \"embeddings\" in obj:\n",
    "        return obj[\"embeddings\"]\n",
    "\n",
    "    if isinstance(obj, dict) and \"body\" in obj:\n",
    "        try:\n",
    "            body = obj[\"body\"]\n",
    "            if isinstance(body, str):\n",
    "                inner = json.loads(body)\n",
    "            else:\n",
    "                inner = body\n",
    "            if isinstance(inner, dict) and \"embeddings\" in inner:\n",
    "                return inner[\"embeddings\"]\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    raise KeyError(\"No 'embeddings' found in response object\")\n",
    "\n",
    "def get_text_embedding(texts, model='e5_mistral_embed_384', timeout=8):\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    if not isinstance(texts, list) or not texts:\n",
    "        raise ValueError(\"Input 'texts' must be a non-empty list of strings.\")\n",
    "\n",
    "    headers = {\n",
    "        \"x-api-key\": API_KEY,\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    out = []\n",
    "    for text in texts:\n",
    "        if not isinstance(text, str):\n",
    "            raise ValueError(\"Each item in 'texts' must be a string.\")\n",
    "\n",
    "        payload = {\"model_name\": model, \"texts\": [text]}\n",
    "\n",
    "        try:\n",
    "            resp = requests.post(EMBEDINNGS_URL, json=payload, headers=headers, timeout=timeout)\n",
    "            resp.raise_for_status()\n",
    "\n",
    "            obj = resp.json()\n",
    "            embeddings = _extract_embeddings_obj(obj)\n",
    "\n",
    "            if (not isinstance(embeddings, list)) or len(embeddings) != 1 or (not isinstance(embeddings[0], list)):\n",
    "                raise KeyError(\"Response did not contain a single embedding vector\")\n",
    "\n",
    "            vec = np.array(embeddings[0], dtype=np.float32).tolist()\n",
    "            out.append(vec)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to get embedding for '{text}': {e}\")\n",
    "            try:\n",
    "                print(f\"[DEBUG] HTTP {resp.status_code} body: {resp.text[:500]}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "            out.append(None)\n",
    "\n",
    "    return out[0] if len(out) == 1 else out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "551e2583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import pandas as pd\n",
    "from functools import lru_cache\n",
    "from typing import Dict, List\n",
    "import copy\n",
    "import math\n",
    "\n",
    "\n",
    "def _ensure_nltk():\n",
    "    try:\n",
    "        import nltk\n",
    "        try:\n",
    "            nltk.data.find(\"tokenizers/punkt\")\n",
    "        except LookupError:\n",
    "            nltk.download(\"punkt\")\n",
    "        try:\n",
    "            nltk.data.find(\"tokenizers/punkt_tab\")\n",
    "        except LookupError:\n",
    "            try:\n",
    "                nltk.download(\"punkt_tab\")\n",
    "            except Exception:\n",
    "                pass\n",
    "        return nltk\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _sent_tokenize(text: str) -> List[str]:\n",
    "    nltk = _ensure_nltk()\n",
    "    if nltk is not None:\n",
    "        from nltk.tokenize import sent_tokenize\n",
    "        return sent_tokenize(text)\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def get_tokenizer(tokenizer_name: str = \"intfloat/e5-small-v2\"):\n",
    "    from transformers import AutoTokenizer\n",
    "    return AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "EXCLUDE_KEYS = {\"file_name\", \"contract_number\", \"opensearch\", \"s3_vectors\"}\n",
    "OLD_KEYS_TO_PRUNE = {\n",
    "    \"created_on\", \"document_effective_date\", \"contract_requester\",\n",
    "    \"reviewing_attorney\", \"client_account\", \"parent_contract\",\n",
    "    \"account_type\", \"related_product\", \"sharepoint_document_url\", \"document_title\"\n",
    "}\n",
    "\n",
    "def _sanitize_value(v):\n",
    "    import math\n",
    "    if v is None:\n",
    "        return \"None\"\n",
    "    if isinstance(v, float):\n",
    "        if math.isnan(v):\n",
    "            return \"None\"\n",
    "        if v.is_integer():\n",
    "            return str(int(v))\n",
    "        else:\n",
    "            return str(v)\n",
    "    if isinstance(v, (int, str)):\n",
    "        return str(v)\n",
    "    return str(v)\n",
    "\n",
    "\n",
    "def _shape_metadata(meta: Dict) -> Dict:\n",
    "\n",
    "    for k in list(meta.keys()):\n",
    "        if k in EXCLUDE_KEYS:\n",
    "            del meta[k]\n",
    "\n",
    "    created_on = meta.get(\"created_on\", \"None\")\n",
    "    document_effective = meta.get(\"document_effective_date\", \"None\")\n",
    "    requester = meta.get(\"contract_requester\", \"None\")\n",
    "    reviewer = meta.get(\"reviewing_attorney\", \"None\")\n",
    "    account_name = meta.get(\"account_name\", \"None\")\n",
    "    parent_contract = meta.get(\"parent_contract\", \"None\")\n",
    "    account_type = meta.get(\"account_type\", \"None\")\n",
    "    related_product = meta.get(\"related_product\", \"None\")\n",
    "    document_title = meta.get(\"document_title\", \"None\")\n",
    "    solution_line = meta.get(\"solution_line\", \"None\")\n",
    "\n",
    "    meta['solution_line'] = solution_line\n",
    "    meta[\"dates\"] = [d for d in [created_on, document_effective, document_title]]\n",
    "    meta[\"attorneys\"] = [a for a in [requester, reviewer]]\n",
    "    meta[\"account_details\"] = [a for a in [account_name, parent_contract, account_type, related_product]]\n",
    "\n",
    "    for k in list(meta.keys()):\n",
    "        if k in OLD_KEYS_TO_PRUNE:\n",
    "            del meta[k]\n",
    "    \n",
    "    for k, v in meta.items():\n",
    "        if isinstance(v, list):\n",
    "            meta[k] = [_sanitize_value(x) for x in v]\n",
    "        else:\n",
    "            meta[k] = _sanitize_value(v)\n",
    "\n",
    "    return meta\n",
    "\n",
    "def chunk_text_and_build_chunks(\n",
    "    text: str,\n",
    "    meta: Dict,\n",
    "    token_limit: int = 400,\n",
    "    safety_margin: int = 16,\n",
    "    tokenizer_name: str = \"intfloat/e5-small-v2\"\n",
    ") -> List[Dict]:\n",
    "\n",
    "    if not text or not isinstance(text, str):\n",
    "        return []\n",
    "\n",
    "    tokenizer = get_tokenizer(tokenizer_name)\n",
    "    model_max = getattr(tokenizer, \"model_max_length\", 512)\n",
    "    if model_max is None or model_max > 4096:\n",
    "        model_max = 512\n",
    "    hard_cap = max(16, min(token_limit, model_max - safety_margin))\n",
    "    sents = _sent_tokenize(text)\n",
    "\n",
    "    chunks_text: List[str] = []\n",
    "    current_sents: List[str] = []\n",
    "    tok_count = 0\n",
    "\n",
    "    for sent in sents:\n",
    "        sent_ids = tokenizer.encode(sent, add_special_tokens=False)\n",
    "        tlen = len(sent_ids)\n",
    "\n",
    "        # Flush current chunk if adding sentence exceeds limit\n",
    "        if current_sents and tok_count + tlen > hard_cap:\n",
    "            candidate_chunk = \" \".join(current_sents)\n",
    "            # Double check token length does NOT exceed hard_cap\n",
    "            candidate_ids = tokenizer.encode(candidate_chunk, add_special_tokens=False)\n",
    "            if len(candidate_ids) > hard_cap:\n",
    "                # If too long, maybe chunk sentence by sentence or split differently\n",
    "                pass\n",
    "            else:\n",
    "                chunks_text.append(candidate_chunk)\n",
    "                current_sents = []\n",
    "                tok_count = 0\n",
    "\n",
    "        # If single sentence too long, split by token windows\n",
    "        if tlen > hard_cap:\n",
    "            if current_sents:\n",
    "                chunks_text.append(\" \".join(current_sents))\n",
    "                current_sents = []\n",
    "                tok_count = 0\n",
    "\n",
    "            start = 0\n",
    "            while start < tlen:\n",
    "                end = min(start + hard_cap, tlen)\n",
    "                seg_ids = sent_ids[start:end]\n",
    "                seg_text = tokenizer.decode(seg_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "                if not seg_text.strip():\n",
    "                    # Fallback: slice raw text chars approx\n",
    "                    seg_text = sent[start:start+1000]\n",
    "                    if not seg_text:\n",
    "                        break\n",
    "\n",
    "                chunks_text.append(seg_text)\n",
    "                start += hard_cap\n",
    "\n",
    "            continue\n",
    "\n",
    "        # Normal case: add sentence to current chunk\n",
    "        current_sents.append(sent)\n",
    "        tok_count += tlen\n",
    "\n",
    "    if current_sents:\n",
    "        chunks_text.append(\" \".join(current_sents))\n",
    "\n",
    "    # Build final chunk objects, copy metadata to avoid mutation issues\n",
    "    out: List[Dict] = []\n",
    "    for ch in chunks_text:\n",
    "        meta_copy = copy.deepcopy(meta)\n",
    "        meta_copy[\"text\"] = ch\n",
    "        shaped = _shape_metadata(meta_copy)\n",
    "        out.append({\n",
    "            \"key\": str(uuid.uuid4()),\n",
    "            \"metadata\": shaped\n",
    "        })\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d583b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple\n",
    "import textract\n",
    "import extract_msg\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_bytes\n",
    "from PyPDF2 import PdfReader\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "s3v = boto3.client(\"s3vectors\", region_name=\"us-east-1\")\n",
    "\n",
    "SUPPORTED_EXTENSIONS = {\".pdf\", \".docx\", \".txt\", \".msg\", \".doc\"}\n",
    "\n",
    "def parse_s3_uri(s3_uri: str) -> Tuple[str, str]:\n",
    "\n",
    "    if not s3_uri.startswith(\"s3://\"):\n",
    "        raise ValueError(f\"Invalid S3 URI: {s3_uri}\")\n",
    "    parts = s3_uri[5:].split(\"/\", 1)\n",
    "    if len(parts) != 2:\n",
    "        raise ValueError(f\"Invalid S3 URI: {s3_uri}\")\n",
    "    return parts[0], parts[1]\n",
    "\n",
    "def download_s3_file(bucket: str, key: str) -> io.BytesIO:\n",
    "    resp = s3.get_object(Bucket=bucket, Key=key)\n",
    "    return io.BytesIO(resp[\"Body\"].read())\n",
    "\n",
    "\n",
    "def extract_from_doc(file_io: io.BytesIO) -> str:\n",
    "    try:\n",
    "        file_io.seek(0)\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".doc\") as tmp:\n",
    "            tmp.write(file_io.read())\n",
    "            tmp_path = tmp.name\n",
    "        text = textract.process(tmp_path).decode(\"utf-8\", errors=\"ignore\").strip()\n",
    "        os.remove(tmp_path)\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùó DOC (textract) error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_with_ocr(file_io: io.BytesIO) -> str:\n",
    "    try:\n",
    "        file_io.seek(0)\n",
    "        images = convert_from_bytes(file_io.read())\n",
    "        out = []\n",
    "        for img in images:\n",
    "            out.append(pytesseract.image_to_string(img))\n",
    "        return \"\\n\".join(out).strip()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùó OCR failed: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_from_pdf(file_io: io.BytesIO) -> str:\n",
    "    try:\n",
    "        file_io.seek(0)\n",
    "        reader = PdfReader(file_io)\n",
    "        texts = []\n",
    "        for p in reader.pages:\n",
    "            t = p.extract_text() or \"\"\n",
    "            if t.strip():\n",
    "                texts.append(t)\n",
    "        return \"\\n\".join(texts).strip()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùó PDF read error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_from_docx(file_io: io.BytesIO) -> str:\n",
    "    try:\n",
    "        from docx import Document\n",
    "        from io import BytesIO\n",
    "        file_io.seek(0)\n",
    "        bio = BytesIO(file_io.read())\n",
    "        doc = Document(bio)\n",
    "        return \"\\n\".join(p.text for p in doc.paragraphs).strip()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùó DOCX error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_from_txt(file_io: io.BytesIO) -> str:\n",
    "    try:\n",
    "        file_io.seek(0)\n",
    "        return file_io.read().decode(\"utf-8\", errors=\"ignore\").strip()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùó TXT read error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_from_msg(file_io: io.BytesIO) -> str:\n",
    "    try:\n",
    "        file_io.seek(0)\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".msg\") as tmp:\n",
    "            tmp.write(file_io.read())\n",
    "            tmp_path = tmp.name\n",
    "        msg = extract_msg.Message(tmp_path)\n",
    "        text = (msg.body or \"\").strip()\n",
    "        os.remove(tmp_path)\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùó MSG read error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_text(file_io: io.BytesIO, ext: str) -> Tuple[str, str]:\n",
    "\n",
    "    ext = ext.lower()\n",
    "    extractors = {\n",
    "        \".pdf\": [extract_from_pdf, extract_with_ocr],\n",
    "        \".docx\": [extract_from_docx, extract_with_ocr],\n",
    "        \".doc\": [extract_from_doc],\n",
    "        \".txt\": [extract_from_txt],\n",
    "        \".msg\": [extract_from_msg],\n",
    "    }\n",
    "    for extractor in extractors.get(ext, []):\n",
    "        file_io.seek(0)\n",
    "        text = extractor(file_io)\n",
    "        if text:\n",
    "            return text, extractor.__name__\n",
    "    return \"\", \"none\"\n",
    "\n",
    "def batched(iterable, n):\n",
    "    batch = []\n",
    "    for x in iterable:\n",
    "        batch.append(x)\n",
    "        if len(batch) == n:\n",
    "            yield batch\n",
    "            batch = []\n",
    "    if batch:\n",
    "        yield batch\n",
    "\n",
    "def upload_chunks_to_s3_vector_index(\n",
    "    chunks: List[Dict],\n",
    "    batch_size: int = 500\n",
    "):\n",
    "    texts = [c[\"metadata\"][\"text\"] for c in chunks]\n",
    "    keys = [c[\"key\"] for c in chunks]\n",
    "\n",
    "    embeddings = get_text_embedding(texts)\n",
    "\n",
    "    if embeddings and isinstance(embeddings[0], float):\n",
    "        embeddings = [embeddings]\n",
    "\n",
    "    vectors = []\n",
    "    for i, c in enumerate(chunks):\n",
    "        emb = embeddings[i] if i < len(embeddings) else None\n",
    "        if emb is None:\n",
    "            continue\n",
    "        vectors.append({\n",
    "            \"key\": keys[i],\n",
    "            \"data\": {\"float32\": emb},\n",
    "            \"metadata\": c[\"metadata\"]\n",
    "        })\n",
    "\n",
    "\n",
    "    responses = []\n",
    "    for batch in batched(vectors, batch_size):\n",
    "        resp = s3v.put_vectors(\n",
    "            vectorBucketName=VECTOR_BUCKET_NAME,\n",
    "            indexName=INDEX_NAME,\n",
    "            vectors=batch\n",
    "        )\n",
    "        responses.append(resp)\n",
    "    return responses\n",
    "\n",
    "def mark_csv_file_processed(csv_path: str, s3_full_path: str) -> bool:\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        if \"s3_full_path\" not in df.columns or \"s3_vectors\" not in df.columns:\n",
    "            print(f\"‚ùó Missing 's3_full_path' or 's3_vectors' in {csv_path}\")\n",
    "            return False\n",
    "\n",
    "        idx = df.index[df[\"s3_full_path\"] == s3_full_path]\n",
    "        if len(idx) == 0:\n",
    "            print(f\"‚ÑπÔ∏è Row not found in CSV for {s3_full_path}\")\n",
    "            return False\n",
    "\n",
    "        df.loc[idx, \"s3_vectors\"] = True\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùó mark_csv_file_processed error: {e}\")\n",
    "        return False\n",
    "\n",
    "def build_items_from_csv(csv_path: str) -> List[Dict]:\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if \"s3_vectors\" not in df.columns or \"s3_full_path\" not in df.columns:\n",
    "        raise ValueError(\"CSV must contain 's3_full_path' and 's3_vectors'\")\n",
    "\n",
    "    df[\"s3_vectors\"] = df[\"s3_vectors\"].astype(str).str.lower()\n",
    "    df = df[df[\"s3_vectors\"] != \"true\"].copy()\n",
    "\n",
    "    items = []\n",
    "    for _, row in df.iterrows():\n",
    "        s3_path = row[\"s3_full_path\"]\n",
    "        try:\n",
    "            b, k = parse_s3_uri(s3_path)\n",
    "        except ValueError as ve:\n",
    "            print(f\"‚ùó Skipping invalid s3 path ({s3_path}): {ve}\")\n",
    "            continue\n",
    "\n",
    "        meta = row.to_dict()\n",
    "        # meta[\"s3_path\"] = s3_path\n",
    "        items.append({\"bucket\": b, \"file_key\": k, \"metadata\": meta})\n",
    "    return items, df\n",
    "\n",
    "def process_documents(\n",
    "    items: List[Dict],\n",
    "    csv_path_to_mark: str\n",
    "):\n",
    "    stats = {\n",
    "        \"processed\": 0, \"failed\": 0,\n",
    "        \"pdf\": 0, \"docx\": 0, \"txt\": 0, \"msg\": 0, \"doc\": 0,\n",
    "        \"ocr\": 0, \"none\": 0, \"unsupported_ext\": 0\n",
    "    }\n",
    "\n",
    "    for idx, item in enumerate(items, 1):\n",
    "        bucket = item[\"bucket\"]\n",
    "        key = item[\"file_key\"]\n",
    "        meta = dict(item.get(\"metadata\", {}))\n",
    "        s3_path = meta.get(\"s3_full_path\")\n",
    "        # s3_path = meta.get(\"s3_path\", f\"s3://{bucket}/{key}\")\n",
    "        ext = os.path.splitext(key)[1].lower()\n",
    "\n",
    "        print(f\"üìÑ [{idx}/{len(items)}] {key}\")\n",
    "\n",
    "        if ext not in SUPPORTED_EXTENSIONS:\n",
    "            print(f\"‚ö†Ô∏è Unsupported extension: {ext} ({s3_path})\")\n",
    "            stats[\"unsupported_ext\"] += 1\n",
    "            stats[\"failed\"] += 1\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            file_io = download_s3_file(bucket, key)\n",
    "            text, method = extract_text(file_io, ext)\n",
    "\n",
    "            if not text:\n",
    "                print(\"‚ö†Ô∏è No text extracted.\")\n",
    "                stats[\"failed\"] += 1\n",
    "                stats[\"none\"] += 1\n",
    "                continue\n",
    "\n",
    "            # Chunk\n",
    "            chunks = chunk_text_and_build_chunks(text, meta)\n",
    "\n",
    "            responses = upload_chunks_to_s3_vector_index(chunks=chunks)\n",
    "\n",
    "            if responses:\n",
    "                if csv_path_to_mark:\n",
    "                    ok = mark_csv_file_processed(csv_path_to_mark, s3_path)\n",
    "                    if ok:\n",
    "                        print(f\"‚úÖ Uploaded {len(chunks)} vectors and marked in CSV\")\n",
    "                    else:\n",
    "                        print(\"‚ÑπÔ∏è Uploaded vectors, but could not mark row in CSV\")\n",
    "            else:\n",
    "                print(\"‚ùó Upload returned no responses\")\n",
    "\n",
    "            stats[\"processed\"] += 1\n",
    "            stats[ext.replace(\".\", \"\")] += 1\n",
    "            if \"ocr\" in method:\n",
    "                stats[\"ocr\"] += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùó Error processing {s3_path}: {e}\")\n",
    "            stats[\"failed\"] += 1\n",
    "            stats[\"none\"] += 1\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\n=== Ingestion Summary ===\")\n",
    "    first = [f\"Processed: {stats['processed']}\", f\"Failed: {stats['failed']}\"]\n",
    "    rest = [f\"{k.upper()}: {v}\" for k, v in stats.items() if k not in {\"processed\", \"failed\"}]\n",
    "    print(\"  \".join(first))\n",
    "    print(\"  \".join(rest))\n",
    "    return stats\n",
    "\n",
    "def run_ingestion(work_csv: str):\n",
    "\n",
    "    if s3v is None or VECTOR_BUCKET_NAME is None or INDEX_NAME is None:\n",
    "        raise ValueError(\"Please provide s3v, VECTOR_BUCKET_NAME, INDEX_NAME\")\n",
    "\n",
    "    items, _df_work = build_items_from_csv(work_csv)\n",
    "\n",
    "    if len(items) == 0:\n",
    "        print(\"Nothing to process. All rows are already marked or CSV is empty.\")\n",
    "        return {\"processed\": 0, \"failed\": 0}\n",
    "\n",
    "    # Go!\n",
    "    return process_documents(\n",
    "        items=items,\n",
    "        csv_path_to_mark=work_csv\n",
    "    )\n",
    "\n",
    "\n",
    "VECTOR_BUCKET_NAME = \"legal-docs-vector-store\"\n",
    "INDEX_NAME = 'token-chunking-new-files'\n",
    "\n",
    "run_ingestion(work_csv=\"gathered_contract_files_valid_new.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
