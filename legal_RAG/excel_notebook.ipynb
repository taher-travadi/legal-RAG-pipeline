{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf4f6f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Downloading Excel: s3://ml-legal-restricted/tabularData/Active Legal Contracts 7-10-2025 1-17-09 PM.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/openpyxl/styles/stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🛑 Reached file limit of 1200 after processing client: Aetna Life Insurance Company\n",
      "✅ CSV saved as 'gathered_contract_files.csv'\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "def download_excel(bucket, key, sheet_name=\"Active Legal Contracts\", column=\"Contract Number\"):\n",
    "    print(f\"📥 Downloading Excel: s3://{bucket}/{key}\")\n",
    "    obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "    excel_data = obj['Body'].read()\n",
    "\n",
    "    df = pd.read_excel(io.BytesIO(excel_data), sheet_name=sheet_name, engine='openpyxl')\n",
    "    if column not in df.columns:\n",
    "        raise ValueError(f\"Column '{column}' not found in sheet '{sheet_name}'\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def list_s3_files_for_contract(bucket, contract_number, prefix_base=\"contract-docs/\"):\n",
    "    prefix = f\"{prefix_base}{contract_number}/\"\n",
    "    files = []\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "\n",
    "    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "        for obj in page.get(\"Contents\", []):\n",
    "            files.append(obj[\"Key\"])\n",
    "    return files\n",
    "\n",
    "def gather_contract_files(bucket, df, limit=1200):\n",
    "    result = []\n",
    "    file_count = 0\n",
    "\n",
    "    # Clean columns\n",
    "    df = df[['Contract Number', 'Account', 'Document Type']].dropna(subset=['Contract Number', 'Account'])\n",
    "\n",
    "    # Convert to string and strip\n",
    "    df['Contract Number'] = df['Contract Number'].astype(str).str.strip()\n",
    "    df['Account'] = df['Account'].astype(str).str.strip()\n",
    "    df['Document Type'] = df['Document Type'].astype(str).str.strip()\n",
    "\n",
    "    grouped = df.groupby('Account')\n",
    "\n",
    "    for client_account, group in grouped:\n",
    "        group_contracts = group['Contract Number'].unique()\n",
    "\n",
    "        temp_result = []\n",
    "\n",
    "        for contract_number in group_contracts:\n",
    "            doc_type = group[group['Contract Number'] == contract_number]['Document Type'].iloc[0]\n",
    "            s3_files = list_s3_files_for_contract(bucket, contract_number)\n",
    "\n",
    "            for key in s3_files:\n",
    "                temp_result.append({\n",
    "                    'contract_number': contract_number,\n",
    "                    'client_account': client_account,\n",
    "                    'S3_full_path': f\"s3://{bucket}/{key}\",\n",
    "                    'file_name': key.split('/')[-1],\n",
    "                    'doc_type': doc_type,\n",
    "                    'S3_vectors': '--',\n",
    "                    'OpenSearch': '--'\n",
    "                })\n",
    "\n",
    "        file_count += len(temp_result)\n",
    "\n",
    "        result.extend(temp_result)\n",
    "\n",
    "        if file_count >= limit:\n",
    "            print(f\"🛑 Reached file limit of {limit} after processing client: {client_account}\")\n",
    "            break\n",
    "\n",
    "    return result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    bucket = \"ml-legal-restricted\"\n",
    "    excel_key = \"tabularData/Active Legal Contracts 7-10-2025 1-17-09 PM.xlsx\"\n",
    "\n",
    "    df = download_excel(bucket, excel_key)\n",
    "    records = gather_contract_files(bucket, df, limit=1200)\n",
    "\n",
    "    output_df = pd.DataFrame(records)\n",
    "    output_df.to_csv(\"gathered_contract_files.csv\", index=False)\n",
    "    print(\"✅ CSV saved as 'gathered_contract_files.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8413a9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Total Files Collected: 1582\n",
      "👤 Total Unique Clients: 120\n",
      "\n",
      "📊 Files Per Client:\n",
      "\n",
      "client_account\n",
      "Aetna Life Insurance Company        666\n",
      "Aerotek                              99\n",
      "1199 SEIU National Benefit Funds     72\n",
      "AArete LLC                           47\n",
      "AAPC                                 45\n",
      "                                   ... \n",
      "Acteva                                1\n",
      "Advanced Systems Group                1\n",
      "Advanced Document Systems             1\n",
      "Adminisoft                            1\n",
      "Advent International Corporation      1\n",
      "Name: file_name, Length: 120, dtype: int64\n",
      "\n",
      "✅ Summary saved to 'files_per_client_summary.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"gathered_contract_files.csv\")\n",
    "\n",
    "total_files = len(df)\n",
    "\n",
    "unique_clients = df['client_account'].nunique()\n",
    "\n",
    "files_per_client = df.groupby('client_account')['file_name'].count().sort_values(ascending=False)\n",
    "\n",
    "print(f\"📁 Total Files Collected: {total_files}\")\n",
    "print(f\"👤 Total Unique Clients: {unique_clients}\")\n",
    "print(\"\\n📊 Files Per Client:\\n\")\n",
    "print(files_per_client)\n",
    "\n",
    "files_per_client.to_csv(\"files_per_client_summary.csv\", header=['file_count'])\n",
    "print(\"\\n✅ Summary saved to 'files_per_client_summary.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cd8c54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidates after anti-join: 12170 rows across 1570 accounts.\n",
      "Selected 56 whole groups -> 558 rows (target 550).\n",
      "Saved: gathered_contract_files_valid_new.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "CSV1 = \"contracts_files.csv\"\n",
    "CSV2 = \"gathered_contract_files_valid.csv\"\n",
    "OUT_CSV = \"gathered_contract_files_valid_new.csv\"\n",
    "TARGET_ROWS = 550\n",
    "RNG_SEED = 42  # make selection reproducible\n",
    "\n",
    "def build_new_valid_files(csv1=CSV1, csv2=CSV2, out_csv=OUT_CSV, target=TARGET_ROWS, seed=RNG_SEED):\n",
    "    # Load\n",
    "    df1 = pd.read_csv(csv1)\n",
    "    df2 = pd.read_csv(csv2)\n",
    "\n",
    "    # Basic checks\n",
    "    if \"s3_full_path\" not in df1.columns:\n",
    "        raise ValueError(f\"'s3_full_path' not found in {csv1}\")\n",
    "    if \"s3_full_path\" not in df2.columns:\n",
    "        raise ValueError(f\"'s3_full_path' not found in {csv2}\")\n",
    "    if \"account_name\" not in df1.columns:\n",
    "        raise ValueError(f\"'account_name' not found in {csv1}\")\n",
    "\n",
    "    # Anti-join: keep rows from df1 whose s3_full_path is NOT present in df2\n",
    "    df2_paths = df2[\"s3_full_path\"].dropna().astype(str).unique()\n",
    "    candidates = (\n",
    "        df1[~df1[\"s3_full_path\"].astype(str).isin(df2_paths)]\n",
    "        .dropna(subset=[\"s3_full_path\"])\n",
    "        .drop_duplicates(subset=[\"s3_full_path\"])\n",
    "        .copy()\n",
    "    )\n",
    "\n",
    "    # Normalize/guard group key\n",
    "    candidates[\"account_name\"] = candidates[\"account_name\"].fillna(\"(Unknown)\")\n",
    "\n",
    "    # Group and shuffle group order\n",
    "    groups = list(candidates.groupby(\"account_name\", as_index=False))\n",
    "    rng = np.random.default_rng(seed)\n",
    "    rng.shuffle(groups)\n",
    "\n",
    "    # Select whole groups until we reach/exceed target\n",
    "    selected_groups = []\n",
    "    running_total = 0\n",
    "    for acct, g in groups:\n",
    "        selected_groups.append(g)\n",
    "        running_total += len(g)\n",
    "        if running_total >= target:\n",
    "            break\n",
    "\n",
    "    # If candidates < target, this will just be all candidates\n",
    "    result = pd.concat(selected_groups, ignore_index=True) if selected_groups else candidates.head(0)\n",
    "    result[\"s3_vectors\"] = \"--\"\n",
    "    result[\"opensearch\"] = \"--\"\n",
    "\n",
    "    # Save\n",
    "    result.to_csv(out_csv, index=False)\n",
    "\n",
    "    # Logging\n",
    "    print(f\"Candidates after anti-join: {len(candidates)} rows across {candidates['account_name'].nunique()} accounts.\")\n",
    "    print(f\"Selected {len(selected_groups)} whole groups -> {len(result)} rows (target {target}).\")\n",
    "    print(f\"Saved: {out_csv}\")\n",
    "\n",
    "\n",
    "build_new_valid_files()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "940412d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#one time job for top15 clients + apply filters\n",
    "\n",
    "\"\"\"\n",
    "Contracts pipeline (final run, no CLI, no dry-run).\n",
    "\n",
    "What it does\n",
    "------------\n",
    "- Load Excel from S3.\n",
    "- Normalize fields:\n",
    "    * Parent Contract -> clean string (no NaN, no trailing \".0\")\n",
    "    * Contract Title / Document Title -> single-line strings\n",
    "    * Normalize SharePoint URL + extract file_name\n",
    "- Core filters:\n",
    "    * sharepoint_document_url non-empty\n",
    "    * status_reason == \"Fully-Executed/Complete\" (case-insensitive)\n",
    "    * account_type == \"Client\" (case-insensitive)\n",
    "- Prioritization:\n",
    "    * Keep ALL rows where solution_line == \"Risk Adjustment\"\n",
    "    * For all other solution lines, keep rows whose account_name matches the top-15 clients\n",
    "      (Horizon must look like Horizon BCBS/NJ; won't match \"Horizon Systems\")\n",
    "- S3 file match:\n",
    "    * Look under contract-docs/{contract_number}/ for exact file basename\n",
    "    * If contract_number is numeric and < 8 digits, also try zero-padded to 8\n",
    "- Outputs:\n",
    "    * Writes `output_csv` with S3 paths + metadata\n",
    "    * Writes `missing_files.csv` with rows where no S3 file match was found\n",
    "- Returns:\n",
    "    * (output_df, missing_df) as pandas DataFrames\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "from typing import Optional, Tuple\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse, parse_qs, urlunparse, unquote\n",
    "\n",
    "DEFAULT_SHEET = \"Active Legal Contracts\"\n",
    "\n",
    "COLUMN_MAP = {\n",
    "    \"Contract Number\": \"contract_number\",\n",
    "    \"Account\": \"account_name\",\n",
    "    \"Document Type\": \"doc_type\",\n",
    "    \"Status Reason\": \"status_reason\",\n",
    "    \"Contract Title\": \"contract_title\",\n",
    "    \"Contract Requester\": \"contract_requester\",\n",
    "    \"Reviewing Attorney\": \"reviewing_attorney\",\n",
    "    \"Created On\": \"created_on\",\n",
    "    \"Document Effective Date\": \"document_effective_date\",\n",
    "    \"Parent Contract\": \"parent_contract\",\n",
    "    \"Solution Line\": \"solution_line\",\n",
    "    \"Account Type\": \"account_type\",\n",
    "    \"Document URL\": \"sharepoint_document_url\",\n",
    "    \"Document Title\": \"document_title\",\n",
    "    \"Related Product\": \"related_product\",\n",
    "}\n",
    "\n",
    "CLIENT_REGEXES = [\n",
    "    (\"Anthem/Elevance\", re.compile(r\"\\b(anthem|elevance)\\b\", re.I)),\n",
    "    (\"Aetna\", re.compile(r\"\\baetna\\b\", re.I)),\n",
    "    (\"Humana\", re.compile(r\"\\bhumana\\b\", re.I)),\n",
    "    (\"United\", re.compile(r\"\\b(united\\s*health( ?care)?|unitedhealth(group)?|uhc)\\b\", re.I)),\n",
    "    (\"Centene\", re.compile(r\"\\bcentene\\b\", re.I)),\n",
    "    (\"HCSC\", re.compile(r\"\\b(hcsc|health\\s*care\\s*service\\s*corporation)\\b\", re.I)),\n",
    "    (\"BCBSTN\", re.compile(r\"\\b(blue\\s*cross\\s*blue\\s*shield\\s*of\\s*tennessee|bcbstn)\\b\", re.I)),\n",
    "    (\"Highmark\", re.compile(r\"\\bhighmark\\b\", re.I)),\n",
    "    (\"UPMC\", re.compile(r\"\\bupmc\\b\", re.I)),\n",
    "    (\"Horizon (NJ BCBS)\", re.compile(r\"\\bhorizon\\s*(blue|bcbs|cross|shield|new\\s*jersey|nj)\\b\", re.I)),\n",
    "    (\"Excellus\", re.compile(r\"\\bexcellus\\b\", re.I)),\n",
    "    (\"Wellmark\", re.compile(r\"\\bwellmark\\b\", re.I)),\n",
    "    (\"BCBS Michigan\", re.compile(r\"\\b(blue\\s*cross\\s*(and\\s*)?blue\\s*shield\\s*of\\s*michigan|bcbsm)\\b\", re.I)),\n",
    "    (\"Kaiser\", re.compile(r\"\\bkaiser(\\s+permanente)?\\b\", re.I)),\n",
    "    (\"BCBSLA\", re.compile(r\"\\b(blue\\s*cross\\s*(and\\s*)?blue\\s*shield\\s*of\\s*louisiana|bcbsla)\\b\", re.I)),\n",
    "]\n",
    "\n",
    "def normalize_url(u: str) -> str:\n",
    "    \"\"\"Normalize URLs: strip, lowercase scheme/host, decode path, drop trailing slash on path.\"\"\"\n",
    "    if not isinstance(u, str):\n",
    "        return \"\"\n",
    "    u = u.strip()\n",
    "    if not u:\n",
    "        return \"\"\n",
    "    try:\n",
    "        p = urlparse(u)\n",
    "        scheme = (p.scheme or \"\").lower()\n",
    "        netloc = (p.netloc or \"\").lower()\n",
    "        path = unquote(p.path or \"\")\n",
    "        if path.endswith(\"/\") and len(path) > 1:\n",
    "            path = path.rstrip(\"/\")\n",
    "        return urlunparse((scheme, netloc, path, p.params, p.query, p.fragment))\n",
    "    except Exception:\n",
    "        return u\n",
    "\n",
    "def filename_from_url(u: str) -> str:\n",
    "    try:\n",
    "        p = urlparse(u)\n",
    "        qs = parse_qs(p.query)\n",
    "        if \"file\" in qs and qs[\"file\"]:\n",
    "            return unquote(qs[\"file\"][0])\n",
    "        path = unquote(p.path or \"\")\n",
    "        return os.path.basename(path)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def normalize_singleline(text) -> str:\n",
    "    \"\"\"Flatten multi-line/paragraph text into a single cleaned line.\"\"\"\n",
    "    if text is None or (isinstance(text, float) and pd.isna(text)):\n",
    "        return \"\"\n",
    "    s = str(text)\n",
    "    s = s.replace(\"\\r\", \" \").replace(\"\\n\", \" \")\n",
    "    s = re.sub(r\"[\\u2028\\u2029]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def map_top_client(account_name: str) -> Tuple[Optional[str], Optional[str]]:\n",
    "    \"\"\"Return (canonical_label, matched_regex_pattern) or (None, None).\"\"\"\n",
    "    if not isinstance(account_name, str) or not account_name.strip():\n",
    "        return (None, None)\n",
    "    for label, rx in CLIENT_REGEXES:\n",
    "        if rx.search(account_name):\n",
    "            return (label, rx.pattern)\n",
    "    return (None, None)\n",
    "\n",
    "def read_excel_from_s3(bucket: str, key: str, sheet_name: str = DEFAULT_SHEET,\n",
    "                       s3_client: Optional[object] = None) -> pd.DataFrame:\n",
    "    \"\"\"Download Excel from S3 and return DataFrame.\"\"\"\n",
    "    s3 = s3_client or boto3.client(\"s3\")\n",
    "    try:\n",
    "        excel_data = s3.get_object(Bucket=bucket, Key=key)[\"Body\"].read()\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to download Excel from s3://{bucket}/{key}: {e}\")\n",
    "    return pd.read_excel(io.BytesIO(excel_data), sheet_name=sheet_name, engine=\"openpyxl\")\n",
    "\n",
    "\n",
    "def base_transform(df_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Select/rename columns, coerce types, normalize titles/URLs, and apply core filters:\n",
    "    - sharepoint_document_url non-empty\n",
    "    - status_reason == Fully-Executed/Complete\n",
    "    - account_type == Client\n",
    "    \"\"\"\n",
    "    missing_cols = [col for col in COLUMN_MAP.keys() if col not in df_raw.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing columns in Excel: {missing_cols}\")\n",
    "\n",
    "    df = df_raw[list(COLUMN_MAP.keys())].dropna(subset=[\"Contract Number\"]).copy()\n",
    "\n",
    "    df[\"Parent Contract\"] = (\n",
    "        df[\"Parent Contract\"]\n",
    "        .apply(lambda x: \"\" if pd.isna(x) else str(x).strip())\n",
    "        .str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "    )\n",
    "\n",
    "    df[\"Contract Number\"] = df[\"Contract Number\"].astype(str).str.strip()\n",
    "\n",
    "    df = df.rename(columns=COLUMN_MAP)\n",
    "\n",
    "    if \"contract_title\" in df.columns:\n",
    "        df[\"contract_title\"] = df[\"contract_title\"].apply(normalize_singleline)\n",
    "    if \"document_title\" in df.columns:\n",
    "        df[\"document_title\"] = df[\"document_title\"].apply(normalize_singleline)\n",
    "\n",
    "    df[\"sharepoint_document_url\"] = df[\"sharepoint_document_url\"].fillna(\"\").astype(str).apply(normalize_url)\n",
    "    df[\"file_name\"] = df[\"sharepoint_document_url\"].apply(filename_from_url)\n",
    "\n",
    "    df = df[df[\"sharepoint_document_url\"].str.strip() != \"\"]\n",
    "\n",
    "    df = df[df[\"status_reason\"].fillna(\"\").astype(str).str.strip().str.casefold()\n",
    "            == \"fully-executed/complete\".casefold()]\n",
    "\n",
    "    df = df[df[\"account_type\"].fillna(\"\").astype(str).str.strip().str.casefold() == \"client\"]\n",
    "\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def apply_prioritization(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Keep all Risk Adjustment; for others, keep only top-15 clients.\"\"\"\n",
    "    mapped = df[\"account_name\"].fillna(\"\").astype(str).apply(map_top_client)\n",
    "    df = df.copy()\n",
    "    df[\"matched_client\"] = mapped.apply(lambda t: t[0])\n",
    "    df[\"matched_pattern\"] = mapped.apply(lambda t: t[1])\n",
    "\n",
    "    ra_mask = df[\"solution_line\"].fillna(\"\").astype(str).str.strip().str.casefold() == \"risk adjustment\"\n",
    "    keep_mask = ra_mask | (~ra_mask & df[\"matched_client\"].notna())\n",
    "    return df[keep_mask].reset_index(drop=True)\n",
    "\n",
    "def find_matching_contract_file(bucket: str,\n",
    "                                contract_number: str,\n",
    "                                target_filename: str,\n",
    "                                prefix_base: str = \"contract-docs/\",\n",
    "                                s3_client: Optional[object] = None) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Search under contract-docs/{contract_number}/ and return the *single* key\n",
    "    that exactly matches the target file name. If not found, return None.\n",
    "    \"\"\"\n",
    "    if not target_filename:\n",
    "        return None\n",
    "\n",
    "    s3 = s3_client or boto3.client(\"s3\")\n",
    "    prefix = f\"{prefix_base}{contract_number}/\"\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "\n",
    "    try:\n",
    "        for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "            for obj in page.get(\"Contents\", []):\n",
    "                key = obj.get(\"Key\", \"\")\n",
    "                if not key or key.endswith(\"/\"):\n",
    "                    continue\n",
    "                if os.path.basename(key) == target_filename:\n",
    "                    return key\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Error listing S3 for prefix {prefix}: {e}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "def process_contracts_with_metadata(bucket: str,\n",
    "                                    excel_key: str,\n",
    "                                    output_csv: str,\n",
    "                                    sheet_name: str = DEFAULT_SHEET,\n",
    "                                    prefix_base: str = \"contract-docs/\",\n",
    "                                    s3_client: Optional[object] = None) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    End-to-end run:\n",
    "      1) Read Excel from S3\n",
    "      2) Transform + core filters\n",
    "      3) Prioritize (Risk Adjustment OR top-15 clients)\n",
    "      4) Match to S3 files\n",
    "      5) Write outputs\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    df_raw = read_excel_from_s3(bucket, excel_key, sheet_name, s3_client=s3_client)\n",
    "    df = base_transform(df_raw)\n",
    "    df = apply_prioritization(df)\n",
    "\n",
    "    output_rows = []\n",
    "    not_found_rows = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        contract_number = str(row.get(\"contract_number\", \"\")).strip()\n",
    "        file_name = str(row.get(\"file_name\", \"\")).strip()\n",
    "\n",
    "        key = find_matching_contract_file(bucket, contract_number, file_name,\n",
    "                                          prefix_base=prefix_base, s3_client=s3_client)\n",
    "\n",
    "        if not key and contract_number.isdigit() and len(contract_number) < 8:\n",
    "            padded = contract_number.zfill(8)\n",
    "            key = find_matching_contract_file(bucket, padded, file_name,\n",
    "                                              prefix_base=prefix_base, s3_client=s3_client)\n",
    "\n",
    "        if key:\n",
    "            full_path = f\"s3://{bucket}/{key}\"\n",
    "            entry = {\"contract_number\": contract_number, \"s3_full_path\": full_path}\n",
    "            for csv_col in COLUMN_MAP.values():\n",
    "                entry[csv_col] = row.get(csv_col, \"\")\n",
    "            entry[\"matched_client\"] = row.get(\"matched_client\", \"\")\n",
    "            entry[\"matched_pattern\"] = row.get(\"matched_pattern\", \"\")\n",
    "            entry[\"file_name\"] = file_name\n",
    "            output_rows.append(entry)\n",
    "        else:\n",
    "            not_found_rows.append(\n",
    "                {\n",
    "                    \"contract_number\": contract_number,\n",
    "                    \"file_name\": file_name,\n",
    "                    \"sharepoint_document_url\": row.get(\"sharepoint_document_url\", \"\"),\n",
    "                    \"account_name\": row.get(\"account_name\", \"\"),\n",
    "                    \"solution_line\": row.get(\"solution_line\", \"\"),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    output_df = pd.DataFrame(output_rows)\n",
    "    missing_df = pd.DataFrame(not_found_rows)\n",
    "\n",
    "\n",
    "    ordered_cols = (\n",
    "        [\"contract_number\", \"s3_full_path\"]\n",
    "        + list(COLUMN_MAP.values())\n",
    "        + [\"matched_client\", \"matched_pattern\", \"file_name\"]\n",
    "    )\n",
    "\n",
    "    seen = set()\n",
    "    ordered_cols = [c for c in ordered_cols if not (c in seen or seen.add(c))]\n",
    "\n",
    "    if not output_df.empty:\n",
    "        if \"parent_contract\" in output_df.columns:\n",
    "            output_df[\"parent_contract\"] = (\n",
    "                output_df[\"parent_contract\"]\n",
    "                .apply(lambda x: \"\" if pd.isna(x) else str(x))\n",
    "                .str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "            )\n",
    "        output_df = output_df.reindex(columns=ordered_cols, fill_value=\"\")\n",
    "        output_df.to_csv(output_csv, index=False)\n",
    "    else:\n",
    "        pd.DataFrame(columns=ordered_cols).to_csv(output_csv, index=False)\n",
    "\n",
    "    missing_df.to_csv(\"missing_files.csv\", index=False)\n",
    "\n",
    "    return output_df, missing_df\n",
    "\n",
    "\n",
    "\n",
    "out_df, missing_df = process_contracts_with_metadata(\n",
    "    bucket=\"ml-legal-restricted\",\n",
    "    excel_key=\"tabularData/Active Legal Contracts 8-1-2025 10-54-06 AM.xlsx\",\n",
    "    output_csv=\"contracts_files_filtered.csv\",\n",
    "    sheet_name=\"Active Legal Contracts\",\n",
    "    prefix_base=\"contract-docs/\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea5340c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Wrote 3505 rows to CSV\n",
      "\n",
      "📊 Counts by matched client label:\n",
      "matched_client\n",
      "Anthem/Elevance      1301\n",
      "United                370\n",
      "Aetna                 357\n",
      "Kaiser                276\n",
      "Centene               242\n",
      "Highmark              210\n",
      "Humana                194\n",
      "None                  114\n",
      "HCSC                   87\n",
      "UPMC                   81\n",
      "Horizon (NJ BCBS)      69\n",
      "BCBS Michigan          58\n",
      "Excellus               44\n",
      "BCBSTN                 42\n",
      "BCBSLA                 32\n",
      "Wellmark               28\n",
      "\n",
      "📊 Counts by solution line (kept):\n",
      "solution_line\n",
      "Population Health Management    2080\n",
      "Payment Accuracy                 701\n",
      "Quality Improvement              208\n",
      "Risk Adjustment                  200\n",
      "Performance Analytics            114\n",
      "Edifecs                           80\n",
      "Payment Integrity                 62\n",
      "Record Import/Clean Up            60\n"
     ]
    }
   ],
   "source": [
    "# import argparse\n",
    "# import os\n",
    "# import re\n",
    "# import pandas as pd\n",
    "# from urllib.parse import urlparse, parse_qs, urlunparse, unquote\n",
    "# from typing import Optional, Tuple\n",
    "\n",
    "# # ---- Column mapping from Excel -> normalized names ----\n",
    "# COLUMN_MAP = {\n",
    "#     \"Contract Number\": \"contract_number\",\n",
    "#     \"Account\": \"account_name\",\n",
    "#     \"Document Type\": \"doc_type\",\n",
    "#     \"Status Reason\": \"status_reason\",\n",
    "#     \"Contract Title\": \"contract_title\",\n",
    "#     \"Contract Requester\": \"contract_requester\",\n",
    "#     \"Reviewing Attorney\": \"reviewing_attorney\",\n",
    "#     \"Created On\": \"created_on\",\n",
    "#     \"Document Effective Date\": \"document_effective_date\",\n",
    "#     \"Parent Contract\": \"parent_contract\",\n",
    "#     \"Solution Line\": \"solution_line\",\n",
    "#     \"Account Type\": \"account_type\",\n",
    "#     \"Document URL\": \"sharepoint_document_url\",\n",
    "#     \"Document Title\": \"document_title\",\n",
    "#     \"Related Product\": \"related_product\",\n",
    "# }\n",
    "\n",
    "# TOP15_CANONICAL = [\n",
    "#     \"Anthem/Elevance\",\n",
    "#     \"Aetna\",\n",
    "#     \"Humana\",\n",
    "#     \"United\",\n",
    "#     \"Centene\",\n",
    "#     \"HCSC\",\n",
    "#     \"BCBSTN\",\n",
    "#     \"Highmark\",\n",
    "#     \"UPMC\",\n",
    "#     \"Horizon (NJ BCBS)\",\n",
    "#     \"Excellus\",\n",
    "#     \"Wellmark\",\n",
    "#     \"BCBS Michigan\",\n",
    "#     \"Kaiser\",\n",
    "#     \"BCBSLA\",\n",
    "# ]\n",
    "\n",
    "# # Carefully tuned regexes for the 15 clients\n",
    "# CLIENT_REGEXES = [\n",
    "#     (\"Anthem/Elevance\", re.compile(r\"\\b(anthem|elevance)\\b\", re.I)),\n",
    "#     (\"Aetna\", re.compile(r\"\\baetna\\b\", re.I)),\n",
    "#     (\"Humana\", re.compile(r\"\\bhumana\\b\", re.I)),\n",
    "#     # Avoid generic \"United\": target UnitedHealthcare and common variants\n",
    "#     (\"United\", re.compile(r\"\\b(united\\s*health( ?care)?|unitedhealth(group)?|uhc)\\b\", re.I)),\n",
    "#     (\"Centene\", re.compile(r\"\\bcentene\\b\", re.I)),\n",
    "#     (\"HCSC\", re.compile(r\"\\b(hcsc|health\\s*care\\s*service\\s*corporation)\\b\", re.I)),\n",
    "#     (\"BCBSTN\", re.compile(r\"\\b(blue\\s*cross\\s*blue\\s*shield\\s*of\\s*tennessee|bcbstn)\\b\", re.I)),\n",
    "#     (\"Highmark\", re.compile(r\"\\bhighmark\\b\", re.I)),\n",
    "#     (\"UPMC\", re.compile(r\"\\bupmc\\b\", re.I)),\n",
    "#     # Horizon must look like Horizon BCBS/NJ — won't match \"Horizon Systems\"\n",
    "#     (\"Horizon (NJ BCBS)\", re.compile(r\"\\bhorizon\\s*(blue|bcbs|cross|shield|new\\s*jersey|nj)\\b\", re.I)),\n",
    "#     (\"Excellus\", re.compile(r\"\\bexcellus\\b\", re.I)),\n",
    "#     (\"Wellmark\", re.compile(r\"\\bwellmark\\b\", re.I)),\n",
    "#     (\"BCBS Michigan\", re.compile(r\"\\b(blue\\s*cross\\s*(and\\s*)?blue\\s*shield\\s*of\\s*michigan|bcbsm)\\b\", re.I)),\n",
    "#     (\"Kaiser\", re.compile(r\"\\bkaiser(\\s+permanente)?\\b\", re.I)),\n",
    "#     (\"BCBSLA\", re.compile(r\"\\b(blue\\s*cross\\s*(and\\s*)?blue\\s*shield\\s*of\\s*louisiana|bcbsla)\\b\", re.I)),\n",
    "# ]\n",
    "\n",
    "# def normalize_url(u: str) -> str:\n",
    "#     if not isinstance(u, str):\n",
    "#         return \"\"\n",
    "#     u = u.strip()\n",
    "#     if not u:\n",
    "#         return \"\"\n",
    "#     try:\n",
    "#         p = urlparse(u)\n",
    "#         scheme = (p.scheme or \"\").lower()\n",
    "#         netloc = (p.netloc or \"\").lower()\n",
    "#         path = unquote(p.path or \"\")\n",
    "#         if path.endswith(\"/\") and len(path) > 1:\n",
    "#             path = path.rstrip(\"/\")\n",
    "#         return urlunparse((scheme, netloc, path, p.params, p.query, p.fragment))\n",
    "#     except Exception:\n",
    "#         return u\n",
    "\n",
    "# def filename_from_url(u: str) -> str:\n",
    "#     try:\n",
    "#         p = urlparse(u)\n",
    "#         qs = parse_qs(p.query)\n",
    "#         if \"file\" in qs and qs[\"file\"]:\n",
    "#             return unquote(qs[\"file\"][0])\n",
    "#         path = unquote(p.path or \"\")\n",
    "#         return os.path.basename(path)\n",
    "#     except Exception:\n",
    "#         return \"\"\n",
    "\n",
    "# def map_top_client(account_name: str) -> Tuple[Optional[str], Optional[str]]:\n",
    "#     \"\"\"Return (canonical_label, matched_regex_pattern) or (None, None).\"\"\"\n",
    "#     if not isinstance(account_name, str) or not account_name.strip():\n",
    "#         return (None, None)\n",
    "#     for label, rx in CLIENT_REGEXES:\n",
    "#         if rx.search(account_name):\n",
    "#             return (label, rx.pattern)\n",
    "#     return (None, None)\n",
    "\n",
    "# def is_top_client(account_name: str) -> bool:\n",
    "#     label, _ = map_top_client(account_name)\n",
    "#     return label is not None\n",
    "\n",
    "# def load_and_filter_excel(path: str, sheet_name: str) -> pd.DataFrame:\n",
    "#     # Read Excel\n",
    "#     df = pd.read_excel(path, sheet_name=sheet_name, engine=\"openpyxl\")\n",
    "\n",
    "#     # Validate required columns\n",
    "#     missing = [c for c in COLUMN_MAP.keys() if c not in df.columns]\n",
    "#     if missing:\n",
    "#         raise ValueError(f\"Missing columns in Excel: {missing}\")\n",
    "\n",
    "#     # Select & coerce\n",
    "#     df = df[list(COLUMN_MAP.keys())].dropna(subset=[\"Contract Number\"]).copy()\n",
    "\n",
    "#     # Parent Contract -> clean string (no NaN, trim, drop trailing \".0\")\n",
    "#     df[\"Parent Contract\"] = (\n",
    "#         df[\"Parent Contract\"]\n",
    "#         .apply(lambda x: \"\" if pd.isna(x) else str(x).strip())\n",
    "#         .str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "#     )\n",
    "\n",
    "#     # Contract Number as string\n",
    "#     df[\"Contract Number\"] = df[\"Contract Number\"].astype(str).str.strip()\n",
    "\n",
    "#     # Rename to normalized names\n",
    "#     df = df.rename(columns=COLUMN_MAP)\n",
    "\n",
    "#     # Normalize/derive URL + filename\n",
    "#     df[\"sharepoint_document_url\"] = df[\"sharepoint_document_url\"].fillna(\"\").astype(str).apply(normalize_url)\n",
    "#     df[\"file_name\"] = df[\"sharepoint_document_url\"].apply(filename_from_url)\n",
    "\n",
    "#     # ---- Core filters (requested) ----\n",
    "#     # 1) non-empty URL\n",
    "#     df = df[df[\"sharepoint_document_url\"].str.strip() != \"\"]\n",
    "#     # 2) Status Reason exactly Fully-Executed/Complete (case-insensitive, trimmed)\n",
    "#     df = df[df[\"status_reason\"].fillna(\"\").astype(str).str.strip().str.casefold()\n",
    "#             == \"fully-executed/complete\".casefold()]\n",
    "#     # 3) Account Type == Client\n",
    "#     df = df[df[\"account_type\"].fillna(\"\").astype(str).str.strip().str.casefold() == \"client\"]\n",
    "\n",
    "#     return df\n",
    "\n",
    "# def prioritize_rows(df: pd.DataFrame) -> pd.DataFrame:\n",
    "#     # Add matched client info\n",
    "#     mapped = df[\"account_name\"].fillna(\"\").astype(str).apply(map_top_client)\n",
    "#     df = df.copy()\n",
    "#     df[\"matched_client\"] = mapped.apply(lambda t: t[0])\n",
    "#     df[\"matched_pattern\"] = mapped.apply(lambda t: t[1])\n",
    "\n",
    "#     # Always keep Solution Line == \"Risk Adjustment\"\n",
    "#     ra_mask = df[\"solution_line\"].fillna(\"\").astype(str).str.strip().str.casefold() == \"risk adjustment\"\n",
    "\n",
    "#     # For other solution lines, keep only top-15 clients\n",
    "#     keep_mask = ra_mask | (~ra_mask & df[\"matched_client\"].notna())\n",
    "\n",
    "#     return df[keep_mask].reset_index(drop=True)\n",
    "\n",
    "# def process_contracts_with_metadata():\n",
    "#     EXCEL_KEY = \"Active Legal Contracts 8-1-2025 10-54-06 AM.xlsx\"\n",
    "#     sheet_name=\"Active Legal Contracts\"\n",
    "\n",
    "#     df = load_and_filter_excel(EXCEL_KEY, sheet_name)\n",
    "#     df_out = prioritize_rows(df)\n",
    "\n",
    "#     # Choose a helpful column order (only include those that exist)\n",
    "#     desired_cols = [\n",
    "#         \"contract_number\", \"account_name\", \"matched_client\", \"solution_line\",\n",
    "#         \"status_reason\", \"account_type\",\n",
    "#         \"contract_title\", \"document_title\", \"doc_type\", \"related_product\",\n",
    "#         \"created_on\", \"document_effective_date\", \"parent_contract\",\n",
    "#         \"sharepoint_document_url\", \"file_name\",\n",
    "#     ]\n",
    "#     cols = [c for c in desired_cols if c in df_out.columns]\n",
    "#     if cols:\n",
    "#         df_out = df_out[cols]\n",
    "\n",
    "#     df_out.to_csv('contracts_files_dry_run.csv', index=False)\n",
    "\n",
    "#     print(f\"\\n✅ Wrote {len(df_out)} rows to CSV\")\n",
    "#     print(\"\\n📊 Counts by matched client label:\")\n",
    "#     print(df_out[\"matched_client\"].value_counts(dropna=False).to_string())\n",
    "#     print(\"\\n📊 Counts by solution line (kept):\")\n",
    "#     print(df_out[\"solution_line\"].value_counts(dropna=False).to_string())\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     process_contracts_with_metadata()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
