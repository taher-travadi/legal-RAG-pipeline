{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7711576f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# !pip install --upgrade boto3==1.39.8\n",
    "import io\n",
    "import os\n",
    "import random\n",
    "import nltk\n",
    "import boto3\n",
    "import requests\n",
    "import json\n",
    "from io import BytesIO\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "\n",
    "S3_BUCKET = \"ml-legal-restricted\"\n",
    "EXCEL_PATH = \"Active Legal Contracts 8-1-2025 10-54-06 AM.xlsx\"\n",
    "VECTOR_BUCKET_NAME = \"legal-docs-vector-store\"\n",
    "SOURCE_INDEX = 'token-chunking-valid'\n",
    "TARGET_INDEX = 'token-chunking-valid-modified'\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "s3 = boto3.client('s3')\n",
    "s3v = boto3.client(\"s3vectors\", region_name=\"us-east-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fa81aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique accounts written to 'files_per_client_summary.csv'\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "input_file = 'merged_files_data.csv'\n",
    "output_file = 'files_per_client_summary.csv'\n",
    "\n",
    "unique_accounts = {}\n",
    "\n",
    "with open(input_file, mode='r', newline='', encoding='utf-8') as infile:\n",
    "    reader = csv.DictReader(infile)\n",
    "    \n",
    "    for row in reader:\n",
    "        account_name = row.get('account_name')\n",
    "        account_type = row.get('account_type')\n",
    "        \n",
    "        if account_name and account_type:\n",
    "            if account_name not in unique_accounts:\n",
    "                unique_accounts[account_name] = account_type\n",
    "\n",
    "with open(output_file, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    writer.writerow(['account_name', 'account_type'])\n",
    "    \n",
    "    for name, acc_type in unique_accounts.items():\n",
    "        writer.writerow([name, acc_type])\n",
    "\n",
    "print(f\"Unique accounts written to '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be0a704",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel non-empty rows: 14644\n",
      "Unique Excel URLs (normalized): 14638\n",
      "Unique CSV URLs (normalized): 539\n",
      "Matches (normalized URL equality): 538\n",
      "File name == URL last segment (exact): 506\n",
      "File name == URL last segment (case-insensitive): 0\n",
      "Total valid matches: 506\n",
      "Cleaned CSV saved to: gathered_contract_files_enriched_new.csv\n",
      "Valid entries saved to: gathered_contract_files_valid.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from urllib.parse import urlparse, urlunparse, unquote\n",
    "import posixpath\n",
    "\n",
    "EXCEL_KEY = \"Active Legal Contracts 8-1-2025 10-54-06 AM.xlsx\"\n",
    "SHEET_NAME = \"Active Legal Contracts\"\n",
    "CSV_FILE = \"gathered_contract_files_enriched.csv\"\n",
    "CSV_VALID = \"gathered_contract_files_valid.csv\"\n",
    "\n",
    "def normalize_url(u: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize URLs so that small formatting differences don't break matches.\n",
    "    - Strip whitespace\n",
    "    - Lowercase scheme/host\n",
    "    - Decode % encodings on the path (so %20 -> space)\n",
    "    - Remove trailing slashes on the path\n",
    "    \"\"\"\n",
    "    if not isinstance(u, str):\n",
    "        return \"\"\n",
    "    u = u.strip()\n",
    "    if not u:\n",
    "        return \"\"\n",
    "    try:\n",
    "        p = urlparse(u)\n",
    "        scheme = (p.scheme or \"\").lower()\n",
    "        netloc = (p.netloc or \"\").lower()\n",
    "        path = unquote(p.path or \"\")\n",
    "        if path.endswith(\"/\") and len(path) > 1:\n",
    "            path = path.rstrip(\"/\")\n",
    "        return urlunparse((scheme, netloc, path, p.params, p.query, p.fragment))\n",
    "    except Exception:\n",
    "        return u\n",
    "\n",
    "df_xl = pd.read_excel(EXCEL_KEY, sheet_name=SHEET_NAME, dtype=str)\n",
    "mask = df_xl['Document URL'].notna() & (df_xl['Document URL'].str.strip() != \"\")\n",
    "df_xl = df_xl.loc[mask].copy()\n",
    "\n",
    "df_csv = pd.read_csv(CSV_FILE, dtype=str)\n",
    "if 'document_url' not in df_csv.columns:\n",
    "    raise KeyError(\"CSV is missing 'document_url' column\")\n",
    "\n",
    "df_csv['document_url'] = df_csv['document_url'].fillna('').astype(str).apply(normalize_url)\n",
    "\n",
    "df_xl['__norm_url'] = df_xl['Document URL'].apply(normalize_url)\n",
    "excel_urls = set(df_xl['__norm_url'])\n",
    "csv_urls = set(df_csv['document_url'])\n",
    "matched_urls = excel_urls & csv_urls\n",
    "\n",
    "print(f\"Excel non-empty rows: {len(df_xl)}\")\n",
    "print(f\"Unique Excel URLs (normalized): {len(excel_urls)}\")\n",
    "print(f\"Unique CSV URLs (normalized): {len(csv_urls)}\")\n",
    "print(f\"Matches (normalized URL equality): {len(matched_urls)}\")\n",
    "\n",
    "def last_segment(u: str) -> str:\n",
    "    if not isinstance(u, str) or not u.strip():\n",
    "        return \"\"\n",
    "    try:\n",
    "        p = urlparse(u)\n",
    "        seg = posixpath.basename(unquote(p.path or \"\")) or \"\"\n",
    "        return seg.strip()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def normalize_filename(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\", \"\"\n",
    "    s = s.strip()\n",
    "    if s.startswith(\",\"):\n",
    "        s = s[1:].strip()\n",
    "    return s, s.casefold()\n",
    "\n",
    "if 'file_name' not in df_csv.columns:\n",
    "    raise KeyError(\"CSV is missing 'file_name' column\")\n",
    "\n",
    "df_csv['__url_file'] = df_csv['document_url'].apply(last_segment)\n",
    "\n",
    "exact_file, ci_file = zip(*df_csv['file_name'].apply(normalize_filename))\n",
    "df_csv['__file_exact'] = list(exact_file)\n",
    "df_csv['__file_ci'] = list(ci_file)\n",
    "\n",
    "exact_url_file, ci_url_file = zip(*df_csv['__url_file'].apply(normalize_filename))\n",
    "df_csv['__url_exact'] = list(exact_url_file)\n",
    "df_csv['__url_ci'] = list(ci_url_file)\n",
    "\n",
    "mask_exact = (df_csv['__file_exact'] != \"\") & (df_csv['__file_exact'] == df_csv['__url_exact'])\n",
    "mask_ci = (df_csv['__file_ci'] != \"\") & (df_csv['__file_ci'] == df_csv['__url_ci'])\n",
    "\n",
    "mask_valid = mask_exact | (~mask_exact & mask_ci)\n",
    "\n",
    "print(f\"File name == URL last segment (exact): {mask_exact.sum()}\")\n",
    "print(f\"File name == URL last segment (case-insensitive): {(mask_ci & ~mask_exact).sum()}\")\n",
    "print(f\"Total valid matches: {mask_valid.sum()}\")\n",
    "\n",
    "valid_entries = df_csv.loc[mask_valid].drop(\n",
    "    columns=['__url_file','__file_exact','__file_ci','__url_exact','__url_ci'],\n",
    "    errors='ignore'\n",
    ")\n",
    "valid_entries.to_csv(CSV_VALID, index=False)\n",
    "\n",
    "print(f\"Valid entries saved to: {CSV_VALID}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ceba2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks vector & stored: 12591\n"
     ]
    }
   ],
   "source": [
    "INDEX_NAME = 'token-chunking-vectors-poc' #'token-chunking-new-files' #'token-chunking-valid'\n",
    "\n",
    "paginator = s3v.get_paginator('list_vectors')\n",
    "\n",
    "total_vectors_count = 0\n",
    "\n",
    "page_iterator = paginator.paginate(\n",
    "    vectorBucketName=VECTOR_BUCKET_NAME,\n",
    "    indexName=INDEX_NAME,\n",
    "    returnData=True,\n",
    "    returnMetadata=True,\n",
    "    PaginationConfig={\n",
    "        'PageSize': 1000  \n",
    "    }\n",
    ")\n",
    "\n",
    "for page in page_iterator:\n",
    "    vectors = page.get('vectors', [])\n",
    "    total_vectors_count += len(vectors)\n",
    "\n",
    "print(f\"Total chunks vector & stored: {total_vectors_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d400da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_additional_metadata(s3_full_path, merged_df):\n",
    "    try:\n",
    "        row = merged_df[merged_df['s3_full_path'] == s3_full_path]\n",
    "        if row.empty:\n",
    "            print(f\"No metadata found for s3_full_path: {s3_full_path}\")\n",
    "            return {}\n",
    "\n",
    "        additional_metadata_keys = [\n",
    "            'parent_contract', 'status_reason', 'solution_line', 'contract_title', 'contract_requester',\n",
    "            'reviewing_attorney', 'created_on', 'document_effective_date', 'account_type', 'related_product'\n",
    "        ]\n",
    "\n",
    "        additional_metadata = row[additional_metadata_keys].iloc[0].to_dict()\n",
    "\n",
    "        for k, v in additional_metadata.items():\n",
    "            if pd.isna(v):\n",
    "                additional_metadata[k] = None\n",
    "            elif hasattr(v, 'strftime'):\n",
    "                additional_metadata[k] = v.strftime('%Y-%m-%d')\n",
    "            else:\n",
    "                additional_metadata[k] = str(v)\n",
    "\n",
    "        return {k: v for k, v in additional_metadata.items() if v is not None}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in get_additional_metadata for {s3_full_path}: {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "def list_vectors_batch(index_name, bucket_name, max_results=500, next_token=None):\n",
    "    try:\n",
    "        params = {\n",
    "            'indexName': index_name,\n",
    "            'vectorBucketName': bucket_name,\n",
    "            'maxResults': max_results,\n",
    "            'returnData': True,\n",
    "            'returnMetadata': True\n",
    "        }\n",
    "        if next_token:\n",
    "            params['nextToken'] = next_token\n",
    "\n",
    "        response = s3v.list_vectors(**params)\n",
    "        return response.get('vectors', []), response.get('nextToken')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in list_vectors_batch: {e}\")\n",
    "        return [], None\n",
    "\n",
    "\n",
    "def insert_vectors_batch(index_name, bucket_name, vectors):\n",
    "    try:\n",
    "        entries = [{\n",
    "            'key': v['key'],\n",
    "            'data': v['data'],\n",
    "            'metadata': v['metadata']\n",
    "        } for v in vectors]\n",
    "\n",
    "        response = s3v.put_vectors(\n",
    "            indexName=index_name,\n",
    "            vectorBucketName=bucket_name,\n",
    "            vectors=entries\n",
    "        )\n",
    "        return response\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in insert_vectors_batch: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def migrate_vectors_with_metadata_update(source_index, target_index, bucket_name, df, batch_size=500):\n",
    "    next_token = None\n",
    "\n",
    "    while True:\n",
    "        vectors, next_token = list_vectors_batch(source_index, bucket_name, batch_size, next_token)\n",
    "        if not vectors:\n",
    "            print(\"No vectors found in this batch. Ending migration.\")\n",
    "            break\n",
    "\n",
    "        for vec in vectors:\n",
    "            try:\n",
    "                if 'metadata' not in vec or vec['metadata'] is None:\n",
    "                    vec['metadata'] = {}\n",
    "\n",
    "                s3_path = vec['metadata'].get('s3_path', '').strip()\n",
    "                \n",
    "                if s3_path:\n",
    "                    dynamic_metadata = get_additional_metadata(s3_path, df)\n",
    "                    vec['metadata'].update(dynamic_metadata)\n",
    "\n",
    "                    for key in ['file_name', 'contract_number', 'opensearch', 's3_vectors']:\n",
    "                        if key in vec['metadata']:\n",
    "                            del vec['metadata'][key]\n",
    "                    \n",
    "                    created_on = dynamic_metadata.get('created_on', 'None')\n",
    "                    document_effective_date = dynamic_metadata.get('document_effective_date', 'None')\n",
    "                    contract_requester = dynamic_metadata.get('contract_requester', 'None')\n",
    "                    reviewing_attorney = dynamic_metadata.get('reviewing_attorney', 'None')\n",
    "                    account_name = vec['metadata'].get('client_account', 'None')\n",
    "                    parent_contract = dynamic_metadata.get('parent_contract', 'None')\n",
    "                    account_type = vec['metadata'].get('account_type', 'None')\n",
    "                    related_product = vec['metadata'].get('related_product', 'None')\n",
    "                    #add document title\n",
    "                    #check for missings\n",
    "                    # document_title = dynamic_metadata.get('document_title', 'None')\n",
    "                    # vec['metadata']['dates'] = [date for date in [created_on, document_effective_date, document_title]]\n",
    "\n",
    "\n",
    "                    vec['metadata']['dates'] = [date for date in [created_on, document_effective_date]]\n",
    "                    vec['metadata']['attorneys'] = [attorney for attorney in [contract_requester, reviewing_attorney]]\n",
    "                    vec['metadata']['account_details'] = [account for account in [account_name, parent_contract, account_type, related_product]]\n",
    "\n",
    "                    old_keys = ['created_on', 'document_effective_date', 'contract_requester', 'reviewing_attorney','client_account', 'parent_contract', 'account_type', 'related_product']\n",
    "                    for old_key in old_keys:\n",
    "                        if old_key in vec['metadata']:\n",
    "                            del vec['metadata'][old_key]\n",
    "\n",
    "                else:\n",
    "                    print(f\"No s3_path found in vector metadata: {vec.get('metadata')}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing vector {vec.get('vectorKey')}: {e}\")\n",
    "\n",
    "        response = insert_vectors_batch(target_index, bucket_name, vectors)\n",
    "\n",
    "        if response is not None:\n",
    "            print(f\"Inserted batch of {len(vectors)} vectors to {target_index}\")\n",
    "        else:\n",
    "            print(\"Failed to insert batch to target index.\")\n",
    "\n",
    "        if not next_token:\n",
    "            print(\"Completed migrating all vectors.\")\n",
    "            break\n",
    "\n",
    "\n",
    "df = pd.read_csv('gathered_contract_files_enriched.csv', dtype={\"parent_contract\": str})\n",
    "migrate_vectors_with_metadata_update(SOURCE_INDEX, TARGET_INDEX, VECTOR_BUCKET_NAME, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f90d4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied batch of 500 vectors, total copied: 500\n",
      "Copied batch of 500 vectors, total copied: 1000\n",
      "Copied batch of 500 vectors, total copied: 1500\n",
      "Copied batch of 500 vectors, total copied: 2000\n",
      "Copied batch of 500 vectors, total copied: 2500\n",
      "Copied batch of 500 vectors, total copied: 3000\n",
      "Copied batch of 500 vectors, total copied: 3500\n",
      "Copied batch of 500 vectors, total copied: 4000\n",
      "Copied batch of 500 vectors, total copied: 4500\n",
      "Copied batch of 500 vectors, total copied: 5000\n",
      "Copied batch of 500 vectors, total copied: 5500\n",
      "Copied batch of 500 vectors, total copied: 6000\n",
      "Copied batch of 500 vectors, total copied: 6500\n",
      "Copied batch of 500 vectors, total copied: 7000\n",
      "Copied batch of 500 vectors, total copied: 7500\n",
      "Copied batch of 500 vectors, total copied: 8000\n",
      "Copied batch of 500 vectors, total copied: 8500\n",
      "Copied batch of 500 vectors, total copied: 9000\n",
      "Copied batch of 500 vectors, total copied: 9500\n",
      "Copied batch of 500 vectors, total copied: 10000\n",
      "Copied batch of 500 vectors, total copied: 10500\n",
      "Copied batch of 500 vectors, total copied: 11000\n",
      "Copied batch of 500 vectors, total copied: 11500\n",
      "Copied batch of 500 vectors, total copied: 12000\n",
      "Copied batch of 500 vectors, total copied: 12500\n",
      "Copied batch of 91 vectors, total copied: 12591\n",
      "Completed copying all vectors, total count: 12591\n"
     ]
    }
   ],
   "source": [
    "#copy entire chunk (vectors + metadata) to another vector index\n",
    "\n",
    "def copy_s3_vectors_in_batches(\n",
    "    source_index_name,\n",
    "    source_bucket_name,\n",
    "    dest_index_name,\n",
    "    dest_bucket_name,\n",
    "    batch_size=500\n",
    "):\n",
    "\n",
    "    next_token = None\n",
    "    total_copied = 0\n",
    "\n",
    "    while True:\n",
    "        list_params = {\n",
    "            'indexName': source_index_name,\n",
    "            'vectorBucketName': source_bucket_name,\n",
    "            'maxResults': batch_size,\n",
    "            'returnData': True,\n",
    "            'returnMetadata': True,\n",
    "        }\n",
    "        if next_token:\n",
    "            list_params['nextToken'] = next_token\n",
    "\n",
    "        try:\n",
    "            response = s3v.list_vectors(**list_params)\n",
    "        except Exception as e:\n",
    "            print(f\"Error listing vectors: {e}\")\n",
    "            break\n",
    "\n",
    "        vectors = response.get('vectors', [])\n",
    "        if not vectors:\n",
    "            print(\"No more vectors to copy.\")\n",
    "            break\n",
    "\n",
    "        batch = [{\n",
    "            'key': v['key'],\n",
    "            'data': v['data'],\n",
    "            'metadata': v['metadata']\n",
    "        } for v in vectors]\n",
    "\n",
    "        try:\n",
    "            s3v.put_vectors(\n",
    "                # indexName=dest_index_name,\n",
    "                indexArn = 'arn:aws:s3vectors:us-east-1:254281203237:bucket/legal-docs-vectors/index/token-chunking-vectors-poc',\n",
    "                # vectorBucketName='legal-docs-vectors',\n",
    "                vectors=batch\n",
    "            )\n",
    "            total_copied += len(batch)\n",
    "            print(f\"Copied batch of {len(batch)} vectors, total copied: {total_copied}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error inserting vectors: {e}\")\n",
    "            break\n",
    "\n",
    "        next_token = response.get('nextToken')\n",
    "        if not next_token:\n",
    "            print(f\"Completed copying all vectors, total count: {total_copied}\")\n",
    "            break\n",
    "\n",
    "\n",
    "copy_s3_vectors_in_batches(\n",
    "    source_index_name='token-chunking-vectors-poc',\n",
    "    source_bucket_name=VECTOR_BUCKET_NAME,\n",
    "    dest_index_name=\"token-chunking-vectors-poc\",\n",
    "    dest_bucket_name=VECTOR_BUCKET_NAME,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a0bb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge two csv's\n",
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv('gathered_contract_files_valid_modified.csv')\n",
    "df2 = pd.read_csv('gathered_contract_files_valid_new.csv')\n",
    "\n",
    "df2 = df2[df1.columns]\n",
    "\n",
    "merged_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "merged_df.to_csv('merged_files_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0b5eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#verify metadata by running the flow\n",
    " \n",
    "import numpy as np\n",
    "EMBEDINNGS_URL = \"https://zgggzg2iqg.execute-api.us-east-1.amazonaws.com/dev/get_embeddings\"\n",
    "API_KEY = \"2jIpWCyNRg3Y8lkbmWG0tkyXwYlJn5QaZ1F3yKf7\"\n",
    "\n",
    "def _extract_embeddings_obj(obj):\n",
    "\n",
    "    if isinstance(obj, dict) and \"embeddings\" in obj:\n",
    "        return obj[\"embeddings\"]\n",
    "\n",
    "    # Case B: wrapper with body string\n",
    "    if isinstance(obj, dict) and \"body\" in obj:\n",
    "        try:\n",
    "            body = obj[\"body\"]\n",
    "            if isinstance(body, str):\n",
    "                inner = json.loads(body)\n",
    "            else:\n",
    "                inner = body\n",
    "            if isinstance(inner, dict) and \"embeddings\" in inner:\n",
    "                return inner[\"embeddings\"]\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    raise KeyError(\"No 'embeddings' found in response object\")\n",
    "\n",
    "def get_text_embedding(texts, model='e5_mistral_embed_384', timeout=8):\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    if not isinstance(texts, list) or not texts:\n",
    "        raise ValueError(\"Input 'texts' must be a non-empty list of strings.\")\n",
    "\n",
    "    headers = {\n",
    "        \"x-api-key\": API_KEY,\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    out = []\n",
    "    for text in texts:\n",
    "        if not isinstance(text, str):\n",
    "            raise ValueError(\"Each item in 'texts' must be a string.\")\n",
    "\n",
    "        payload = {\"model_name\": model, \"texts\": [text]}\n",
    "\n",
    "        try:\n",
    "            resp = requests.post(EMBEDINNGS_URL, json=payload, headers=headers, timeout=timeout)\n",
    "            resp.raise_for_status()\n",
    "\n",
    "            # Try both shapes\n",
    "            obj = resp.json()\n",
    "            embeddings = _extract_embeddings_obj(obj)\n",
    "\n",
    "            if (not isinstance(embeddings, list)) or len(embeddings) != 1 or (not isinstance(embeddings[0], list)):\n",
    "                raise KeyError(\"Response did not contain a single embedding vector\")\n",
    "\n",
    "            # Convert to float32 for consistency / memory\n",
    "            vec = np.array(embeddings[0], dtype=np.float32).tolist()\n",
    "            out.append(vec)\n",
    "\n",
    "        except Exception as e:\n",
    "            # Print useful diagnostics once\n",
    "            print(f\"[ERROR] Failed to get embedding for '{text}': {e}\")\n",
    "            try:\n",
    "                print(f\"[DEBUG] HTTP {resp.status_code} body: {resp.text[:500]}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "            out.append(None)\n",
    "\n",
    "    return out[0] if len(out) == 1 else out\n",
    "\n",
    "print(get_text_embedding(['Taher Hellot']))\n",
    "\n",
    "\n",
    "\n",
    "def query_s3_vector_store(query_text, client_account_filter, INDEX_NAME, top_k = 5):\n",
    "    print(f\"\\n--- Processing Query: '{query_text}' ---\")\n",
    "\n",
    "    query_embedding = get_text_embedding(query_text)\n",
    "    filter_expression = None\n",
    "    if client_account_filter is not None:\n",
    "        filter_expression = {\n",
    "            \"client_account\": {\n",
    "                \"$eq\": client_account_filter\n",
    "            }\n",
    "        }\n",
    "\n",
    "    try:\n",
    "        response = s3v.query_vectors(\n",
    "            vectorBucketName=VECTOR_BUCKET_NAME,\n",
    "            indexName=INDEX_NAME,\n",
    "            topK=top_k,\n",
    "            queryVector={\n",
    "                'float32': query_embedding\n",
    "            },\n",
    "            returnMetadata=True,\n",
    "            returnDistance=True,\n",
    "            filter=filter_expression\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying S3 Vector Store: {e}\")\n",
    "        return None\n",
    "\n",
    "user_questions = [\n",
    "    \"What obligations does Cotiviti have under Schedule C for Prepay FWAV Services?\", #66179\n",
    "    \"Under Schedule C, what services is Cotiviti required to provide?\", #66179\n",
    "    \"In the Prepay FWAV Services section, what are Cotiviti's main deliverables?\", #66179\n",
    "    \"What restrictions are placed on disclosing confidential information?\", #67566\n",
    "    \"What is Amendment #4 to the Verisk Health License Agreement about?\", #53985\n",
    "    \"What is the purpose of Amendment #4 as stated in the document?\", #53985\n",
    "]\n",
    "\n",
    "INDEX_NAME = \"token-chunking-vectors-poc\"\n",
    "client = None #'UST Global'\n",
    "\n",
    "for question in user_questions:\n",
    "    query_results = query_s3_vector_store(question, client, INDEX_NAME, top_k=5)\n",
    "\n",
    "    if query_results and 'vectors' in query_results:\n",
    "        print(f\"Retrieved {len(query_results['vectors'])}\")\n",
    "        for i, chunk_data in enumerate(query_results['vectors']):\n",
    "            print(f\"    Metadata {i+1}: {chunk_data['metadata']}\")\n",
    "            # print(f\"    Distance: {chunk_data.get('distance', 'N/A')}\")\n",
    "    else:\n",
    "        print(f\"No results or an error occurred for query: '{question}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca23aa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#time pass billing for SageMaker\n",
    "\n",
    "import boto3\n",
    "\n",
    "REGION = \"us-east-1\"  # Change if needed\n",
    "sm = boto3.client(\"sagemaker\", region_name=REGION)\n",
    "\n",
    "grand_total_gb = 0\n",
    "domain_totals = []\n",
    "\n",
    "# 1. List all domains\n",
    "domains = sm.list_domains().get(\"Domains\", [])\n",
    "\n",
    "for d in domains:\n",
    "    domain_id = d[\"DomainId\"]\n",
    "    domain_name = d[\"DomainName\"]\n",
    "\n",
    "    domain_total_gb = 0\n",
    "    spaces = []\n",
    "\n",
    "    # 2. Paginate over all spaces in the domain\n",
    "    paginator = sm.get_paginator(\"list_spaces\")\n",
    "    for page in paginator.paginate(DomainIdEquals=domain_id):\n",
    "        for s in page.get(\"Spaces\", []):\n",
    "            size_gb = (\n",
    "                s.get(\"SpaceSettingsSummary\", {})\n",
    "                 .get(\"SpaceStorageSettings\", {})\n",
    "                 .get(\"EbsStorageSettings\", {})\n",
    "                 .get(\"EbsVolumeSizeInGb\", 0)\n",
    "            )\n",
    "            domain_total_gb += size_gb\n",
    "            spaces.append({\n",
    "                \"SpaceName\": s.get(\"SpaceName\"),\n",
    "                \"Status\": s.get(\"Status\"),\n",
    "                \"AppType\": s.get(\"SpaceSettingsSummary\", {}).get(\"AppType\"),\n",
    "                \"EbsVolumeSizeInGb\": size_gb\n",
    "            })\n",
    "\n",
    "    grand_total_gb += domain_total_gb\n",
    "    domain_totals.append({\n",
    "        \"DomainName\": domain_name,\n",
    "        \"DomainId\": domain_id,\n",
    "        \"TotalGB\": domain_total_gb,\n",
    "        \"Spaces\": spaces\n",
    "    })\n",
    "\n",
    "# 3. Print results\n",
    "for d in domain_totals:\n",
    "    print(f\"\\nDomain: {d['DomainName']} ({d['DomainId']})\")\n",
    "    print(f\"  Total gp3 volume: {d['TotalGB']} GB\")\n",
    "    for s in sorted(d[\"Spaces\"], key=lambda x: x[\"EbsVolumeSizeInGb\"], reverse=True):\n",
    "        print(f\"    - {s['SpaceName']}: {s['EbsVolumeSizeInGb']} GB (status={s['Status']}, app={s['AppType']})\")\n",
    "\n",
    "print(f\"\\n==== GRAND TOTAL gp3 volume across all domains: {grand_total_gb} GB ====\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e7115b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': '0891ea5e-35bc-4b90-ad7d-191136b79571', 'HostId': '', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Tue, 12 Aug 2025 16:48:16 GMT', 'content-type': 'application/json', 'content-length': '14', 'connection': 'keep-alive', 'x-amz-request-id': '0891ea5e-35bc-4b90-ad7d-191136b79571', 'access-control-allow-origin': '*', 'vary': 'origin, access-control-request-method, access-control-request-headers', 'access-control-expose-headers': '*'}, 'RetryAttempts': 0}, 'indexes': []}\n",
      "Created index: {'ResponseMetadata': {'RequestId': '6c887001-74b8-4ed2-bd46-f04560647c0a', 'HostId': '', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Tue, 12 Aug 2025 16:48:16 GMT', 'content-type': 'application/json', 'content-length': '2', 'connection': 'keep-alive', 'x-amz-request-id': '6c887001-74b8-4ed2-bd46-f04560647c0a', 'access-control-allow-origin': '*', 'vary': 'origin, access-control-request-method, access-control-request-headers', 'access-control-expose-headers': '*'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "#create or delete vectors\n",
    "\n",
    "s3v = boto3.client(\"s3vectors\", region_name=\"us-east-1\")\n",
    "response = s3v.list_indexes(\n",
    "    # vectorBucketName='legal-docs-vectors',\n",
    "    vectorBucketArn = 'arn:aws:s3vectors:us-east-1:254281203237:bucket/legal-docs-vectors'\n",
    ")\n",
    "print(response)\n",
    "\n",
    "INDEX_NAME = 'token-chunking-vectors-poc'\n",
    "VECTOR_DIM = 384\n",
    "DISTANCE_METRIC = \"cosine\"\n",
    "NON_FILTERABLE_KEYS = ['text']\n",
    "\n",
    "# response = s3v.delete_index(\n",
    "#     vectorBucketName=VECTOR_BUCKET_NAME,\n",
    "#     indexName=INDEX_NAME,\n",
    "# )\n",
    "\n",
    "response = s3v.create_index(\n",
    "    vectorBucketArn = 'arn:aws:s3vectors:us-east-1:254281203237:bucket/legal-docs-vectors',\n",
    "    indexName=INDEX_NAME,\n",
    "    dataType=\"float32\",\n",
    "    dimension=VECTOR_DIM,\n",
    "    distanceMetric=DISTANCE_METRIC,\n",
    "    metadataConfiguration={\n",
    "        \"nonFilterableMetadataKeys\": NON_FILTERABLE_KEYS\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Created index: {response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
